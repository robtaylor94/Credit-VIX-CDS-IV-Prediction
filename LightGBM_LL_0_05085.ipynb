{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZekJohN0NP2O",
        "outputId": "e84dcb8a-8dd3-4294-ca62-6b43a909a9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GZwHUdVSEfpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e49cd23-aa4a-406d-c2e8-938bc627eb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import warnings\n",
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "68dYi4PDEhRp"
      },
      "outputs": [],
      "source": [
        "combined_features_path = '/content/drive/My Drive/Data/c_features.pkl'\n",
        "y_df_path = '/content/drive/My Drive/Data/c_target.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "BKRaZ58CFIeh"
      },
      "outputs": [],
      "source": [
        "def preprocess_features(X):\n",
        "    scaler = StandardScaler()\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = minmax_scaler.fit_transform(X_scaled)\n",
        "\n",
        "    X_scaled[np.isnan(X_scaled)] = 0\n",
        "    X_scaled[np.isinf(X_scaled)] = 0\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "def calculate_mape(actual, forecast):\n",
        "    return np.mean(np.abs((actual - forecast) / actual)) * 100\n",
        "\n",
        "def calculate_log_loss(actual, forecast):\n",
        "    return np.mean((np.log(actual / forecast))**2)\n",
        "\n",
        "class SuppressStdoutStderr:\n",
        "    def __init__(self):\n",
        "        self.old_stdout = sys.stdout\n",
        "        self.old_stderr = sys.stderr\n",
        "        self.mystdout = StringIO()\n",
        "        self.mystderr = StringIO()\n",
        "\n",
        "    def __enter__(self):\n",
        "        sys.stdout = self.mystdout\n",
        "        sys.stderr = self.mystderr\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self.old_stdout\n",
        "        sys.stderr = self.old_stderr\n",
        "        self.mystdout.seek(0)\n",
        "        self.mystderr.seek(0)\n",
        "        out_lines = self.mystdout.read().splitlines()\n",
        "        err_lines = self.mystderr.read().splitlines()\n",
        "        if len(out_lines) > 0:\n",
        "            print(out_lines[0])\n",
        "            print(f\"...(suppressed {len(out_lines)-1} lines)\")\n",
        "        if len(err_lines) > 0:\n",
        "            print(err_lines[0])\n",
        "            print(f\"...(suppressed {len(err_lines)-1} lines)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WdNmPn-RFDgN"
      },
      "outputs": [],
      "source": [
        "combined_features = pd.read_pickle(combined_features_path)\n",
        "y_df = pd.read_pickle(y_df_path)\n",
        "\n",
        "combined_features = combined_features.loc[y_df.index]\n",
        "y = y_df.values.flatten()\n",
        "\n",
        "X_scaled = preprocess_features(combined_features)\n",
        "\n",
        "splits = TimeSeriesSplit(n_splits=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpATWJJnELL6",
        "outputId": "8e5322c5-02f3-4faf-d21a-4c0f22f1b7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "...(suppressed 150112 lines)\n",
            "Best parameters found:  {'learning_rate': 0.1, 'min_child_samples': 20, 'min_split_gain': 0.01, 'n_estimators': 500, 'num_leaves': 31}\n",
            "Best MAE score:  8.767671352496158\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'num_leaves': [31, 50],\n",
        "    'learning_rate': [0.1, 0.01, 0.005],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'min_child_samples': [20, 50],\n",
        "    'min_split_gain': [0.0, 0.1, 0.01]\n",
        "}\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor()\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "with SuppressStdoutStderr():\n",
        "    grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring=mae_scorer, cv=splits, verbose=2)\n",
        "    grid_search.fit(X_scaled, y)\n",
        "\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best MAE score: \", -grid_search.best_score_)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "final_model = lgb.LGBMRegressor(**best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vtvbk71xETpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac809f0-7bec-48a8-b974-dbced2f58c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000088 seconds.\n",
            "...(suppressed 2661 lines)\n",
            "Average MAE: 8.767671352496158\n",
            "Average MAPE: 17.475629604825443\n",
            "Average Log Loss: 0.050657289296926346\n"
          ]
        }
      ],
      "source": [
        "forecasts = []\n",
        "forecast_indices = []\n",
        "mae_scores = []\n",
        "mape_scores = []\n",
        "log_loss_scores = []\n",
        "\n",
        "with SuppressStdoutStderr():\n",
        "    for train_idx, test_idx in splits.split(X_scaled):\n",
        "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        final_model.fit(X_train, y_train)\n",
        "        forecast = final_model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, forecast)\n",
        "        mape = calculate_mape(y_test, forecast)\n",
        "        log_loss = calculate_log_loss(y_test, forecast)\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        mape_scores.append(mape)\n",
        "        log_loss_scores.append(log_loss)\n",
        "        forecasts.extend(forecast)\n",
        "        forecast_indices.extend(test_idx)\n",
        "\n",
        "average_mae = np.mean(mae_scores)\n",
        "average_mape = np.mean(mape_scores)\n",
        "average_log_loss = np.mean(log_loss_scores)\n",
        "\n",
        "print(f\"Average MAE: {average_mae}\")\n",
        "print(f\"Average MAPE: {average_mape}\")\n",
        "print(f\"Average Log Loss: {average_log_loss}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}