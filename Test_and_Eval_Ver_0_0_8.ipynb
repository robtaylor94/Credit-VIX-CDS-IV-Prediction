{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFzsVerO5jwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e01ece-ece4-473b-abc7-c7d38c14db83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zqmEwUKt5fbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9259c52b-47fa-4dce-aee2-5032736dfec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0 found\n",
            "X shape: (156, 15)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Risk Forecasting')\n",
        "\n",
        "import numpy as np\n",
        "import inspect\n",
        "import pandas as pd\n",
        "import traceback\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from contextlib import redirect_stdout\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from models import SVM, TFT_GRU, LGBM, Naive\n",
        "\n",
        "plt.rc('font', family='serif')\n",
        "plt.rc('text', usetex=False)\n",
        "plt.rc('mathtext', fontset='stix')\n",
        "\n",
        "# check GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(f'{device_name} found')\n",
        "else:\n",
        "    print(\"No GPU found\")\n",
        "\n",
        "feature_matrix = pd.read_pickle('/content/drive/My Drive/Risk Forecasting/features.pkl')\n",
        "target_vector = (pd.read_excel('/content/drive/My Drive/Risk Forecasting/RESPONSE Main 1m.xls', skiprows=range(6))\n",
        "                .rename(columns=lambda x: x.strip()).assign(**{'Effective date': lambda df: pd.to_datetime(df['Effective date'], errors='coerce')})\n",
        "                .dropna(subset=['Effective date']).set_index('Effective date'))\n",
        "\n",
        "feature_matrix.index = pd.to_datetime(feature_matrix.index, errors='coerce')\n",
        "target_vector.index = pd.to_datetime(target_vector.index, errors='coerce')\n",
        "\n",
        "# align indices\n",
        "X = feature_matrix.loc[feature_matrix.index.intersection(target_vector.index)]\n",
        "y = target_vector.loc[feature_matrix.index.intersection(target_vector.index)]\n",
        "\n",
        "# # add time-related features\n",
        "# X['increasing_line'] = (X.index - X.index[0]).days\n",
        "# days_in_year = 252\n",
        "# X['sine_wave'] = np.sin(2 * np.pi * X['increasing_line'] / days_in_year)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ut-eHS095fbz"
      },
      "outputs": [],
      "source": [
        "def calculate_mape(actual, forecast):\n",
        "    return np.mean(np.abs((actual - forecast) / actual)) * 100\n",
        "\n",
        "def log_loss(actual, forecast):\n",
        "    return np.mean((np.log(actual / forecast))**2)\n",
        "\n",
        "def calculate_rmse(actual, forecast):\n",
        "    return np.sqrt(np.mean((actual - forecast) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aKU98C6e5fb2"
      },
      "outputs": [],
      "source": [
        "def rolling_test(model_class, feature_matrix, feature_vector, chunk, predictions, sequence_length=None, model_name=\"\"):\n",
        "    '''rolling (sliding) test, that runs a model over t specified periods, models go through an independent walk-forward validation for each prediction.'''\n",
        "    forecasts = np.array([])\n",
        "    forecast_dates = []\n",
        "    mae_scores = np.array([])\n",
        "    mape_scores = np.array([])\n",
        "    log_loss_scores = np.array([])\n",
        "    rmse_scores = np.array([])\n",
        "\n",
        "    t = 0\n",
        "\n",
        "    log_file_path = f'/content/drive/My Drive/Risk Forecasting/rolling_test_{model_name}_log.txt'\n",
        "\n",
        "    with open(log_file_path, 'w') as log_file:\n",
        "        while t < predictions:\n",
        "            window = (chunk - predictions) + t\n",
        "\n",
        "            X_fit = feature_matrix.iloc[t:window]\n",
        "            y_fit = feature_vector.iloc[t:window]\n",
        "            X_test = feature_matrix.iloc[window:window + 1]\n",
        "            y_test = feature_vector.iloc[window:window + 1]\n",
        "\n",
        "            if y_test.empty:\n",
        "                print(f\"no data for {window}\")\n",
        "                log_file.write(f\"no data for {window}\\n\")\n",
        "                t += 1\n",
        "                continue\n",
        "\n",
        "            print(f\"starting prediction for {y_test.index[0].strftime('%Y-%m-%d')}\")\n",
        "            log_file.write(f\"starting prediction for {y_test.index[0].strftime('%Y-%m-%d')}\\n\")\n",
        "            print(f\"training window shape: X_fit={X_fit.shape}, y_fit={y_fit.shape}\")\n",
        "            log_file.write(f\"training window shape: X_fit={X_fit.shape}, y_fit={y_fit.shape}\\n\")\n",
        "            print(f\"test window shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "            log_file.write(f\"test window shape: X_test={X_test.shape}, y_test={y_test.shape}\\n\")\n",
        "\n",
        "            try:\n",
        "                model_args = inspect.signature(model_class.__init__).parameters\n",
        "                model = model_class(sequence_length=sequence_length) if 'sequence_length' in model_args else model_class()\n",
        "                model.fit(X_fit.values, y_fit.values)\n",
        "                forecast = model.predict().flatten()\n",
        "\n",
        "                forecasts = np.append(forecasts, forecast)\n",
        "                forecast_dates.append(y_test.index[0])\n",
        "\n",
        "                mae = mean_absolute_error(y_test.values.flatten(), forecast)\n",
        "                mape = calculate_mape(y_test.values.flatten(), forecast)\n",
        "                log_loss_value = log_loss(np.expand_dims(y_test.values.flatten(), axis=0), np.expand_dims(forecast, axis=0))\n",
        "                rmse = calculate_rmse(y_test.values.flatten(), forecast)\n",
        "\n",
        "                mae_scores = np.append(mae_scores, mae)\n",
        "                mape_scores = np.append(mape_scores, mape)\n",
        "                log_loss_scores = np.append(log_loss_scores, log_loss_value)\n",
        "                rmse_scores = np.append(rmse_scores, rmse)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\\nTraceback:\\n{traceback.format_exc()}\")\n",
        "                log_file.write(f\"An error occurred: {e}\\nTraceback:\\n{traceback.format_exc()}\\n\")\n",
        "                print(f\"types - X_fit: {type(X_fit)}, y_fit: {type(y_fit)}\")\n",
        "                log_file.write(f\"types - X_fit: {type(X_fit)}, y_fit: {type(y_fit)}\\n\")\n",
        "                print(f\"dims - X_fit: {X_fit.shape}, y_fit: {y_fit.shape}\")\n",
        "                log_file.write(f\"dims - X_fit: {X_fit.shape}, y_fit: {y_fit.shape}\\n\")\n",
        "\n",
        "            t += 1\n",
        "\n",
        "        avg_mae = np.mean(mae_scores)\n",
        "        avg_mape = np.mean(mape_scores)\n",
        "        avg_log_loss = np.mean(log_loss_scores)\n",
        "        avg_rmse = np.mean(rmse_scores)\n",
        "\n",
        "        print(f\"avg. MAE: {avg_mae}\")\n",
        "        log_file.write(f\"avg. MAE: {avg_mae}\\n\")\n",
        "        print(f\"avg. MAPE: {avg_mape}\")\n",
        "        log_file.write(f\"avg. MAPE: {avg_mape}\\n\")\n",
        "        print(f\"avg. Log Loss: {avg_log_loss}\")\n",
        "        log_file.write(f\"avg. Log Loss: {avg_log_loss}\\n\")\n",
        "        print(f\"avg. RMSE: {avg_rmse}\")\n",
        "        log_file.write(f\"avg. RMSE: {avg_rmse}\\n\")\n",
        "\n",
        "    if forecast_dates:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        forecast_dates = pd.DatetimeIndex(forecast_dates)\n",
        "        actuals = feature_vector.loc[forecast_dates]\n",
        "        estimates = forecasts[:len(forecast_dates)]\n",
        "        plt.plot(forecast_dates, actuals, 'o', color='grey', label='Actual')\n",
        "        plt.plot(forecast_dates, estimates, 'x', color='black', label='Estimate')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title(f'Estimate vs Actual - {model_name}')\n",
        "        plt.legend()\n",
        "        plt.savefig(f'/content/drive/My Drive/Risk Forecasting/{model_name}_estimates_vs_actuals.png')\n",
        "        plt.show()\n",
        "\n",
        "        residuals = actuals.values.flatten() - estimates\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(forecast_dates, residuals, 'x', color='black')\n",
        "        plt.axhline(y=0, color='grey', linestyle='--')\n",
        "        for date, residual in zip(forecast_dates, residuals):\n",
        "            plt.vlines(x=date, ymin=0, ymax=residual, color='black', linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title(f'Residual Plot - {model_name}')\n",
        "        plt.savefig(f'/content/drive/My Drive/Risk Forecasting/{model_name}_residual_plot.png')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"forecasts were not made: unable to plot results.\")\n",
        "\n",
        "    return {'MAE': avg_mae, 'MAPE': avg_mape, 'Log Loss': avg_log_loss, 'RMSE': avg_rmse}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N7QZsFEKWLVD"
      },
      "outputs": [],
      "source": [
        "window = 50 # number of observations passed to model\n",
        "predictions = 10 # subset at end of sliced observations dedicated to estimates (model will be fit on observations - predictions)\n",
        "sequence_length = 5 # length of sequences (set to 5 for trading week)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_metrics = {}"
      ],
      "metadata": {
        "id": "METC53PFHNpg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yeSvJRK8RCvi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c29f9fd-0824-4036-aad5-3087a18eae7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting prediction for 2023-02-10\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-13\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-14\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-15\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-16\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-17\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-20\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-21\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-22\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "starting prediction for 2023-02-23\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "avg. MAE: 1.611\n",
            "avg. MAPE: 3.564270837823829\n",
            "avg. Log Loss: 0.002741774827865124\n",
            "avg. RMSE: 1.611\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_metrics['Nave Forecast'] = rolling_test(Naive, X, y, chunk=(window + predictions), predictions=predictions, model_name='Nave')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cFaBiEdFJTNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27fbcf81-139e-4534-8458-d1768cc6f7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 4 Validation MSE: 0.5908583280578203\n",
            "Step 5: Training on 9 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.2140 - val_loss: 0.3227 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1922 - val_loss: 0.3260 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4648 - val_loss: 0.2323 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.9541 - val_loss: 0.2045 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2613 - val_loss: 0.1802 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1931 - val_loss: 0.1041 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.4124 - val_loss: 0.0685 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 5 Validation MSE: 0.3259907118391465\n",
            "Step 6: Training on 10 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5917 - val_loss: 0.4478 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.4821 - val_loss: 0.4770 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4220 - val_loss: 0.4119 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.2686 - val_loss: 0.2690 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.9644 - val_loss: 0.2656 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2995 - val_loss: 0.2939 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7486 - val_loss: 0.3526 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4170 - val_loss: 0.3907 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.2700 - val_loss: 0.3570 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 6 Validation MSE: 0.26899233972886427\n",
            "Step 7: Training on 11 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3812 - val_loss: 0.3174 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5072 - val_loss: 0.3262 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5544 - val_loss: 0.3985 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3228 - val_loss: 0.4356 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5929 - val_loss: 0.5244 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3019 - val_loss: 0.6383 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3298 - val_loss: 0.7191 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7125 - val_loss: 0.8927 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4683 - val_loss: 0.8838 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4808 - val_loss: 0.6312 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2143 - val_loss: 0.5413 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3250 - val_loss: 0.5092 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.7450 - val_loss: 0.5411 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3005 - val_loss: 0.5777 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2135 - val_loss: 0.5680 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6755 - val_loss: 0.5929 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1106 - val_loss: 0.5926 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3189 - val_loss: 0.6294 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4885 - val_loss: 0.4588 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3194 - val_loss: 0.3237 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 7 Validation MSE: 0.3237249870244672\n",
            "Step 8: Training on 12 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3171 - val_loss: 0.1968 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3789 - val_loss: 0.1710 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1842 - val_loss: 0.1413 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2247 - val_loss: 0.1192 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2700 - val_loss: 0.1266 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4276 - val_loss: 0.1746 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2951 - val_loss: 0.2718 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.2917 - val_loss: 0.4069 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 8 Validation MSE: 0.14132774191184522\n",
            "Step 9: Training on 13 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3421 - val_loss: 0.5671 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1500 - val_loss: 0.8176 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4923 - val_loss: 0.9192 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6238 - val_loss: 0.9097 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1728 - val_loss: 0.7681 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2460 - val_loss: 0.6155 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2605 - val_loss: 0.4845 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 9 Validation MSE: 0.8175903001735184\n",
            "Step 10: Training on 14 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.2041 - val_loss: 0.8140 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3277 - val_loss: 0.6531 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1682 - val_loss: 0.5811 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2823 - val_loss: 0.6051 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3111 - val_loss: 0.7715 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2281 - val_loss: 0.9169 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4339 - val_loss: 0.9196 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.2477 - val_loss: 0.8464 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 10 Validation MSE: 0.581135190139171\n",
            "Step 11: Training on 15 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.2635 - val_loss: 0.3099 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3858 - val_loss: 0.2161 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2136 - val_loss: 0.1465 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.2133 - val_loss: 0.1265 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3797 - val_loss: 0.1994 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2309 - val_loss: 0.2735 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2808 - val_loss: 0.3493 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1422 - val_loss: 0.3609 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1833 - val_loss: 0.3053 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2938 - val_loss: 0.2420 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3372 - val_loss: 0.2407 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2126 - val_loss: 0.2799 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1525 - val_loss: 0.3288 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 11 Validation MSE: 0.3608957591560012\n",
            "Step 12: Training on 16 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3480 - val_loss: 0.2573 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3608 - val_loss: 0.1977 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3633 - val_loss: 0.1703 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2397 - val_loss: 0.1404 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.2035 - val_loss: 0.1123 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3180 - val_loss: 0.1035 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0747 - val_loss: 0.1049 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0712 - val_loss: 0.1237 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2340 - val_loss: 0.1678 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2497 - val_loss: 0.2068 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2074 - val_loss: 0.2310 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3851 - val_loss: 0.1821 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1623 - val_loss: 0.1697 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 12 Validation MSE: 0.12371090927070412\n",
            "Step 13: Training on 17 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.2744 - val_loss: 0.0879 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2624 - val_loss: 0.0992 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1816 - val_loss: 0.1406 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2260 - val_loss: 0.2293 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1271 - val_loss: 0.3179 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2935 - val_loss: 0.3706 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3004 - val_loss: 0.3743 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2925 - val_loss: 0.3147 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3782 - val_loss: 0.1910 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1243 - val_loss: 0.0908 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2612 - val_loss: 0.0165 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3011 - val_loss: 0.0049 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3166 - val_loss: 0.0120 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2080 - val_loss: 0.0413 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1449 - val_loss: 0.0901 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 13 Validation MSE: 0.09079808716857618\n",
            "Step 14: Training on 18 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.2411 - val_loss: 0.2345 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2487 - val_loss: 0.3415 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2305 - val_loss: 0.4015 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1208 - val_loss: 0.4605 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2550 - val_loss: 0.4301 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2866 - val_loss: 0.3492 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1555 - val_loss: 0.3278 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1138 - val_loss: 0.2837 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1387 - val_loss: 0.2588 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1367 - val_loss: 0.2448 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1240 - val_loss: 0.2141 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1197 - val_loss: 0.2112 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1668 - val_loss: 0.2284 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 14 Validation MSE: 0.2836796055454957\n",
            "Step 15: Training on 19 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0845 - val_loss: 0.4612 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1605 - val_loss: 0.4744 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1360 - val_loss: 0.4612 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1646 - val_loss: 0.4204 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1711 - val_loss: 0.3734 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1326 - val_loss: 0.3639 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 15 Validation MSE: 0.46124919103926976\n",
            "Step 16: Training on 20 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.1963 - val_loss: 0.2979 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1250 - val_loss: 0.2825 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1044 - val_loss: 0.2774 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1675 - val_loss: 0.2461 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0739 - val_loss: 0.2215 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1403 - val_loss: 0.2038 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2595 - val_loss: 0.2124 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0874 - val_loss: 0.2553 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1655 - val_loss: 0.3164 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1417 - val_loss: 0.3845 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 16 Validation MSE: 0.22153643797794037\n",
            "Step 17: Training on 21 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.1175 - val_loss: 0.5946 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1314 - val_loss: 0.6807 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1476 - val_loss: 0.7472 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1666 - val_loss: 0.7139 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0822 - val_loss: 0.6608 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1494 - val_loss: 0.5203 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1533 - val_loss: 0.4095 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1539 - val_loss: 0.3499 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1097 - val_loss: 0.3299 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1765 - val_loss: 0.3453 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 17 Validation MSE: 0.6608353262172053\n",
            "Step 18: Training on 22 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.1372 - val_loss: 0.6600 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1144 - val_loss: 0.6240 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0736 - val_loss: 0.6041 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1217 - val_loss: 0.5710 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1150 - val_loss: 0.5154 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1773 - val_loss: 0.4371 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1301 - val_loss: 0.3795 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1735 - val_loss: 0.3969 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 18 Validation MSE: 0.6041311777347972\n",
            "Step 19: Training on 23 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.1588 - val_loss: 0.4526 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1212 - val_loss: 0.4733 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1232 - val_loss: 0.4520 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0747 - val_loss: 0.4392 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1533 - val_loss: 0.3728 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1270 - val_loss: 0.2840 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1640 - val_loss: 0.2364 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1147 - val_loss: 0.2260 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.1571 - val_loss: 0.2494 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 19 Validation MSE: 0.43917781833114156\n",
            "Step 20: Training on 24 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0485 - val_loss: 0.4741 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1231 - val_loss: 0.4468 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1904 - val_loss: 0.4315 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1915 - val_loss: 0.4158 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1301 - val_loss: 0.3586 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.1357 - val_loss: 0.3131 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 20 Validation MSE: 0.47410818806907434\n",
            "Step 21: Training on 25 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1853 - val_loss: 0.4669 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1320 - val_loss: 0.3554 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1080 - val_loss: 0.2942 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1903 - val_loss: 0.2646 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1079 - val_loss: 0.2695 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1073 - val_loss: 0.2982 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1367 - val_loss: 0.3449 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0830 - val_loss: 0.3592 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0880 - val_loss: 0.3990 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1121 - val_loss: 0.4238 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0788 - val_loss: 0.4341 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0730 - val_loss: 0.4672 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1401 - val_loss: 0.5092 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0861 - val_loss: 0.5338 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1147 - val_loss: 0.5177 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.1977 - val_loss: 0.4609 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1107 - val_loss: 0.4059 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 21 Validation MSE: 0.4671508491525675\n",
            "Step 22: Training on 26 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0817 - val_loss: 0.2419 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1331 - val_loss: 0.2284 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1474 - val_loss: 0.2296 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1064 - val_loss: 0.2416 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0780 - val_loss: 0.2474 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1134 - val_loss: 0.2316 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0497 - val_loss: 0.2075 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0748 - val_loss: 0.1886 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1429 - val_loss: 0.1912 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.1100 - val_loss: 0.2091 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0789 - val_loss: 0.2198 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0564 - val_loss: 0.2339 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 22 Validation MSE: 0.20746192920694698\n",
            "Step 23: Training on 27 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0776 - val_loss: 0.2306 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0740 - val_loss: 0.2364 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1145 - val_loss: 0.2298 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1011 - val_loss: 0.2130 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0998 - val_loss: 0.2030 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0928 - val_loss: 0.2060 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0650 - val_loss: 0.2245 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1084 - val_loss: 0.2526 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0882 - val_loss: 0.2551 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0811 - val_loss: 0.2391 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1176 - val_loss: 0.1997 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0733 - val_loss: 0.1888 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 23 Validation MSE: 0.22450051194171197\n",
            "Step 24: Training on 28 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0753 - val_loss: 0.2873 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0634 - val_loss: 0.2893 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0939 - val_loss: 0.3058 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0769 - val_loss: 0.3131 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1073 - val_loss: 0.2970 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1191 - val_loss: 0.2473 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.1105 - val_loss: 0.2110 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 24 Validation MSE: 0.2892898840458129\n",
            "Step 25: Training on 29 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0835 - val_loss: 0.2872 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0979 - val_loss: 0.2326 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0756 - val_loss: 0.2077 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0825 - val_loss: 0.2002 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0957 - val_loss: 0.1906 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1030 - val_loss: 0.1905 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0934 - val_loss: 0.2099 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1391 - val_loss: 0.2166 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 25 Validation MSE: 0.2076658944133669\n",
            "Step 26: Training on 30 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0979 - val_loss: 0.0459 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0961 - val_loss: 0.0582 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1162 - val_loss: 0.0702 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0521 - val_loss: 0.0791 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0985 - val_loss: 0.0842 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0727 - val_loss: 0.0974 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0734 - val_loss: 0.0940 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.1316 - val_loss: 0.0920 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.1094 - val_loss: 0.0960 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 26 Validation MSE: 0.07906920763964283\n",
            "Step 27: Training on 31 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0959 - val_loss: 0.0276 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0690 - val_loss: 0.0231 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1276 - val_loss: 0.0243 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0951 - val_loss: 0.0331 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0764 - val_loss: 0.0490 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0858 - val_loss: 0.0544 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0736 - val_loss: 0.0480 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 27 Validation MSE: 0.023050120804110043\n",
            "Step 28: Training on 32 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.1046 - val_loss: 0.1281 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0731 - val_loss: 0.1402 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1097 - val_loss: 0.1674 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0708 - val_loss: 0.1947 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0986 - val_loss: 0.2011 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1294 - val_loss: 0.1674 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0852 - val_loss: 0.1472 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0887 - val_loss: 0.1351 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0576 - val_loss: 0.1281 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0569 - val_loss: 0.1306 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0847 - val_loss: 0.1527 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0843 - val_loss: 0.1801 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0630 - val_loss: 0.1927 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0685 - val_loss: 0.1923 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0735 - val_loss: 0.1978 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 28 Validation MSE: 0.1306171245709047\n",
            "Step 29: Training on 33 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0955 - val_loss: 0.1576 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0721 - val_loss: 0.1565 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1125 - val_loss: 0.1666 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0667 - val_loss: 0.1686 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0607 - val_loss: 0.1623 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0741 - val_loss: 0.1599 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0615 - val_loss: 0.1765 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0655 - val_loss: 0.1971 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0684 - val_loss: 0.1975 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0809 - val_loss: 0.1930 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 29 Validation MSE: 0.16228999611197598\n",
            "Step 30: Training on 34 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0708 - val_loss: 0.1476 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0805 - val_loss: 0.1421 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0916 - val_loss: 0.1546 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0731 - val_loss: 0.1739 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0790 - val_loss: 0.2017 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0874 - val_loss: 0.2010 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 30 Validation MSE: 0.1475496007865534\n",
            "Step 31: Training on 35 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0459 - val_loss: 0.3116 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0888 - val_loss: 0.3369 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0334 - val_loss: 0.3512 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0687 - val_loss: 0.3299 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0710 - val_loss: 0.3112 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0848 - val_loss: 0.3167 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0759 - val_loss: 0.3227 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0535 - val_loss: 0.3206 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 31 Validation MSE: 0.35117722675970325\n",
            "Step 32: Training on 36 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0985 - val_loss: 0.2870 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0732 - val_loss: 0.2988 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0413 - val_loss: 0.3113 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0650 - val_loss: 0.3554 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0442 - val_loss: 0.4028 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0832 - val_loss: 0.4831 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0815 - val_loss: 0.5490 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0832 - val_loss: 0.5592 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 32 Validation MSE: 0.3113194201749086\n",
            "Step 33: Training on 37 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0809 - val_loss: 0.3235 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0704 - val_loss: 0.3458 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0372 - val_loss: 0.3701 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0510 - val_loss: 0.3941 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0659 - val_loss: 0.4100 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0705 - val_loss: 0.4048 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0784 - val_loss: 0.3920 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0710 - val_loss: 0.3638 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 33 Validation MSE: 0.37010948374218083\n",
            "Step 34: Training on 38 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0788 - val_loss: 0.2837 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0461 - val_loss: 0.2617 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0715 - val_loss: 0.2225 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0871 - val_loss: 0.2007 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0754 - val_loss: 0.1889 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0686 - val_loss: 0.1941 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0446 - val_loss: 0.2118 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0546 - val_loss: 0.2380 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0863 - val_loss: 0.2826 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0565 - val_loss: 0.3383 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0696 - val_loss: 0.3817 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0768 - val_loss: 0.4151 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 34 Validation MSE: 0.21180233716520303\n",
            "Step 35: Training on 39 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0926 - val_loss: 0.2919 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0636 - val_loss: 0.3491 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0521 - val_loss: 0.3897 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0486 - val_loss: 0.3893 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0672 - val_loss: 0.3559 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0670 - val_loss: 0.2992 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0566 - val_loss: 0.2455 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0463 - val_loss: 0.2050 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0687 - val_loss: 0.1870 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0598 - val_loss: 0.1883 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0408 - val_loss: 0.1995 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0611 - val_loss: 0.2218 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0369 - val_loss: 0.2647 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0413 - val_loss: 0.2955 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0468 - val_loss: 0.3156 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0647 - val_loss: 0.3070 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0403 - val_loss: 0.2930 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0597 - val_loss: 0.2742 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 35 Validation MSE: 0.2646883942499739\n",
            "Step 36: Training on 40 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0386 - val_loss: 0.3273 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0507 - val_loss: 0.3140 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0502 - val_loss: 0.3300 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0489 - val_loss: 0.3349 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0514 - val_loss: 0.3383 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0642 - val_loss: 0.3475 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 36 Validation MSE: 0.3272522218176164\n",
            "Step 37: Training on 41 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0466 - val_loss: 0.6160 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0496 - val_loss: 0.6344 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0511 - val_loss: 0.6327 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0787 - val_loss: 0.6040 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0423 - val_loss: 0.5889 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0469 - val_loss: 0.5786 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0614 - val_loss: 0.5694 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0566 - val_loss: 0.5717 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0664 - val_loss: 0.6129 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0572 - val_loss: 0.6504 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 37 Validation MSE: 0.5888597537856834\n",
            "Step 38: Training on 42 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0456 - val_loss: 0.5439 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0302 - val_loss: 0.6048 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0461 - val_loss: 0.6425 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0520 - val_loss: 0.6283 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0413 - val_loss: 0.6025 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0699 - val_loss: 0.5517 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0560 - val_loss: 0.5020 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 38 Validation MSE: 0.6047516163849289\n",
            "Step 39: Training on 43 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0767 - val_loss: 0.4498 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0369 - val_loss: 0.3938 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0435 - val_loss: 0.3485 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0633 - val_loss: 0.3254 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0628 - val_loss: 0.3293 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0501 - val_loss: 0.3647 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0481 - val_loss: 0.4130 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 39 Validation MSE: 0.39379378446377455\n",
            "Step 40: Training on 44 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0589 - val_loss: 0.3793 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0334 - val_loss: 0.4093 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0552 - val_loss: 0.4053 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0489 - val_loss: 0.3783 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0578 - val_loss: 0.3397 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0683 - val_loss: 0.2848 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0452 - val_loss: 0.2526 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 40 Validation MSE: 0.4092932215260589\n",
            "Step 41: Training on 45 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0580 - val_loss: 0.2627 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0341 - val_loss: 0.2114 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0410 - val_loss: 0.1724 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0311 - val_loss: 0.1511 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0524 - val_loss: 0.1520 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0471 - val_loss: 0.1707 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0624 - val_loss: 0.2023 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0588 - val_loss: 0.2354 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0350 - val_loss: 0.2675 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 41 Validation MSE: 0.1510940576867187\n",
            "Step 42: Training on 46 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0438 - val_loss: 0.0885 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0448 - val_loss: 0.1303 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0510 - val_loss: 0.1754 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0511 - val_loss: 0.2016 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0882 - val_loss: 0.1939 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0467 - val_loss: 0.1643 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Step 42 Validation MSE: 0.08849771596500668\n",
            "Mean validation error: 0.5824602035666455\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0712 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0602 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0530 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0649 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0510 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0593 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0627 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0417 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0430 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0481 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0502 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0333 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0652 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0477 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0525 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0554 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0423 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "starting prediction for 2023-02-20\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "X sequenced dim: (46, 5, 15)\n",
            "y dim: (46,)\n",
            "Step 1: Training on 5 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 10s 10s/step - loss: 2.4838 - val_loss: 0.0875 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.0546 - val_loss: 1.8849 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0161 - val_loss: 1.3242 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8506 - val_loss: 1.9855 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3241 - val_loss: 1.0031 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2497 - val_loss: 0.2846 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7751 - val_loss: 0.5086 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.2475 - val_loss: 0.8102 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4387 - val_loss: 0.8563 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6440 - val_loss: 0.5031 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6369 - val_loss: 0.3111 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4726 - val_loss: 0.2154 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1136 - val_loss: 0.2364 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1425 - val_loss: 0.2857 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4280 - val_loss: 0.2125 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5014 - val_loss: 0.1769 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2546 - val_loss: 0.1139 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.3469 - val_loss: 0.1457 - lr: 0.0010\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Step 1 Validation MSE: 0.2364257074334725\n",
            "Step 2: Training on 6 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3652 - val_loss: 0.4272 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4425 - val_loss: 0.4438 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1963 - val_loss: 0.3794 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1691 - val_loss: 0.4328 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7120 - val_loss: 0.4054 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2659 - val_loss: 0.5228 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0913 - val_loss: 0.7249 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2089 - val_loss: 0.8239 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4126 - val_loss: 0.7480 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3656 - val_loss: 0.5853 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1987 - val_loss: 0.3388 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1451 - val_loss: 0.1700 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 2 Validation MSE: 0.7248525894918412\n",
            "Step 3: Training on 7 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0731 - val_loss: 0.1133 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2503 - val_loss: 0.0375 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1505 - val_loss: 0.0014 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4301 - val_loss: 0.0018 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5264 - val_loss: 0.0301 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.2458 - val_loss: 0.1068 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 3 Validation MSE: 0.1132876691938355\n",
            "Step 4: Training on 8 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.1980 - val_loss: 0.4152 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1805 - val_loss: 0.5709 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.6217 - val_loss: 0.5701 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3189 - val_loss: 0.4845 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5939 - val_loss: 0.3294 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1076 - val_loss: 0.2235 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1099 - val_loss: 0.1860 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2092 - val_loss: 0.1759 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1295 - val_loss: 0.1780 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2755 - val_loss: 0.1450 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.2588 - val_loss: 0.1493 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 4 Validation MSE: 0.22345326457554368\n",
            "Step 5: Training on 9 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.2626 - val_loss: 0.1642 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.1308 - val_loss: 0.1370 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1145 - val_loss: 0.1098 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1771 - val_loss: 0.0709 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3408 - val_loss: 0.0590 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2325 - val_loss: 0.0759 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0862 - val_loss: 0.1036 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1157 - val_loss: 0.1428 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0831 - val_loss: 0.1824 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0533 - val_loss: 0.2227 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3060 - val_loss: 0.2862 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4033 - val_loss: 0.2828 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1136 - val_loss: 0.2431 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4439 - val_loss: 0.1464 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.1534 - val_loss: 0.0676 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 5 Validation MSE: 0.22272506151581048\n",
            "Step 6: Training on 10 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.1295 - val_loss: 0.1784 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1472 - val_loss: 0.0698 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0383 - val_loss: 0.0172 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3210 - val_loss: 0.0014 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4009 - val_loss: 0.0012 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3570 - val_loss: 0.0103 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3276 - val_loss: 0.0444 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.3065 - val_loss: 0.1405 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 6 Validation MSE: 0.01721178562569026\n",
            "Step 7: Training on 11 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3257 - val_loss: 0.0983 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2068 - val_loss: 0.2906 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1171 - val_loss: 0.5711 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1695 - val_loss: 0.8483 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5621 - val_loss: 0.8767 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5820 - val_loss: 0.7068 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1099 - val_loss: 0.5069 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2374 - val_loss: 0.3258 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1296 - val_loss: 0.1781 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0924 - val_loss: 0.0719 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0987 - val_loss: 0.0224 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2701 - val_loss: 0.0092 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3144 - val_loss: 0.0169 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3261 - val_loss: 0.0526 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1053 - val_loss: 0.1252 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 7 Validation MSE: 0.07188684259748614\n",
            "Step 8: Training on 12 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.1260 - val_loss: 0.2787 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1198 - val_loss: 0.4578 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2711 - val_loss: 0.6526 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1098 - val_loss: 0.7946 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1928 - val_loss: 0.8645 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2140 - val_loss: 0.8313 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1090 - val_loss: 0.7832 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2448 - val_loss: 0.6701 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2977 - val_loss: 0.5046 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2380 - val_loss: 0.3657 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1856 - val_loss: 0.2861 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1011 - val_loss: 0.2230 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1076 - val_loss: 0.1993 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0838 - val_loss: 0.1934 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1678 - val_loss: 0.2229 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0914 - val_loss: 0.2671 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1240 - val_loss: 0.3282 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1183 - val_loss: 0.3996 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.2519 - val_loss: 0.4397 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 8 Validation MSE: 0.19337541917154857\n",
            "Step 9: Training on 13 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1255 - val_loss: 0.0250 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1259 - val_loss: 0.0620 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1571 - val_loss: 0.1088 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1093 - val_loss: 0.1818 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0478 - val_loss: 0.2528 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0670 - val_loss: 0.3340 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0941 - val_loss: 0.3610 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1908 - val_loss: 0.3421 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2390 - val_loss: 0.2841 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1427 - val_loss: 0.2124 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 9 Validation MSE: 0.2527548928750883\n",
            "Step 10: Training on 14 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.1069 - val_loss: 0.1826 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1321 - val_loss: 0.0990 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.1105 - val_loss: 0.0450 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1099 - val_loss: 0.0229 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0733 - val_loss: 0.0143 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0685 - val_loss: 0.0124 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2462 - val_loss: 0.0181 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1015 - val_loss: 0.0289 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0842 - val_loss: 0.0426 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1295 - val_loss: 0.0517 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1444 - val_loss: 0.0545 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 10 Validation MSE: 0.012415160661936566\n",
            "Step 11: Training on 15 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.1304 - val_loss: 0.0826 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0744 - val_loss: 0.1096 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0644 - val_loss: 0.1411 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0796 - val_loss: 0.1820 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1335 - val_loss: 0.2265 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0631 - val_loss: 0.2694 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1421 - val_loss: 0.2797 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1166 - val_loss: 0.2659 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0681 - val_loss: 0.2384 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0952 - val_loss: 0.1996 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0803 - val_loss: 0.1704 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 11 Validation MSE: 0.26938485349437596\n",
            "Step 12: Training on 16 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.1366 - val_loss: 0.2202 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0844 - val_loss: 0.1665 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0523 - val_loss: 0.1236 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0857 - val_loss: 0.0931 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1930 - val_loss: 0.0840 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0863 - val_loss: 0.0891 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1400 - val_loss: 0.1150 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0649 - val_loss: 0.1431 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 12 Validation MSE: 0.12363674973333035\n",
            "Step 13: Training on 17 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0550 - val_loss: 0.1906 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0606 - val_loss: 0.2516 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0633 - val_loss: 0.3017 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0879 - val_loss: 0.3386 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1105 - val_loss: 0.3697 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0625 - val_loss: 0.3806 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 13 Validation MSE: 0.19061279681047594\n",
            "Step 14: Training on 18 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0657 - val_loss: 0.2800 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0853 - val_loss: 0.2973 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0536 - val_loss: 0.3160 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0862 - val_loss: 0.3114 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0913 - val_loss: 0.3089 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0679 - val_loss: 0.2958 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1007 - val_loss: 0.2576 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0917 - val_loss: 0.2272 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 14 Validation MSE: 0.31597707053183494\n",
            "Step 15: Training on 19 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0817 - val_loss: 0.2522 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0612 - val_loss: 0.2261 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0950 - val_loss: 0.2076 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0505 - val_loss: 0.1870 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1512 - val_loss: 0.1863 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0574 - val_loss: 0.1919 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0714 - val_loss: 0.2120 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1209 - val_loss: 0.2237 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0645 - val_loss: 0.2404 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 15 Validation MSE: 0.18698617065793074\n",
            "Step 16: Training on 20 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0616 - val_loss: 0.2217 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0683 - val_loss: 0.2444 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0788 - val_loss: 0.2594 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0725 - val_loss: 0.2723 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0593 - val_loss: 0.2828 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0547 - val_loss: 0.2911 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0692 - val_loss: 0.3007 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0679 - val_loss: 0.2891 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0346 - val_loss: 0.2799 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0883 - val_loss: 0.2679 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0670 - val_loss: 0.2574 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0705 - val_loss: 0.2428 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0346 - val_loss: 0.2330 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0651 - val_loss: 0.2206 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 16 Validation MSE: 0.27991504035817744\n",
            "Step 17: Training on 21 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0701 - val_loss: 0.4727 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0564 - val_loss: 0.4397 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0367 - val_loss: 0.4076 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0958 - val_loss: 0.3825 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0752 - val_loss: 0.3707 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0799 - val_loss: 0.3829 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.1315 - val_loss: 0.4382 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0552 - val_loss: 0.4817 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 17 Validation MSE: 0.4075642545107785\n",
            "Step 18: Training on 22 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0912 - val_loss: 0.3047 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1019 - val_loss: 0.3577 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0518 - val_loss: 0.3905 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0228 - val_loss: 0.4199 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0699 - val_loss: 0.4452 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0553 - val_loss: 0.4623 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0794 - val_loss: 0.4599 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1076 - val_loss: 0.4028 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0604 - val_loss: 0.3306 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 18 Validation MSE: 0.41985855738187267\n",
            "Step 19: Training on 23 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0509 - val_loss: 0.3163 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0652 - val_loss: 0.2433 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0508 - val_loss: 0.1981 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0510 - val_loss: 0.1630 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0512 - val_loss: 0.1506 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0734 - val_loss: 0.1510 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1360 - val_loss: 0.1822 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0614 - val_loss: 0.2221 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 19 Validation MSE: 0.19812183953414078\n",
            "Step 20: Training on 24 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0782 - val_loss: 0.2496 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0616 - val_loss: 0.3010 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0396 - val_loss: 0.3417 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0745 - val_loss: 0.3626 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0675 - val_loss: 0.3647 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0640 - val_loss: 0.3676 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0569 - val_loss: 0.3573 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0539 - val_loss: 0.3292 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 20 Validation MSE: 0.34172120148057716\n",
            "Step 21: Training on 25 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0472 - val_loss: 0.3048 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0798 - val_loss: 0.2457 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0757 - val_loss: 0.1959 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0980 - val_loss: 0.1641 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0771 - val_loss: 0.1520 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0495 - val_loss: 0.1557 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 21 Validation MSE: 0.3047791027717538\n",
            "Step 22: Training on 26 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0502 - val_loss: 0.2287 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0571 - val_loss: 0.2221 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0670 - val_loss: 0.2149 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0670 - val_loss: 0.2084 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0415 - val_loss: 0.1996 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0504 - val_loss: 0.1890 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0747 - val_loss: 0.1807 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0517 - val_loss: 0.1644 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0201 - val_loss: 0.1518 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0769 - val_loss: 0.1478 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0560 - val_loss: 0.1527 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0838 - val_loss: 0.1774 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0596 - val_loss: 0.1895 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0705 - val_loss: 0.1966 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 22 Validation MSE: 0.15179237372065163\n",
            "Step 23: Training on 27 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0525 - val_loss: 0.0999 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0514 - val_loss: 0.1030 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0502 - val_loss: 0.1075 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0701 - val_loss: 0.1078 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0665 - val_loss: 0.1089 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0400 - val_loss: 0.1087 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0541 - val_loss: 0.1093 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0476 - val_loss: 0.1117 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0787 - val_loss: 0.1085 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0308 - val_loss: 0.1117 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0853 - val_loss: 0.1094 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0487 - val_loss: 0.1019 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0648 - val_loss: 0.0884 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0421 - val_loss: 0.0697 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0457 - val_loss: 0.0570 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 23 Validation MSE: 0.1117339977040885\n",
            "Step 24: Training on 28 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0729 - val_loss: 0.1321 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0480 - val_loss: 0.1073 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0435 - val_loss: 0.0903 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0465 - val_loss: 0.0778 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0461 - val_loss: 0.0688 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0505 - val_loss: 0.0662 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0471 - val_loss: 0.0644 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0532 - val_loss: 0.0668 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 24 Validation MSE: 0.09031338267959654\n",
            "Step 25: Training on 29 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0365 - val_loss: 0.0277 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0442 - val_loss: 0.0344 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0487 - val_loss: 0.0427 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0667 - val_loss: 0.0466 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0467 - val_loss: 0.0465 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0343 - val_loss: 0.0458 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0302 - val_loss: 0.0423 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0501 - val_loss: 0.0446 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0413 - val_loss: 0.0469 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0475 - val_loss: 0.0473 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0743 - val_loss: 0.0391 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0388 - val_loss: 0.0346 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 25 Validation MSE: 0.04226683250404883\n",
            "Step 26: Training on 30 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0492 - val_loss: 0.0781 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0450 - val_loss: 0.0613 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0579 - val_loss: 0.0497 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0558 - val_loss: 0.0430 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0427 - val_loss: 0.0410 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0334 - val_loss: 0.0460 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0594 - val_loss: 0.0570 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0356 - val_loss: 0.0673 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0238 - val_loss: 0.0777 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0352 - val_loss: 0.0812 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0336 - val_loss: 0.0837 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0306 - val_loss: 0.0853 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0378 - val_loss: 0.0811 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0260 - val_loss: 0.0755 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 26 Validation MSE: 0.07765214599727266\n",
            "Step 27: Training on 31 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0695 - val_loss: 0.0159 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0401 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0399 - val_loss: 0.0099 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0667 - val_loss: 0.0119 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0447 - val_loss: 0.0151 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0370 - val_loss: 0.0188 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0388 - val_loss: 0.0202 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0405 - val_loss: 0.0182 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0370 - val_loss: 0.0153 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0448 - val_loss: 0.0135 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0318 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0470 - val_loss: 0.0130 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0375 - val_loss: 0.0148 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0420 - val_loss: 0.0165 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0507 - val_loss: 0.0184 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0405 - val_loss: 0.0221 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 27 Validation MSE: 0.011387882016622037\n",
            "Step 28: Training on 32 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0412 - val_loss: 0.0262 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0367 - val_loss: 0.0373 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0202 - val_loss: 0.0460 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0334 - val_loss: 0.0515 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0517 - val_loss: 0.0527 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0651 - val_loss: 0.0459 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0303 - val_loss: 0.0372 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0567 - val_loss: 0.0282 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 28 Validation MSE: 0.04598314037753783\n",
            "Step 29: Training on 33 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0288 - val_loss: 0.0668 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0347 - val_loss: 0.0472 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0159 - val_loss: 0.0343 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0495 - val_loss: 0.0300 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0287 - val_loss: 0.0287 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0695 - val_loss: 0.0372 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0375 - val_loss: 0.0489 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0340 - val_loss: 0.0623 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 29 Validation MSE: 0.0343501676377145\n",
            "Step 30: Training on 34 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0284 - val_loss: 0.1523 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0805 - val_loss: 0.1744 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0415 - val_loss: 0.1975 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0518 - val_loss: 0.2193 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0341 - val_loss: 0.2278 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0466 - val_loss: 0.2189 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 30 Validation MSE: 0.15228429651595674\n",
            "Step 31: Training on 35 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0488 - val_loss: 0.1711 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0468 - val_loss: 0.1926 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0482 - val_loss: 0.2174 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0490 - val_loss: 0.2582 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0381 - val_loss: 0.3027 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0459 - val_loss: 0.3336 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0492 - val_loss: 0.3402 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0562 - val_loss: 0.3200 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0347 - val_loss: 0.2863 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0317 - val_loss: 0.2477 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0377 - val_loss: 0.2149 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0274 - val_loss: 0.1923 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0267 - val_loss: 0.1836 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0214 - val_loss: 0.1739 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0283 - val_loss: 0.1755 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0341 - val_loss: 0.1842 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0375 - val_loss: 0.1940 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0288 - val_loss: 0.2037 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.2207 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 31 Validation MSE: 0.17386937732363847\n",
            "Step 32: Training on 36 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0500 - val_loss: 0.1905 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0428 - val_loss: 0.2124 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0456 - val_loss: 0.2395 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0495 - val_loss: 0.2553 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0260 - val_loss: 0.2592 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0285 - val_loss: 0.2540 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0333 - val_loss: 0.2525 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0366 - val_loss: 0.2390 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0378 - val_loss: 0.2265 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0352 - val_loss: 0.2039 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 32 Validation MSE: 0.2591558144822737\n",
            "Step 33: Training on 37 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0271 - val_loss: 0.1989 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0328 - val_loss: 0.1801 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0367 - val_loss: 0.1625 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0220 - val_loss: 0.1485 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0239 - val_loss: 0.1473 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0416 - val_loss: 0.1564 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0526 - val_loss: 0.1684 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0239 - val_loss: 0.1829 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0241 - val_loss: 0.1965 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 33 Validation MSE: 0.14851429048946835\n",
            "Step 34: Training on 38 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0466 - val_loss: 0.2054 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0338 - val_loss: 0.2316 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0269 - val_loss: 0.2530 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0366 - val_loss: 0.2677 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0314 - val_loss: 0.2732 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0332 - val_loss: 0.2757 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0339 - val_loss: 0.2689 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0329 - val_loss: 0.2498 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 34 Validation MSE: 0.2530208506026288\n",
            "Step 35: Training on 39 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0311 - val_loss: 0.2438 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0405 - val_loss: 0.2280 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0259 - val_loss: 0.2098 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0234 - val_loss: 0.2021 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0409 - val_loss: 0.2048 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0207 - val_loss: 0.2080 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0336 - val_loss: 0.2116 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0212 - val_loss: 0.2173 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0262 - val_loss: 0.2326 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0222 - val_loss: 0.2419 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0476 - val_loss: 0.2609 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 35 Validation MSE: 0.20797196053899825\n",
            "Step 36: Training on 40 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0227 - val_loss: 0.4003 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0292 - val_loss: 0.4296 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0173 - val_loss: 0.4486 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0414 - val_loss: 0.4474 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0320 - val_loss: 0.4345 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0358 - val_loss: 0.4311 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0279 - val_loss: 0.4219 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0280 - val_loss: 0.4204 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 36 Validation MSE: 0.4486060234875464\n",
            "Step 37: Training on 41 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0157 - val_loss: 0.5192 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0200 - val_loss: 0.5047 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0227 - val_loss: 0.4877 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0322 - val_loss: 0.4633 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0345 - val_loss: 0.4521 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0494 - val_loss: 0.4549 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 37 Validation MSE: 0.5192003330748778\n",
            "Step 38: Training on 42 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0240 - val_loss: 0.3578 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0245 - val_loss: 0.3528 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0367 - val_loss: 0.3355 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0365 - val_loss: 0.3252 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0222 - val_loss: 0.3119 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0245 - val_loss: 0.3054 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0588 - val_loss: 0.3041 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0231 - val_loss: 0.3053 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0233 - val_loss: 0.3123 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0370 - val_loss: 0.3215 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 38 Validation MSE: 0.311917034537179\n",
            "Step 39: Training on 43 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0551 - val_loss: 0.1358 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0253 - val_loss: 0.1335 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0239 - val_loss: 0.1324 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0233 - val_loss: 0.1308 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0258 - val_loss: 0.1331 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0184 - val_loss: 0.1325 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0221 - val_loss: 0.1350 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0359 - val_loss: 0.1422 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0426 - val_loss: 0.1449 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0232 - val_loss: 0.1432 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0248 - val_loss: 0.1340 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 39 Validation MSE: 0.13254294824993787\n",
            "Step 40: Training on 44 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0239 - val_loss: 0.1184 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0250 - val_loss: 0.1142 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0237 - val_loss: 0.1125 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0252 - val_loss: 0.1130 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0281 - val_loss: 0.1199 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0210 - val_loss: 0.1237 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0254 - val_loss: 0.1276 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0263 - val_loss: 0.1273 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0268 - val_loss: 0.1267 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0295 - val_loss: 0.1215 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0333 - val_loss: 0.1195 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 40 Validation MSE: 0.123734417439996\n",
            "Step 41: Training on 45 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0313 - val_loss: 0.0568 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0237 - val_loss: 0.0558 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0188 - val_loss: 0.0548 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0234 - val_loss: 0.0543 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0346 - val_loss: 0.0587 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0216 - val_loss: 0.0604 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0339 - val_loss: 0.0659 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0221 - val_loss: 0.0727 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 41 Validation MSE: 0.0548013268813725\n",
            "Step 42: Training on 46 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0243 - val_loss: 0.1459 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0284 - val_loss: 0.1540 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0219 - val_loss: 0.1596 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0302 - val_loss: 0.1646 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0342 - val_loss: 0.1652 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0367 - val_loss: 0.1677 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0307 - val_loss: 0.1641 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0249 - val_loss: 0.1628 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 42 Validation MSE: 0.15955365644677144\n",
            "Mean validation error: 0.20518091150275422\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0286 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0255 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0284 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0299 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0185 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0279 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0312 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0275 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0308 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0333 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "starting prediction for 2023-02-21\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "X sequenced dim: (46, 5, 15)\n",
            "y dim: (46,)\n",
            "Step 1: Training on 5 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 11s 11s/step - loss: 1.1731 - val_loss: 6.9217 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 6.8934 - val_loss: 0.7647 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9581 - val_loss: 0.0315 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 2.5421 - val_loss: 0.2962 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.5398 - val_loss: 0.1373 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.3323 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9274 - val_loss: 0.4149 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.7687 - val_loss: 0.5966 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.9075 - val_loss: 0.5852 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.5242 - val_loss: 0.2866 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 2.1450 - val_loss: 0.4589 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1862 - val_loss: 0.4633 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.7674 - val_loss: 0.6426 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4974 - val_loss: 0.4828 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.9127 - val_loss: 0.1464 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2630 - val_loss: 0.0101 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.7770 - val_loss: 0.0021 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.0558 - val_loss: 6.7846e-04 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5389 - val_loss: 0.0828 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.8260 - val_loss: 0.1860 - lr: 0.0010\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Step 1 Validation MSE: 0.1860400025100413\n",
            "Step 2: Training on 6 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.9462 - val_loss: 0.9805 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.0034 - val_loss: 0.7512 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1971 - val_loss: 0.6257 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.9293 - val_loss: 0.5515 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1241 - val_loss: 0.4956 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.0635 - val_loss: 0.3408 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5172 - val_loss: 0.1721 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.6999 - val_loss: 0.0831 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.8136 - val_loss: 0.0322 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5144 - val_loss: 0.0094 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 2 Validation MSE: 0.4956195664126611\n",
            "Step 3: Training on 7 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.2461 - val_loss: 1.0513 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.0010 - val_loss: 0.7037 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3829 - val_loss: 0.5006 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.5299 - val_loss: 0.3648 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4795 - val_loss: 0.2832 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.4454 - val_loss: 0.2646 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 3 Validation MSE: 1.0513060872137756\n",
            "Step 4: Training on 8 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3381 - val_loss: 0.5845 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6953 - val_loss: 0.3496 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5967 - val_loss: 0.1935 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2694 - val_loss: 0.1168 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.8932 - val_loss: 0.0606 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5850 - val_loss: 0.0620 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.8016 - val_loss: 0.1237 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5057 - val_loss: 0.2277 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.5632 - val_loss: 0.3621 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 4 Validation MSE: 0.11681884535588227\n",
            "Step 5: Training on 9 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.5366 - val_loss: 0.1079 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.4644 - val_loss: 0.2444 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3443 - val_loss: 0.3862 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2794 - val_loss: 0.4985 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3709 - val_loss: 0.4880 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.5254 - val_loss: 0.3932 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2257 - val_loss: 0.2565 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.5220 - val_loss: 0.1533 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2293 - val_loss: 0.0807 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4261 - val_loss: 0.0429 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1903 - val_loss: 0.0206 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3891 - val_loss: 0.0150 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2505 - val_loss: 0.0252 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2756 - val_loss: 0.0553 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4082 - val_loss: 0.1263 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1581 - val_loss: 0.2223 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1552 - val_loss: 0.2936 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1768 - val_loss: 0.3640 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5417 - val_loss: 0.4519 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5471 - val_loss: 0.4118 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Step 5 Validation MSE: 0.4117536046508729\n",
            "Step 6: Training on 10 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3231 - val_loss: 0.4738 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4691 - val_loss: 0.3741 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1461 - val_loss: 0.3020 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2310 - val_loss: 0.2250 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2245 - val_loss: 0.1602 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1773 - val_loss: 0.1128 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2336 - val_loss: 0.0969 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.6296 - val_loss: 0.1009 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 6 Validation MSE: 0.30200555804392915\n",
            "Step 7: Training on 11 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.1990 - val_loss: 0.3377 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4555 - val_loss: 0.3425 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2691 - val_loss: 0.3755 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3297 - val_loss: 0.3893 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2649 - val_loss: 0.3335 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1433 - val_loss: 0.2934 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2643 - val_loss: 0.2364 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2565 - val_loss: 0.2162 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4189 - val_loss: 0.2181 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1824 - val_loss: 0.2272 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.1718 - val_loss: 0.2453 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 7 Validation MSE: 0.2934101062808397\n",
            "Step 8: Training on 12 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.2256 - val_loss: 0.2566 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1011 - val_loss: 0.2664 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0933 - val_loss: 0.2566 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2609 - val_loss: 0.2344 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4049 - val_loss: 0.2350 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0816 - val_loss: 0.2353 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1983 - val_loss: 0.2593 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2058 - val_loss: 0.3077 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2756 - val_loss: 0.3523 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3573 - val_loss: 0.3729 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.4275 - val_loss: 0.3517 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 8 Validation MSE: 0.23534845674643018\n",
            "Step 9: Training on 13 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3233 - val_loss: 0.2438 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3446 - val_loss: 0.2921 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3891 - val_loss: 0.3048 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2991 - val_loss: 0.2820 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2969 - val_loss: 0.2720 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.1827 - val_loss: 0.2375 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3559 - val_loss: 0.1913 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.1442 - val_loss: 0.1524 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2214 - val_loss: 0.1160 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3632 - val_loss: 0.0870 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3561 - val_loss: 0.0542 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3227 - val_loss: 0.0372 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1749 - val_loss: 0.0325 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 9 Validation MSE: 0.15244773270041018\n",
            "Step 10: Training on 14 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.2424 - val_loss: 0.1660 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1954 - val_loss: 0.1796 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2031 - val_loss: 0.1872 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3007 - val_loss: 0.1944 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1846 - val_loss: 0.1746 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1728 - val_loss: 0.1503 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2822 - val_loss: 0.1232 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1754 - val_loss: 0.1010 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2352 - val_loss: 0.1052 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0953 - val_loss: 0.1094 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0936 - val_loss: 0.1177 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1617 - val_loss: 0.1125 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1016 - val_loss: 0.1054 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2366 - val_loss: 0.1001 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1525 - val_loss: 0.0919 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.2015 - val_loss: 0.0899 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 10 Validation MSE: 0.1176499630146654\n",
            "Step 11: Training on 15 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.2168 - val_loss: 0.0594 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1554 - val_loss: 0.0549 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2002 - val_loss: 0.0546 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2275 - val_loss: 0.0489 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1586 - val_loss: 0.0404 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1924 - val_loss: 0.0431 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.2577 - val_loss: 0.0537 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 11 Validation MSE: 0.05491576309511292\n",
            "Step 12: Training on 16 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.2972 - val_loss: 0.2255 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1336 - val_loss: 0.2972 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1859 - val_loss: 0.3477 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2041 - val_loss: 0.3838 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2513 - val_loss: 0.3790 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3056 - val_loss: 0.3062 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1156 - val_loss: 0.2538 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1541 - val_loss: 0.1964 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1902 - val_loss: 0.1716 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2292 - val_loss: 0.1363 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2007 - val_loss: 0.1218 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0731 - val_loss: 0.1135 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1190 - val_loss: 0.1119 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2296 - val_loss: 0.1239 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1128 - val_loss: 0.1547 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1957 - val_loss: 0.2058 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1700 - val_loss: 0.2615 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 12 Validation MSE: 0.11347777491844961\n",
            "Step 13: Training on 17 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.2168 - val_loss: 0.1693 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0527 - val_loss: 0.2452 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1179 - val_loss: 0.3038 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1375 - val_loss: 0.3727 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0875 - val_loss: 0.4197 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1610 - val_loss: 0.4242 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1483 - val_loss: 0.3870 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 13 Validation MSE: 0.24519754318232054\n",
            "Step 14: Training on 18 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1530 - val_loss: 0.2344 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2040 - val_loss: 0.2163 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1121 - val_loss: 0.1985 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1020 - val_loss: 0.1829 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1004 - val_loss: 0.1713 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1088 - val_loss: 0.1585 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1958 - val_loss: 0.1676 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1857 - val_loss: 0.1770 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1153 - val_loss: 0.1906 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0825 - val_loss: 0.2016 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1199 - val_loss: 0.2271 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0989 - val_loss: 0.2505 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1922 - val_loss: 0.2805 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1041 - val_loss: 0.2907 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1467 - val_loss: 0.2993 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 14 Validation MSE: 0.20156463549008752\n",
            "Step 15: Training on 19 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.2028 - val_loss: 0.5255 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1966 - val_loss: 0.6265 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0766 - val_loss: 0.7285 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1576 - val_loss: 0.7695 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1146 - val_loss: 0.7459 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1276 - val_loss: 0.6763 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2945 - val_loss: 0.5306 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.1218 - val_loss: 0.4378 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 15 Validation MSE: 0.7284727729261824\n",
            "Step 16: Training on 20 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1736 - val_loss: 0.5047 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1020 - val_loss: 0.3784 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1280 - val_loss: 0.2712 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0816 - val_loss: 0.1998 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1310 - val_loss: 0.1602 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1631 - val_loss: 0.1543 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1170 - val_loss: 0.1701 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1536 - val_loss: 0.2148 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0889 - val_loss: 0.2647 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 16 Validation MSE: 0.19981020044125844\n",
            "Step 17: Training on 21 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.1482 - val_loss: 0.0632 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0709 - val_loss: 0.1021 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1444 - val_loss: 0.1517 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1202 - val_loss: 0.2088 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1924 - val_loss: 0.2330 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0921 - val_loss: 0.2371 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.1005 - val_loss: 0.2393 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 17 Validation MSE: 0.10212196632774813\n",
            "Step 18: Training on 22 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.1985 - val_loss: 0.0823 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1117 - val_loss: 0.0752 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1497 - val_loss: 0.0808 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1024 - val_loss: 0.0895 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0998 - val_loss: 0.1134 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1307 - val_loss: 0.1584 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1218 - val_loss: 0.1943 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0939 - val_loss: 0.2213 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1006 - val_loss: 0.2480 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1343 - val_loss: 0.2472 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1375 - val_loss: 0.2212 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1154 - val_loss: 0.1754 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.1609 - val_loss: 0.1386 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Step 18 Validation MSE: 0.22133739892393217\n",
            "Step 19: Training on 23 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0567 - val_loss: 0.3336 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1056 - val_loss: 0.2630 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1036 - val_loss: 0.2140 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1382 - val_loss: 0.1802 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1617 - val_loss: 0.1508 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.1009 - val_loss: 0.1270 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 19 Validation MSE: 0.3335774543616934\n",
            "Step 20: Training on 24 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.1595 - val_loss: 0.2375 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0950 - val_loss: 0.2032 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1033 - val_loss: 0.1761 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0778 - val_loss: 0.1478 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1286 - val_loss: 0.1350 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1979 - val_loss: 0.1458 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1679 - val_loss: 0.1690 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1296 - val_loss: 0.1979 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0820 - val_loss: 0.2275 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 20 Validation MSE: 0.14784304270728113\n",
            "Step 21: Training on 25 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0966 - val_loss: 0.1291 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0864 - val_loss: 0.1629 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0769 - val_loss: 0.2006 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0897 - val_loss: 0.2348 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0829 - val_loss: 0.2447 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0823 - val_loss: 0.2348 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1204 - val_loss: 0.1990 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.1132 - val_loss: 0.1598 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 21 Validation MSE: 0.20059535064557796\n",
            "Step 22: Training on 26 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0981 - val_loss: 0.1827 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1170 - val_loss: 0.1314 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0851 - val_loss: 0.0982 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1332 - val_loss: 0.0734 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1219 - val_loss: 0.0612 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1022 - val_loss: 0.0621 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1126 - val_loss: 0.0789 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0947 - val_loss: 0.1097 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Step 22 Validation MSE: 0.09815259899953363\n",
            "Step 23: Training on 27 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.1261 - val_loss: 0.2078 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0965 - val_loss: 0.2545 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1196 - val_loss: 0.2901 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0857 - val_loss: 0.3045 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1067 - val_loss: 0.3080 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0875 - val_loss: 0.2930 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1064 - val_loss: 0.2580 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1102 - val_loss: 0.2241 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0906 - val_loss: 0.1958 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 23 Validation MSE: 0.3045082506869221\n",
            "Step 24: Training on 28 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0523 - val_loss: 0.1031 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1470 - val_loss: 0.0850 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0876 - val_loss: 0.0683 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0772 - val_loss: 0.0584 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1340 - val_loss: 0.0598 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1015 - val_loss: 0.0617 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 24 Validation MSE: 0.10314804084608313\n",
            "Step 25: Training on 29 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0564 - val_loss: 0.1777 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1443 - val_loss: 0.1619 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0913 - val_loss: 0.1342 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0850 - val_loss: 0.1155 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0742 - val_loss: 0.0990 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0661 - val_loss: 0.0875 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 25 Validation MSE: 0.1777191869035608\n",
            "Step 26: Training on 30 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0882 - val_loss: 0.1465 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1148 - val_loss: 0.1270 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0636 - val_loss: 0.1144 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0845 - val_loss: 0.0921 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1261 - val_loss: 0.0799 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0724 - val_loss: 0.0791 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0482 - val_loss: 0.0820 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0976 - val_loss: 0.0875 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0892 - val_loss: 0.0955 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1085 - val_loss: 0.1004 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0691 - val_loss: 0.1156 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1056 - val_loss: 0.1265 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 26 Validation MSE: 0.08197121286988145\n",
            "Step 27: Training on 31 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.1452 - val_loss: 0.1794 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1119 - val_loss: 0.1949 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0532 - val_loss: 0.2125 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0671 - val_loss: 0.2230 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0856 - val_loss: 0.2271 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0838 - val_loss: 0.2355 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0877 - val_loss: 0.2249 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.1136 - val_loss: 0.2090 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 27 Validation MSE: 0.21250670653917\n",
            "Step 28: Training on 32 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0630 - val_loss: 0.0901 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0883 - val_loss: 0.0876 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0660 - val_loss: 0.0837 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0596 - val_loss: 0.0782 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1367 - val_loss: 0.0769 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0816 - val_loss: 0.0774 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0860 - val_loss: 0.0844 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0599 - val_loss: 0.0890 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0681 - val_loss: 0.0875 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 28 Validation MSE: 0.07817568558177493\n",
            "Step 29: Training on 33 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0602 - val_loss: 0.1803 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0701 - val_loss: 0.1880 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0885 - val_loss: 0.2007 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0740 - val_loss: 0.2127 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0767 - val_loss: 0.2223 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0678 - val_loss: 0.2390 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 29 Validation MSE: 0.18026487389588752\n",
            "Step 30: Training on 34 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.1026 - val_loss: 0.2609 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0863 - val_loss: 0.2960 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0987 - val_loss: 0.3536 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0506 - val_loss: 0.4158 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0811 - val_loss: 0.4687 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0893 - val_loss: 0.5017 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1215 - val_loss: 0.5172 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0614 - val_loss: 0.5160 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0749 - val_loss: 0.4859 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 30 Validation MSE: 0.4157593106218654\n",
            "Step 31: Training on 35 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0753 - val_loss: 0.3695 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0906 - val_loss: 0.3599 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1094 - val_loss: 0.3511 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0797 - val_loss: 0.3309 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0886 - val_loss: 0.3191 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0848 - val_loss: 0.3074 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 31 Validation MSE: 0.3694970637727786\n",
            "Step 32: Training on 36 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0860 - val_loss: 0.2540 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0924 - val_loss: 0.2208 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1128 - val_loss: 0.1805 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0742 - val_loss: 0.1619 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0865 - val_loss: 0.1509 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0977 - val_loss: 0.1557 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0746 - val_loss: 0.1725 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0535 - val_loss: 0.1965 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0743 - val_loss: 0.2367 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1360 - val_loss: 0.2584 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0753 - val_loss: 0.2793 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0769 - val_loss: 0.2832 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0729 - val_loss: 0.2807 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 32 Validation MSE: 0.19647919276750458\n",
            "Step 33: Training on 37 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0727 - val_loss: 0.0863 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0970 - val_loss: 0.0957 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0659 - val_loss: 0.1049 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0544 - val_loss: 0.1135 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1118 - val_loss: 0.1277 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0753 - val_loss: 0.1429 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0710 - val_loss: 0.1511 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0919 - val_loss: 0.1467 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0638 - val_loss: 0.1300 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 33 Validation MSE: 0.1134841487892892\n",
            "Step 34: Training on 38 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0707 - val_loss: 0.1805 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0514 - val_loss: 0.1538 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0946 - val_loss: 0.1317 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0520 - val_loss: 0.1166 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0661 - val_loss: 0.1089 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0705 - val_loss: 0.1050 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0671 - val_loss: 0.1062 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 34 Validation MSE: 0.15381821640767157\n",
            "Step 35: Training on 39 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0729 - val_loss: 0.2468 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0846 - val_loss: 0.2466 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0968 - val_loss: 0.2359 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0589 - val_loss: 0.2367 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0603 - val_loss: 0.2373 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0878 - val_loss: 0.2362 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0650 - val_loss: 0.2412 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0701 - val_loss: 0.2488 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0536 - val_loss: 0.2679 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0924 - val_loss: 0.2936 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0790 - val_loss: 0.3235 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0759 - val_loss: 0.3571 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0693 - val_loss: 0.3770 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0616 - val_loss: 0.3914 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 35 Validation MSE: 0.267946192898698\n",
            "Step 36: Training on 40 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0571 - val_loss: 0.2681 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0348 - val_loss: 0.2763 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0806 - val_loss: 0.2821 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0494 - val_loss: 0.2954 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0595 - val_loss: 0.3093 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0525 - val_loss: 0.3310 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0740 - val_loss: 0.3336 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 36 Validation MSE: 0.2763292838607228\n",
            "Step 37: Training on 41 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0453 - val_loss: 0.2609 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0583 - val_loss: 0.2595 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0779 - val_loss: 0.2471 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0598 - val_loss: 0.2398 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0852 - val_loss: 0.2282 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0701 - val_loss: 0.2257 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 37 Validation MSE: 0.260905123432746\n",
            "Step 38: Training on 42 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0692 - val_loss: 0.2023 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0769 - val_loss: 0.1928 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0881 - val_loss: 0.1888 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0743 - val_loss: 0.1822 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0725 - val_loss: 0.1828 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0693 - val_loss: 0.2024 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 38 Validation MSE: 0.2023438869111483\n",
            "Step 39: Training on 43 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0608 - val_loss: 0.1506 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0719 - val_loss: 0.1682 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0442 - val_loss: 0.1873 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0821 - val_loss: 0.1962 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0467 - val_loss: 0.1978 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0417 - val_loss: 0.1964 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0510 - val_loss: 0.1886 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0790 - val_loss: 0.1709 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0672 - val_loss: 0.1529 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0368 - val_loss: 0.1289 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0623 - val_loss: 0.1086 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0471 - val_loss: 0.0957 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0518 - val_loss: 0.0863 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0820 - val_loss: 0.0868 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0522 - val_loss: 0.0963 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 39 Validation MSE: 0.1289106481282261\n",
            "Step 40: Training on 44 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0703 - val_loss: 0.0328 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0505 - val_loss: 0.0377 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0873 - val_loss: 0.0377 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0700 - val_loss: 0.0367 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0462 - val_loss: 0.0382 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0647 - val_loss: 0.0405 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0720 - val_loss: 0.0402 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0524 - val_loss: 0.0397 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0592 - val_loss: 0.0369 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0940 - val_loss: 0.0356 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 40 Validation MSE: 0.03818435392177209\n",
            "Step 41: Training on 45 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0627 - val_loss: 0.0790 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0572 - val_loss: 0.0724 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0677 - val_loss: 0.0654 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0532 - val_loss: 0.0632 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0639 - val_loss: 0.0589 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0505 - val_loss: 0.0535 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0537 - val_loss: 0.0537 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0419 - val_loss: 0.0566 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0635 - val_loss: 0.0662 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0779 - val_loss: 0.0825 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0647 - val_loss: 0.0938 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0547 - val_loss: 0.1011 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0493 - val_loss: 0.1028 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 41 Validation MSE: 0.056594926038780656\n",
            "Step 42: Training on 46 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0682 - val_loss: 0.1478 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0507 - val_loss: 0.1723 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0653 - val_loss: 0.2046 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0757 - val_loss: 0.2303 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0555 - val_loss: 0.2462 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0493 - val_loss: 0.2500 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0782 - val_loss: 0.2408 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0727 - val_loss: 0.2245 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0441 - val_loss: 0.2005 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0498 - val_loss: 0.1739 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0412 - val_loss: 0.1577 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0710 - val_loss: 0.1447 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0691 - val_loss: 0.1381 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0567 - val_loss: 0.1387 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0312 - val_loss: 0.1454 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0713 - val_loss: 0.1531 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0382 - val_loss: 0.1662 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0450 - val_loss: 0.1773 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0464 - val_loss: 0.1911 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0540 - val_loss: 0.2064 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 42 Validation MSE: 0.14544011701459486\n",
            "Mean validation error: 0.23270125825804194\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0466 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0560 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0519 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0643 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0477 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0553 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "starting prediction for 2023-02-22\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "X sequenced dim: (46, 5, 15)\n",
            "y dim: (46,)\n",
            "Step 1: Training on 5 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 11s 11s/step - loss: 8.3137 - val_loss: 0.6493 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.8262 - val_loss: 2.9536 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 2.6961 - val_loss: 1.6537 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.0689 - val_loss: 0.3496 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.0512 - val_loss: 0.0590 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.2506 - val_loss: 0.0012 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.5672 - val_loss: 0.0168 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.1125 - val_loss: 0.1101 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4408 - val_loss: 0.3805 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.3290 - val_loss: 0.7948 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.9611 - val_loss: 1.3934 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.0247 - val_loss: 2.1782 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.8897 - val_loss: 2.3822 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.0844 - val_loss: 2.0895 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.2936 - val_loss: 1.7185 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.8537 - val_loss: 1.0396 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3193 - val_loss: 0.5757 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4629 - val_loss: 0.2967 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5552 - val_loss: 0.2776 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.3787 - val_loss: 0.2034 - lr: 0.0010\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Step 1 Validation MSE: 1.718463514649207\n",
            "Step 2: Training on 6 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.5749 - val_loss: 1.3369 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3845 - val_loss: 1.1457 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.2071 - val_loss: 0.5880 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2597 - val_loss: 0.2611 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6908 - val_loss: 0.2059 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6240 - val_loss: 0.2882 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1828 - val_loss: 0.3548 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5951 - val_loss: 0.3808 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3343 - val_loss: 0.4540 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.8656 - val_loss: 0.3529 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4495 - val_loss: 0.2703 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0972 - val_loss: 0.2146 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3140 - val_loss: 0.2367 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3452 - val_loss: 0.2294 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2936 - val_loss: 0.2335 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2003 - val_loss: 0.2461 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0899 - val_loss: 0.2590 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2768 - val_loss: 0.2331 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7038 - val_loss: 0.1431 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5156 - val_loss: 0.1241 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 2 Validation MSE: 0.12405978917351378\n",
            "Step 3: Training on 7 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0782 - val_loss: 0.1112 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2271 - val_loss: 0.1090 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2905 - val_loss: 0.1195 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2478 - val_loss: 0.1056 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2926 - val_loss: 0.0937 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.4290 - val_loss: 0.0763 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 3 Validation MSE: 0.11118555255758016\n",
            "Step 4: Training on 8 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3608 - val_loss: 0.1655 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5734 - val_loss: 0.2074 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4153 - val_loss: 0.2189 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3062 - val_loss: 0.2272 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0732 - val_loss: 0.2342 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3769 - val_loss: 0.1938 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4367 - val_loss: 0.1205 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4067 - val_loss: 0.0752 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2519 - val_loss: 0.0488 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3073 - val_loss: 0.0428 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 4 Validation MSE: 0.2342484080204995\n",
            "Step 5: Training on 9 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.1606 - val_loss: 0.1429 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5499 - val_loss: 0.0817 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4219 - val_loss: 0.0515 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1284 - val_loss: 0.0368 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2639 - val_loss: 0.0352 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1356 - val_loss: 0.0345 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4408 - val_loss: 0.0321 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3879 - val_loss: 0.0281 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2223 - val_loss: 0.0240 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 5 Validation MSE: 0.03679031420240688\n",
            "Step 6: Training on 10 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.1703 - val_loss: 0.1364 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4124 - val_loss: 0.1060 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1704 - val_loss: 0.0991 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4354 - val_loss: 0.1322 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2356 - val_loss: 0.1564 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.4308 - val_loss: 0.2028 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 6 Validation MSE: 0.13635336049357283\n",
            "Step 7: Training on 11 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3612 - val_loss: 0.1142 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1866 - val_loss: 0.0976 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2605 - val_loss: 0.0911 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0680 - val_loss: 0.0788 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2483 - val_loss: 0.0517 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1699 - val_loss: 0.0313 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1598 - val_loss: 0.0192 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2152 - val_loss: 0.0215 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.3175 - val_loss: 0.0190 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 7 Validation MSE: 0.07884685073280548\n",
            "Step 8: Training on 12 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.1195 - val_loss: 0.0610 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1900 - val_loss: 0.0823 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2853 - val_loss: 0.1043 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1978 - val_loss: 0.1580 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2213 - val_loss: 0.2417 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3188 - val_loss: 0.3101 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 8 Validation MSE: 0.06102104606575035\n",
            "Step 9: Training on 13 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.1715 - val_loss: 0.0235 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1904 - val_loss: 0.0527 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2708 - val_loss: 0.0785 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2509 - val_loss: 0.0858 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2512 - val_loss: 0.0831 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1795 - val_loss: 0.0958 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 9 Validation MSE: 0.023518536284938194\n",
            "Step 10: Training on 14 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.2211 - val_loss: 0.0783 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3087 - val_loss: 0.0694 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2358 - val_loss: 0.0623 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2492 - val_loss: 0.0770 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2545 - val_loss: 0.0903 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.2251 - val_loss: 0.1017 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 10 Validation MSE: 0.07833763447785574\n",
            "Step 11: Training on 15 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.1975 - val_loss: 0.1819 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1461 - val_loss: 0.1921 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2972 - val_loss: 0.1654 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2242 - val_loss: 0.1159 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2880 - val_loss: 0.0804 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3978 - val_loss: 0.0927 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.2716 - val_loss: 0.1067 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 11 Validation MSE: 0.19209794433265626\n",
            "Step 12: Training on 16 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.2110 - val_loss: 0.1279 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5191 - val_loss: 0.1647 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1241 - val_loss: 0.2173 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2045 - val_loss: 0.2336 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3509 - val_loss: 0.2090 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2329 - val_loss: 0.1716 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1581 - val_loss: 0.1298 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.3059 - val_loss: 0.0891 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Step 12 Validation MSE: 0.2172763848438891\n",
            "Step 13: Training on 17 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.2606 - val_loss: 0.1134 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1290 - val_loss: 0.0691 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1946 - val_loss: 0.0331 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2415 - val_loss: 0.0134 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1877 - val_loss: 0.0062 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1974 - val_loss: 0.0057 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.2802 - val_loss: 0.0142 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 13 Validation MSE: 0.06908909895459477\n",
            "Step 14: Training on 18 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.2255 - val_loss: 0.1248 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.2009 - val_loss: 0.1208 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2092 - val_loss: 0.1212 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2614 - val_loss: 0.1141 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1695 - val_loss: 0.0995 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2273 - val_loss: 0.0946 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1193 - val_loss: 0.0935 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1544 - val_loss: 0.1022 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1651 - val_loss: 0.1058 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1349 - val_loss: 0.1108 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1263 - val_loss: 0.1335 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1014 - val_loss: 0.1596 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0942 - val_loss: 0.1670 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1641 - val_loss: 0.1928 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2641 - val_loss: 0.1615 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1287 - val_loss: 0.1307 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2379 - val_loss: 0.1188 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2030 - val_loss: 0.1048 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 14 Validation MSE: 0.16696169615650877\n",
            "Step 15: Training on 19 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.2558 - val_loss: 0.1922 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1831 - val_loss: 0.1730 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1365 - val_loss: 0.1614 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1265 - val_loss: 0.1449 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0470 - val_loss: 0.1382 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2314 - val_loss: 0.1246 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2615 - val_loss: 0.0869 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.1246 - val_loss: 0.0700 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0805 - val_loss: 0.0513 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1266 - val_loss: 0.0457 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 15 Validation MSE: 0.13819150521999882\n",
            "Step 16: Training on 20 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.1461 - val_loss: 0.0508 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1130 - val_loss: 0.0446 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0773 - val_loss: 0.0501 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1421 - val_loss: 0.0488 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0678 - val_loss: 0.0531 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2609 - val_loss: 0.0648 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0977 - val_loss: 0.0721 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1815 - val_loss: 0.0836 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1241 - val_loss: 0.0756 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.2244 - val_loss: 0.0628 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 16 Validation MSE: 0.053054531635342586\n",
            "Step 17: Training on 21 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1845 - val_loss: 0.0551 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0821 - val_loss: 0.0600 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1107 - val_loss: 0.0640 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1701 - val_loss: 0.0733 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1429 - val_loss: 0.0756 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1836 - val_loss: 0.0869 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1548 - val_loss: 0.0940 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 17 Validation MSE: 0.05995458914199774\n",
            "Step 18: Training on 22 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.1245 - val_loss: 0.0683 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0882 - val_loss: 0.0778 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1492 - val_loss: 0.0779 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1145 - val_loss: 0.0841 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1511 - val_loss: 0.0924 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1339 - val_loss: 0.0835 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1235 - val_loss: 0.0740 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 18 Validation MSE: 0.07782002192139689\n",
            "Step 19: Training on 23 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1365 - val_loss: 0.1726 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1949 - val_loss: 0.1809 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1371 - val_loss: 0.1861 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1326 - val_loss: 0.1777 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1178 - val_loss: 0.1716 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1595 - val_loss: 0.1674 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0994 - val_loss: 0.1684 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1456 - val_loss: 0.1718 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0909 - val_loss: 0.1609 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1065 - val_loss: 0.1444 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1035 - val_loss: 0.1423 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1024 - val_loss: 0.1309 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0820 - val_loss: 0.1218 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1158 - val_loss: 0.1145 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0843 - val_loss: 0.0985 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1961 - val_loss: 0.0935 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1587 - val_loss: 0.0841 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1354 - val_loss: 0.0792 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 19 Validation MSE: 0.12175590938630128\n",
            "Step 20: Training on 24 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.1222 - val_loss: 0.0982 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1602 - val_loss: 0.0979 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1123 - val_loss: 0.1016 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1594 - val_loss: 0.1157 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0838 - val_loss: 0.1274 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0789 - val_loss: 0.1295 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1213 - val_loss: 0.1464 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1117 - val_loss: 0.1601 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0591 - val_loss: 0.1720 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0949 - val_loss: 0.1913 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0677 - val_loss: 0.1947 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1362 - val_loss: 0.1825 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0778 - val_loss: 0.1652 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.1070 - val_loss: 0.1469 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 20 Validation MSE: 0.17202546066194985\n",
            "Step 21: Training on 25 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1055 - val_loss: 0.1644 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1107 - val_loss: 0.1574 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0712 - val_loss: 0.1370 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0566 - val_loss: 0.1203 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1178 - val_loss: 0.1059 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0614 - val_loss: 0.0933 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0932 - val_loss: 0.0844 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0475 - val_loss: 0.0789 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0891 - val_loss: 0.0805 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1816 - val_loss: 0.0970 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1242 - val_loss: 0.1352 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0904 - val_loss: 0.1793 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0867 - val_loss: 0.2180 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 21 Validation MSE: 0.07891053302979546\n",
            "Step 22: Training on 26 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.1406 - val_loss: 0.0944 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0947 - val_loss: 0.1273 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0903 - val_loss: 0.1580 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0867 - val_loss: 0.1850 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0897 - val_loss: 0.1913 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0970 - val_loss: 0.1925 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1026 - val_loss: 0.1886 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1578 - val_loss: 0.1866 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0986 - val_loss: 0.1845 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 22 Validation MSE: 0.1849888503929833\n",
            "Step 23: Training on 27 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0919 - val_loss: 0.0868 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0820 - val_loss: 0.0666 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1114 - val_loss: 0.0473 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0934 - val_loss: 0.0380 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1199 - val_loss: 0.0291 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1308 - val_loss: 0.0274 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1169 - val_loss: 0.0292 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 23 Validation MSE: 0.06655370181435705\n",
            "Step 24: Training on 28 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1017 - val_loss: 0.0815 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1282 - val_loss: 0.0733 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0485 - val_loss: 0.0680 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0972 - val_loss: 0.0664 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0974 - val_loss: 0.0625 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0819 - val_loss: 0.0603 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0624 - val_loss: 0.0606 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0677 - val_loss: 0.0654 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 24 Validation MSE: 0.06801115796202868\n",
            "Step 25: Training on 29 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0705 - val_loss: 0.1732 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0879 - val_loss: 0.1783 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1304 - val_loss: 0.1713 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0948 - val_loss: 0.1615 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0710 - val_loss: 0.1543 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0669 - val_loss: 0.1473 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0568 - val_loss: 0.1480 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0980 - val_loss: 0.1478 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0797 - val_loss: 0.1561 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0744 - val_loss: 0.1724 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0511 - val_loss: 0.1906 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0819 - val_loss: 0.2182 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0520 - val_loss: 0.2260 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1480 - val_loss: 0.2136 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0763 - val_loss: 0.2043 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0737 - val_loss: 0.1940 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 25 Validation MSE: 0.19061412918662946\n",
            "Step 26: Training on 30 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0529 - val_loss: 0.2493 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0581 - val_loss: 0.2345 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0679 - val_loss: 0.2279 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0864 - val_loss: 0.2383 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0737 - val_loss: 0.2511 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0690 - val_loss: 0.2752 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 26 Validation MSE: 0.24933028310686464\n",
            "Step 27: Training on 31 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.1008 - val_loss: 0.3219 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0479 - val_loss: 0.3402 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1218 - val_loss: 0.3378 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0922 - val_loss: 0.3316 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0799 - val_loss: 0.3341 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0672 - val_loss: 0.3211 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0942 - val_loss: 0.3150 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 27 Validation MSE: 0.34022395876611217\n",
            "Step 28: Training on 32 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0855 - val_loss: 0.3045 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0577 - val_loss: 0.2800 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0685 - val_loss: 0.2613 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0700 - val_loss: 0.2620 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0732 - val_loss: 0.2722 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0819 - val_loss: 0.2861 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0565 - val_loss: 0.2981 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0655 - val_loss: 0.3128 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1006 - val_loss: 0.3396 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0569 - val_loss: 0.3610 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0776 - val_loss: 0.3611 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0470 - val_loss: 0.3538 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0705 - val_loss: 0.3380 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1121 - val_loss: 0.3081 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0491 - val_loss: 0.2825 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0577 - val_loss: 0.2700 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0504 - val_loss: 0.2560 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 28 Validation MSE: 0.3538134913781894\n",
            "Step 29: Training on 33 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0764 - val_loss: 0.2244 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0565 - val_loss: 0.2296 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0420 - val_loss: 0.2329 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0603 - val_loss: 0.2508 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0607 - val_loss: 0.2572 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0712 - val_loss: 0.2590 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0697 - val_loss: 0.2463 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0522 - val_loss: 0.2408 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 29 Validation MSE: 0.2328666253128119\n",
            "Step 30: Training on 34 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0837 - val_loss: 0.1519 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0545 - val_loss: 0.1636 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0493 - val_loss: 0.1663 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0599 - val_loss: 0.1650 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0859 - val_loss: 0.1612 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0855 - val_loss: 0.1598 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0711 - val_loss: 0.1510 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0587 - val_loss: 0.1371 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 30 Validation MSE: 0.1662704654642397\n",
            "Step 31: Training on 35 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0728 - val_loss: 0.2246 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0718 - val_loss: 0.1889 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0667 - val_loss: 0.1652 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0754 - val_loss: 0.1558 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0525 - val_loss: 0.1581 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0846 - val_loss: 0.1851 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0748 - val_loss: 0.2322 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0627 - val_loss: 0.2720 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0799 - val_loss: 0.2960 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0730 - val_loss: 0.2972 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 31 Validation MSE: 0.15808098417669783\n",
            "Step 32: Training on 36 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0757 - val_loss: 0.1524 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0554 - val_loss: 0.1663 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0831 - val_loss: 0.1823 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0688 - val_loss: 0.1961 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0425 - val_loss: 0.2109 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0459 - val_loss: 0.2324 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0467 - val_loss: 0.2447 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0696 - val_loss: 0.2560 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0550 - val_loss: 0.2604 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0603 - val_loss: 0.2439 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 32 Validation MSE: 0.21086462318359253\n",
            "Step 33: Training on 37 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0764 - val_loss: 0.4584 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0556 - val_loss: 0.4548 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0439 - val_loss: 0.4592 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0579 - val_loss: 0.4768 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0544 - val_loss: 0.4749 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0531 - val_loss: 0.4583 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0643 - val_loss: 0.4636 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0393 - val_loss: 0.4825 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0644 - val_loss: 0.4963 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0365 - val_loss: 0.5175 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0581 - val_loss: 0.5131 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0736 - val_loss: 0.5019 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0466 - val_loss: 0.5077 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0912 - val_loss: 0.5038 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0599 - val_loss: 0.4809 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 33 Validation MSE: 0.5175122456275858\n",
            "Step 34: Training on 38 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0532 - val_loss: 0.3117 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0468 - val_loss: 0.2988 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0563 - val_loss: 0.2839 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0503 - val_loss: 0.2647 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0353 - val_loss: 0.2435 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0547 - val_loss: 0.2311 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0510 - val_loss: 0.2296 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0277 - val_loss: 0.2340 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0272 - val_loss: 0.2410 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0525 - val_loss: 0.2527 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0306 - val_loss: 0.2652 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0409 - val_loss: 0.2665 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0467 - val_loss: 0.2679 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0399 - val_loss: 0.2686 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 34 Validation MSE: 0.24099955314481966\n",
            "Step 35: Training on 39 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0424 - val_loss: 0.3734 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0734 - val_loss: 0.3969 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0528 - val_loss: 0.4294 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0409 - val_loss: 0.4480 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0341 - val_loss: 0.4563 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0386 - val_loss: 0.4607 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0340 - val_loss: 0.4492 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0359 - val_loss: 0.4237 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0291 - val_loss: 0.3985 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0429 - val_loss: 0.3792 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0437 - val_loss: 0.3524 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0659 - val_loss: 0.3294 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0406 - val_loss: 0.3231 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0449 - val_loss: 0.3254 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 35 Validation MSE: 0.39847617721567075\n",
            "Step 36: Training on 40 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0559 - val_loss: 0.3938 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0347 - val_loss: 0.4002 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0399 - val_loss: 0.3972 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0397 - val_loss: 0.3842 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0465 - val_loss: 0.3738 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0439 - val_loss: 0.3694 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0389 - val_loss: 0.3577 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 36 Validation MSE: 0.4002305693697381\n",
            "Step 37: Training on 41 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0416 - val_loss: 0.2298 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0394 - val_loss: 0.2297 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0377 - val_loss: 0.2253 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0555 - val_loss: 0.2199 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0579 - val_loss: 0.2190 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0412 - val_loss: 0.2174 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0429 - val_loss: 0.2207 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0324 - val_loss: 0.2176 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0364 - val_loss: 0.2237 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0452 - val_loss: 0.2270 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0342 - val_loss: 0.2293 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0329 - val_loss: 0.2302 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0495 - val_loss: 0.2204 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 37 Validation MSE: 0.21755514454769542\n",
            "Step 38: Training on 42 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0590 - val_loss: 0.1535 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0506 - val_loss: 0.1429 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0419 - val_loss: 0.1334 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0617 - val_loss: 0.1290 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0505 - val_loss: 0.1319 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0469 - val_loss: 0.1422 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0351 - val_loss: 0.1549 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0458 - val_loss: 0.1611 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0383 - val_loss: 0.1713 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0388 - val_loss: 0.1765 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0507 - val_loss: 0.1887 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0507 - val_loss: 0.1857 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 38 Validation MSE: 0.15487484323719847\n",
            "Step 39: Training on 43 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0385 - val_loss: 0.0765 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0262 - val_loss: 0.0812 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0374 - val_loss: 0.0895 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0418 - val_loss: 0.0933 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0231 - val_loss: 0.0953 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0468 - val_loss: 0.0950 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0382 - val_loss: 0.0915 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0472 - val_loss: 0.0894 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0240 - val_loss: 0.0822 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0439 - val_loss: 0.0767 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 39 Validation MSE: 0.09527738442653808\n",
            "Step 40: Training on 44 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0498 - val_loss: 0.1541 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0336 - val_loss: 0.1361 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0441 - val_loss: 0.1256 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0464 - val_loss: 0.1111 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0589 - val_loss: 0.1133 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0305 - val_loss: 0.1210 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0502 - val_loss: 0.1377 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0380 - val_loss: 0.1522 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0339 - val_loss: 0.1642 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0418 - val_loss: 0.1755 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0288 - val_loss: 0.1861 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0413 - val_loss: 0.1859 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0277 - val_loss: 0.1742 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0309 - val_loss: 0.1647 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0353 - val_loss: 0.1516 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0506 - val_loss: 0.1340 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0489 - val_loss: 0.1217 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0474 - val_loss: 0.1106 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 40 Validation MSE: 0.1741696922838695\n",
            "Step 41: Training on 45 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0282 - val_loss: 0.2705 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0279 - val_loss: 0.2543 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0388 - val_loss: 0.2371 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0290 - val_loss: 0.2275 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0420 - val_loss: 0.2156 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0354 - val_loss: 0.2150 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0428 - val_loss: 0.2233 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 41 Validation MSE: 0.2542977939293714\n",
            "Step 42: Training on 46 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0426 - val_loss: 0.2800 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0338 - val_loss: 0.2915 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0253 - val_loss: 0.3040 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0513 - val_loss: 0.3083 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0321 - val_loss: 0.3108 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0302 - val_loss: 0.3031 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0241 - val_loss: 0.2921 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0353 - val_loss: 0.2686 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0300 - val_loss: 0.2470 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0267 - val_loss: 0.2331 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0364 - val_loss: 0.2259 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0321 - val_loss: 0.2230 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 42 Validation MSE: 0.29205666058222046\n",
            "Mean validation error: 0.2123102623107566\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0232 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0309 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0372 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0357 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0288 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0304 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "starting prediction for 2023-02-23\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "X sequenced dim: (46, 5, 15)\n",
            "y dim: (46,)\n",
            "Step 1: Training on 5 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 11s 11s/step - loss: 1.2017 - val_loss: 0.0362 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 8.2541 - val_loss: 2.7412 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 4.1343 - val_loss: 11.7074 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.4790 - val_loss: 16.6630 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 9.3723 - val_loss: 9.1835 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 6.3361 - val_loss: 3.3323 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0524 - val_loss: 0.5946 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6099 - val_loss: 0.0576 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.7773 - val_loss: 0.0415 - lr: 0.0010\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Step 1 Validation MSE: 16.66303144697481\n",
            "Step 2: Training on 6 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 6.1985 - val_loss: 3.0624 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.2106 - val_loss: 0.1984 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3604 - val_loss: 0.1907 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 2.3583 - val_loss: 0.8187 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 3.5813 - val_loss: 0.9962 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.0134 - val_loss: 0.8406 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 5.3977 - val_loss: 0.2856 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.4038 - val_loss: 0.0027 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 2 Validation MSE: 0.19072726490209002\n",
            "Step 3: Training on 7 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 2.2963 - val_loss: 0.4441 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.8842 - val_loss: 1.2810 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.2266 - val_loss: 2.1373 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.5331 - val_loss: 2.8084 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.2573 - val_loss: 2.9616 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7296 - val_loss: 2.7303 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7846 - val_loss: 2.2332 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.9216 - val_loss: 1.6580 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.9099 - val_loss: 1.3425 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.7043 - val_loss: 1.1126 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.3090 - val_loss: 0.9972 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2814 - val_loss: 0.8116 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.2428 - val_loss: 0.7935 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.3396 - val_loss: 0.9011 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3476 - val_loss: 0.9514 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3838 - val_loss: 1.0214 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.3101 - val_loss: 1.1005 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 3 Validation MSE: 0.8115528370226076\n",
            "Step 4: Training on 8 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3653 - val_loss: 0.4014 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.0997 - val_loss: 0.5411 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.9563 - val_loss: 0.7738 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1762 - val_loss: 0.9800 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.4357 - val_loss: 0.9685 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4506 - val_loss: 0.9099 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3260 - val_loss: 0.8750 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.9681 - val_loss: 0.7996 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.6655 - val_loss: 0.7076 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 4 Validation MSE: 0.979969788870986\n",
            "Step 5: Training on 9 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.8420 - val_loss: 0.7292 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.7799 - val_loss: 0.5015 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.6782 - val_loss: 0.2977 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7831 - val_loss: 0.2239 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.8008 - val_loss: 0.2162 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.9687 - val_loss: 0.2864 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.6091 - val_loss: 0.4436 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3823 - val_loss: 0.6390 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3429 - val_loss: 0.8291 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4910 - val_loss: 1.0983 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7378 - val_loss: 1.2636 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.9234 - val_loss: 1.3618 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5376 - val_loss: 1.3587 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.6515 - val_loss: 1.3212 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 5 Validation MSE: 0.8291375715080845\n",
            "Step 6: Training on 10 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.8248 - val_loss: 0.1209 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.7641 - val_loss: 0.0752 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.6280 - val_loss: 0.0943 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7372 - val_loss: 0.0786 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.5454 - val_loss: 0.0532 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6727 - val_loss: 0.0242 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2969 - val_loss: 0.0040 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3145 - val_loss: 0.0013 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7330 - val_loss: 0.0028 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7734 - val_loss: 0.0067 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7598 - val_loss: 0.0195 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.5772 - val_loss: 0.0309 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 6 Validation MSE: 0.003964736802153862\n",
            "Step 7: Training on 11 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 1.0538 - val_loss: 0.0666 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2889 - val_loss: 0.1452 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4803 - val_loss: 0.2732 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7638 - val_loss: 0.4428 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5409 - val_loss: 0.6673 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.9836 - val_loss: 0.9023 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.4972 - val_loss: 0.9882 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 7 Validation MSE: 0.1451583722653126\n",
            "Step 8: Training on 12 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.4666 - val_loss: 0.1231 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.8153 - val_loss: 0.1513 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.2816 - val_loss: 0.1859 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3376 - val_loss: 0.2532 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2536 - val_loss: 0.2675 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3626 - val_loss: 0.2950 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3447 - val_loss: 0.2704 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3334 - val_loss: 0.2282 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3715 - val_loss: 0.1946 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.5256 - val_loss: 0.1685 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 8 Validation MSE: 0.26753483976612247\n",
            "Step 9: Training on 13 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.2608 - val_loss: 0.3639 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6447 - val_loss: 0.2634 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3931 - val_loss: 0.2281 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4249 - val_loss: 0.2531 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3771 - val_loss: 0.3076 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.5678 - val_loss: 0.3722 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 9 Validation MSE: 0.36387720483550423\n",
            "Step 10: Training on 14 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.4949 - val_loss: 0.5916 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3407 - val_loss: 0.6308 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3668 - val_loss: 0.6559 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.8536 - val_loss: 0.6675 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6251 - val_loss: 0.5898 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5033 - val_loss: 0.5024 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.5693 - val_loss: 0.3943 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 10 Validation MSE: 0.630821961276251\n",
            "Step 11: Training on 15 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5268 - val_loss: 0.2802 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4094 - val_loss: 0.1289 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4144 - val_loss: 0.0395 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5432 - val_loss: 0.0201 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4779 - val_loss: 0.0230 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6547 - val_loss: 0.0575 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.4241 - val_loss: 0.1357 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 11 Validation MSE: 0.1288735063345196\n",
            "Step 12: Training on 16 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.7899 - val_loss: 0.2674 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2362 - val_loss: 0.3949 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3233 - val_loss: 0.4819 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3761 - val_loss: 0.5075 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4816 - val_loss: 0.4887 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.8665 - val_loss: 0.4533 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.5165 - val_loss: 0.3747 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 12 Validation MSE: 0.3948901576595571\n",
            "Step 13: Training on 17 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.4355 - val_loss: 0.2728 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4887 - val_loss: 0.1798 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3694 - val_loss: 0.1160 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6323 - val_loss: 0.0852 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5495 - val_loss: 0.0633 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4194 - val_loss: 0.0652 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4745 - val_loss: 0.0782 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2649 - val_loss: 0.0908 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3379 - val_loss: 0.1371 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5017 - val_loss: 0.2460 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2586 - val_loss: 0.3351 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1893 - val_loss: 0.3633 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2782 - val_loss: 0.3680 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3381 - val_loss: 0.3181 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3657 - val_loss: 0.2457 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3152 - val_loss: 0.1705 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.3094 - val_loss: 0.1041 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 13 Validation MSE: 0.36334538251496645\n",
            "Step 14: Training on 18 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.4297 - val_loss: 0.3630 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2223 - val_loss: 0.2370 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1230 - val_loss: 0.1661 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4626 - val_loss: 0.1297 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1402 - val_loss: 0.1145 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2135 - val_loss: 0.1220 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1787 - val_loss: 0.1087 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.4152 - val_loss: 0.1117 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 14 Validation MSE: 0.16611234070381825\n",
            "Step 15: Training on 19 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.4193 - val_loss: 0.0910 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4026 - val_loss: 0.1248 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3453 - val_loss: 0.1507 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4182 - val_loss: 0.1941 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3286 - val_loss: 0.2239 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.3167 - val_loss: 0.2352 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.2849 - val_loss: 0.2495 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2984 - val_loss: 0.2563 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4127 - val_loss: 0.2371 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.2640 - val_loss: 0.1886 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2290 - val_loss: 0.1258 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1724 - val_loss: 0.0834 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1879 - val_loss: 0.0587 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3622 - val_loss: 0.0590 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1941 - val_loss: 0.0531 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3205 - val_loss: 0.0747 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3082 - val_loss: 0.1046 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 15 Validation MSE: 0.08341190216617944\n",
            "Step 16: Training on 20 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3177 - val_loss: 0.1838 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1672 - val_loss: 0.2652 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2935 - val_loss: 0.3168 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3122 - val_loss: 0.2936 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3082 - val_loss: 0.2455 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2563 - val_loss: 0.1654 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.2147 - val_loss: 0.1020 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 16 Validation MSE: 0.26521887057984567\n",
            "Step 17: Training on 21 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.2975 - val_loss: 0.2787 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3482 - val_loss: 0.1854 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.2007 - val_loss: 0.1367 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3121 - val_loss: 0.1140 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1341 - val_loss: 0.0940 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.1605 - val_loss: 0.0992 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1748 - val_loss: 0.1169 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2510 - val_loss: 0.1486 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3638 - val_loss: 0.2020 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1382 - val_loss: 0.2613 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 17 Validation MSE: 0.09403390057766879\n",
            "Step 18: Training on 22 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.4122 - val_loss: 0.4069 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2855 - val_loss: 0.5301 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1684 - val_loss: 0.6675 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1852 - val_loss: 0.7700 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4059 - val_loss: 0.8144 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5272 - val_loss: 0.7493 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2168 - val_loss: 0.6594 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.2551 - val_loss: 0.5316 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 18 Validation MSE: 0.6675368220845489\n",
            "Step 19: Training on 23 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.2649 - val_loss: 0.2755 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1672 - val_loss: 0.1785 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2697 - val_loss: 0.1157 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2378 - val_loss: 0.0819 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2552 - val_loss: 0.0765 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3193 - val_loss: 0.0917 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.2279 - val_loss: 0.1263 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 19 Validation MSE: 0.17848652321959596\n",
            "Step 20: Training on 24 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.2681 - val_loss: 0.2015 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2911 - val_loss: 0.2154 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1708 - val_loss: 0.2345 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1431 - val_loss: 0.2592 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1578 - val_loss: 0.2898 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2272 - val_loss: 0.2676 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1754 - val_loss: 0.2492 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3039 - val_loss: 0.2208 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.3040 - val_loss: 0.1866 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 20 Validation MSE: 0.2591766327247952\n",
            "Step 21: Training on 25 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.2107 - val_loss: 0.2338 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2695 - val_loss: 0.1924 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1874 - val_loss: 0.1615 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1558 - val_loss: 0.1257 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2202 - val_loss: 0.1109 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1154 - val_loss: 0.1007 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2570 - val_loss: 0.1055 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1918 - val_loss: 0.1214 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3208 - val_loss: 0.1637 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1160 - val_loss: 0.2189 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1728 - val_loss: 0.2630 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 21 Validation MSE: 0.1007415356580168\n",
            "Step 22: Training on 26 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.2206 - val_loss: 0.0668 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1801 - val_loss: 0.1188 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1708 - val_loss: 0.1990 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1631 - val_loss: 0.2615 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2195 - val_loss: 0.2962 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3818 - val_loss: 0.3064 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2545 - val_loss: 0.2774 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2065 - val_loss: 0.2230 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2136 - val_loss: 0.1506 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 22 Validation MSE: 0.2614733045432714\n",
            "Step 23: Training on 27 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.2731 - val_loss: 0.1293 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2933 - val_loss: 0.0569 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1921 - val_loss: 0.0173 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1635 - val_loss: 0.0030 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2137 - val_loss: 3.0436e-04 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4199 - val_loss: 1.2005e-04 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.2601 - val_loss: 0.0015 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1830 - val_loss: 0.0143 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1884 - val_loss: 0.0420 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 23 Validation MSE: 0.0029767200723291995\n",
            "Step 24: Training on 28 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.2163 - val_loss: 0.0860 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1958 - val_loss: 0.1404 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1999 - val_loss: 0.2255 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3004 - val_loss: 0.2911 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3046 - val_loss: 0.3398 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2562 - val_loss: 0.3380 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1452 - val_loss: 0.3092 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2039 - val_loss: 0.3111 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1486 - val_loss: 0.2950 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3470 - val_loss: 0.2376 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.1572 - val_loss: 0.1874 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1403 - val_loss: 0.1496 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1652 - val_loss: 0.1294 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1054 - val_loss: 0.1089 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2034 - val_loss: 0.1038 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.2459 - val_loss: 0.1266 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1521 - val_loss: 0.1394 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1332 - val_loss: 0.1511 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.1226 - val_loss: 0.1631 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 24 Validation MSE: 0.10886958830514791\n",
            "Step 25: Training on 29 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.1794 - val_loss: 0.1018 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2054 - val_loss: 0.1356 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1556 - val_loss: 0.1735 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0839 - val_loss: 0.1976 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1630 - val_loss: 0.2203 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1404 - val_loss: 0.2287 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2035 - val_loss: 0.2134 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1629 - val_loss: 0.1912 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.2260 - val_loss: 0.1445 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 25 Validation MSE: 0.19756774320940124\n",
            "Step 26: Training on 30 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.1409 - val_loss: 0.2093 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1219 - val_loss: 0.1552 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1272 - val_loss: 0.1258 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2201 - val_loss: 0.1250 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1414 - val_loss: 0.1383 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0807 - val_loss: 0.1596 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2016 - val_loss: 0.1942 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1655 - val_loss: 0.2195 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1584 - val_loss: 0.2345 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1400 - val_loss: 0.2400 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1338 - val_loss: 0.2581 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 26 Validation MSE: 0.15955670395949922\n",
            "Step 27: Training on 31 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.1603 - val_loss: 0.2249 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1537 - val_loss: 0.2425 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1178 - val_loss: 0.2552 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1082 - val_loss: 0.2623 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1412 - val_loss: 0.2511 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2123 - val_loss: 0.2259 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1702 - val_loss: 0.2109 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1174 - val_loss: 0.1876 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.1406 - val_loss: 0.1687 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 27 Validation MSE: 0.26229383390406175\n",
            "Step 28: Training on 32 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.1303 - val_loss: 0.2430 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1721 - val_loss: 0.2680 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1296 - val_loss: 0.2888 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1805 - val_loss: 0.3055 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1652 - val_loss: 0.3150 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1238 - val_loss: 0.3259 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1096 - val_loss: 0.3310 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1084 - val_loss: 0.3197 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1752 - val_loss: 0.3165 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1046 - val_loss: 0.3107 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1415 - val_loss: 0.2942 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1301 - val_loss: 0.2757 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1040 - val_loss: 0.2608 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.1164 - val_loss: 0.2551 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1324 - val_loss: 0.2483 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0697 - val_loss: 0.2495 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1240 - val_loss: 0.2610 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0811 - val_loss: 0.2791 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0886 - val_loss: 0.2912 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1137 - val_loss: 0.3137 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 28 Validation MSE: 0.31367366155165133\n",
            "Step 29: Training on 33 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.1228 - val_loss: 0.1593 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1377 - val_loss: 0.1753 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1104 - val_loss: 0.1787 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0974 - val_loss: 0.1721 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1054 - val_loss: 0.1616 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1269 - val_loss: 0.1380 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1335 - val_loss: 0.1093 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1031 - val_loss: 0.0972 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1389 - val_loss: 0.0833 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 29 Validation MSE: 0.17207875267884393\n",
            "Step 30: Training on 34 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0667 - val_loss: 0.1529 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1211 - val_loss: 0.1299 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1286 - val_loss: 0.1045 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0980 - val_loss: 0.0899 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1141 - val_loss: 0.0875 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.1127 - val_loss: 0.0965 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 30 Validation MSE: 0.1529334353424866\n",
            "Step 31: Training on 35 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0903 - val_loss: 0.2858 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1329 - val_loss: 0.2875 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0930 - val_loss: 0.2821 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0843 - val_loss: 0.2604 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1195 - val_loss: 0.2636 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1485 - val_loss: 0.2563 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.1473 - val_loss: 0.2395 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1541 - val_loss: 0.2216 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0831 - val_loss: 0.2011 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0787 - val_loss: 0.1799 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1576 - val_loss: 0.1673 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0799 - val_loss: 0.1539 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0770 - val_loss: 0.1451 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1219 - val_loss: 0.1533 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0742 - val_loss: 0.1715 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1044 - val_loss: 0.1989 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0794 - val_loss: 0.2304 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1831 - val_loss: 0.2576 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1186 - val_loss: 0.2852 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0746 - val_loss: 0.2998 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 31 Validation MSE: 0.1715220627222196\n",
            "Step 32: Training on 36 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0572 - val_loss: 0.3574 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0846 - val_loss: 0.3763 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0975 - val_loss: 0.3945 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1401 - val_loss: 0.4183 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1096 - val_loss: 0.4304 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0975 - val_loss: 0.4369 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 32 Validation MSE: 0.35742101731011644\n",
            "Step 33: Training on 37 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0913 - val_loss: 0.2926 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1123 - val_loss: 0.3180 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0726 - val_loss: 0.3386 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1143 - val_loss: 0.3518 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1062 - val_loss: 0.3494 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1124 - val_loss: 0.3379 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0984 - val_loss: 0.3191 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0804 - val_loss: 0.3115 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 33 Validation MSE: 0.33855152489096324\n",
            "Step 34: Training on 38 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0729 - val_loss: 0.2863 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1151 - val_loss: 0.2616 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0892 - val_loss: 0.2435 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0818 - val_loss: 0.2336 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0852 - val_loss: 0.2324 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.1465 - val_loss: 0.2366 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 34 Validation MSE: 0.2863159480804823\n",
            "Step 35: Training on 39 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.1246 - val_loss: 0.3625 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1419 - val_loss: 0.3309 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1221 - val_loss: 0.2958 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1069 - val_loss: 0.2900 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0803 - val_loss: 0.2977 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1069 - val_loss: 0.3357 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0857 - val_loss: 0.3721 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1007 - val_loss: 0.4061 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1322 - val_loss: 0.4140 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0602 - val_loss: 0.4143 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1283 - val_loss: 0.4062 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0697 - val_loss: 0.3889 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0957 - val_loss: 0.3581 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1220 - val_loss: 0.3177 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.1018 - val_loss: 0.2941 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Step 35 Validation MSE: 0.4143453054339588\n",
            "Step 36: Training on 40 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0945 - val_loss: 0.3060 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0938 - val_loss: 0.2737 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1015 - val_loss: 0.2508 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0886 - val_loss: 0.2383 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1002 - val_loss: 0.2454 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0881 - val_loss: 0.2568 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0758 - val_loss: 0.2708 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1340 - val_loss: 0.2777 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0865 - val_loss: 0.2861 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1081 - val_loss: 0.2960 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1038 - val_loss: 0.3027 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.1124 - val_loss: 0.2905 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 36 Validation MSE: 0.2708502360913914\n",
            "Step 37: Training on 41 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0774 - val_loss: 0.2220 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0792 - val_loss: 0.2260 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0598 - val_loss: 0.2323 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0822 - val_loss: 0.2382 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0707 - val_loss: 0.2268 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0694 - val_loss: 0.2171 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0709 - val_loss: 0.2094 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1123 - val_loss: 0.1993 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 37 Validation MSE: 0.2323427990863316\n",
            "Step 38: Training on 42 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0995 - val_loss: 0.2209 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0865 - val_loss: 0.2135 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0597 - val_loss: 0.2086 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0955 - val_loss: 0.1922 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0617 - val_loss: 0.1895 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0913 - val_loss: 0.1950 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0642 - val_loss: 0.2041 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.1088 - val_loss: 0.1930 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Step 38 Validation MSE: 0.20862686293208998\n",
            "Step 39: Training on 43 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0703 - val_loss: 0.1870 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0648 - val_loss: 0.1776 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0746 - val_loss: 0.1731 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0815 - val_loss: 0.1820 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0849 - val_loss: 0.1945 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0677 - val_loss: 0.2064 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0695 - val_loss: 0.2040 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 39 Validation MSE: 0.17762862729283588\n",
            "Step 40: Training on 44 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0867 - val_loss: 0.1837 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1018 - val_loss: 0.1873 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1210 - val_loss: 0.1792 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0640 - val_loss: 0.1686 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.1195 - val_loss: 0.1798 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0739 - val_loss: 0.1872 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0746 - val_loss: 0.2001 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0764 - val_loss: 0.2095 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0680 - val_loss: 0.2156 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 40 Validation MSE: 0.16857204112494317\n",
            "Step 41: Training on 45 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0866 - val_loss: 0.2594 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0853 - val_loss: 0.2796 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1184 - val_loss: 0.2876 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1230 - val_loss: 0.3089 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0539 - val_loss: 0.3148 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1093 - val_loss: 0.3233 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0693 - val_loss: 0.3211 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0671 - val_loss: 0.2992 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0759 - val_loss: 0.2687 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0989 - val_loss: 0.2469 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Step 41 Validation MSE: 0.31476712394322026\n",
            "Step 42: Training on 46 samples, Validating on 1 samples\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0813 - val_loss: 0.3069 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0761 - val_loss: 0.2761 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1237 - val_loss: 0.2550 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0718 - val_loss: 0.2371 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0597 - val_loss: 0.2387 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0819 - val_loss: 0.2466 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0834 - val_loss: 0.2453 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0697 - val_loss: 0.2507 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0525 - val_loss: 0.2482 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0549 - val_loss: 0.2421 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0612 - val_loss: 0.2261 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0427 - val_loss: 0.2115 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0545 - val_loss: 0.2081 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0815 - val_loss: 0.2239 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0841 - val_loss: 0.2470 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0759 - val_loss: 0.2549 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0759 - val_loss: 0.2662 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Step 42 Validation MSE: 0.21149318364972977\n",
            "Mean validation error: 0.675511049394581\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0556 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0661 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0938 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0462 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0716 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0640 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0610 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0642 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0636 - lr: 0.0010\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "avg. MAE: 3.3772614746093743\n",
            "avg. MAPE: 7.389590955448448\n",
            "avg. Log Loss: 0.009345886400813682\n",
            "avg. RMSE: 3.3772614746093743\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+UAAAIiCAYAAACuWWkyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABS2UlEQVR4nO3deXxTVf7/8XfTjcWmILIqpVUUikARFFnEFlCpiuMGVVARcWNQxsLXn4LaUhtGUFRQcAYdEXQcxTrjMrgUNxCVZVRkVCyurWHYRMCmQptIcn5/8GgkNC0tpL1J+3o+Hn08mntvzv0kPYW+c849N8oYYwQAAAAAABqczeoCAAAAAABoqgjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAAAAAABYhFAOAAAAAIBFCOUAACAsDBkyRB06dFBUVJTVpQAA0GCijDHG6iIAAJGhvLxcAwcO1Pbt27Vjxw6lpqYqLi4u4Jhff/1VJ5xwglauXFnrdleuXKmVK1cqLy+vyr4zzzxTXbp0UUFBwVFWX3d5eXnKyMhQRkZGg587FM4880x988032rZtm5o1a3bE7SxZskSSNH78+NAUVoO8vDzde++9CtWfJ8nJyWrRooW/n/7666/6/vvv1blzZx177LGSJI/Ho3379qmoqOiw/bvy2CVLlmjkyJHq2rWrf5/T6dSePXuUlpbm37Z9+3ZNnDgxaN+WpB07dmjevHl66623ZIzR/v37JUldu3ZVZmamRo4cqU6dOkmSrrrqKq1bty6gfmOMfvnlF7Vs2VITJkxQdna2YmJiJEkvvPCCZs2apa+++kpxcXHq2rWr5s+fryFDhkgK/H2WpA4dOmjNmjVq3rz50b7tAIC6MAAA1NGMGTOMJFNcXFxl34oVK0x6evoRtRfMqFGjzG233Vb3IkNAkpkxY4Yl5z5aX3zxhYmJiTGSzLPPPntUbaWnp9f5Z3qkauoLR6JLly4B/XTFihVGklm8eLF/W3FxsenSpUuVGoL178pjg/Xza6+9tkrtM2bMqLYPffjhh+a4444zkydPNrt37/ZvLy0tNXfeeaeRZG655ZaA5wSr3xhjFixYUG1/7dKlS40/v5pqBADUP6avAwBCqlevXpo1a1bI2nvxxRc1b968kLXXVCxatEj333+/mjdvrkWLFlldjmXS09MPO/LbvHlzpaen16q99u3b66mnntKxxx6r/v37H/b4rl27BoymV9q2bZsuvvhiDR8+XI8++qhat27t32e32zV79mxdfPHFtapJkm655RbZ7XY9/fTTtX4OACA8EMoBACGTnJyssrIyDRw40L/t119/1eTJk9WrVy+ddtppSktL0y233KIffvhBknTFFVdo4cKFkqQ+ffqoT58+GjVqlLxer/r06aNjjz1WycnJ/vauuuoqJSUlKSoqSu+//74uvfRSde/eXd27d9frr78ur9eradOm6fTTT1eXLl305z//OaDGiooK3XXXXerXr5/69eun3r1769JLL9U333zjP2bFihXq06ePJGnhwoX+ut555x3/MYWFhRowYIC6du2q5ORkjR49WsXFxdW+N+Xl5UpLS1NUVJTatm2roUOH+veNGDFCrVu3VnJyspYvX679+/crJydHvXv3Vt++fdW7d2+NHz9eGzZsqNXPwePx6M0339SkSZM0ZswYrVy50v9+H+q7777T6NGjlZSUpLS0NKWlpen2229XcXGxysrK1KdPH33yySf65JNP/O/D7NmztXjxYvXo0UNRUVH+6e3l5eXq06ePjjnmmCpT/l966SWdd9556tu3r/r06aMzzjhDzz//fK1ez9F4+umn1b59+xqPad++fa3CbEZGhtatW6dhw4apd+/eeuCBBw77nKuvvlpXX311le0PP/ywdu3apSlTplT73Pz8fF166aWHPYckGWPk9Xq1a9euWh0PAAgjVg/VAwAiT3XTew+dKmyMMTfccIM599xzjcfjMcYYs23bNnPyyScHTL+tacrytddeGzC12BhjFi9ebCSZyy67zPz666/GGGNuv/12Ex8fb3Jzc80333xjjDFm2bJlRpJZsWKF/7nbtm0zbdu2Nd9//70xxhifz2dmz55tOnfubMrKygLOo2qmA7/00kvGZrOZBQsWGGOM+e2338wVV1xhjj/+eLNr166gr6NSv379TJ8+fQK2+Xw+c9JJJ5kff/zRGGPMzJkzzamnnup/baWlpeass86q9RTjF1980dxxxx3GGGPWr19vJJl77rmnynElJSWmTZs2Zty4cea3334zxhyY9t6qVSszd+5c/3HVTV8vLi4OOpU62PEjRowwjz76qP/xF198YY499ljzyiuvBBwX6unrh6pu+newGg7ty+np6QF96VDBpq9XJzU11cTFxfnf99oKVr/H4zF5eXlGkvnDH/5Q5TlMXweA8MZIOQDgiF1wwQX+0dM+ffpo69atVY5Zs2aNunTpotjYWEkHFpOaM2eOevTocdTnv/rqq9WyZUtJ0pVXXim3261ffvlFJ598siRp5MiROuaYYwJGuI877jitXr1aJ554oiQpKipKt912mzZv3qw33njjsOc0xmjKlCnq3r27brnlFklSTEyM5syZoy1btuixxx6r8fnXX3+9NmzYoPXr1/u3vffee+ratauSkpIkHXjPOnbs6H9tdrtd9913nwYMGFCr92Xx4sX64x//KEk67bTTNGjQIC1ZskQ+ny/guBkzZqisrEwPPfSQf3Gwnj176sYbb6yywNnRmj9/viZNmuR/3LNnT5177rl6/PHHQ3qeUDq0f3/yyScha7ukpERt2rTxv+91lZubqz59+ujkk09WixYtdP/992vUqFFh/X4CAII7sv8JAACQ9MYbbwRMLT/4+0rDhw/X/PnzVVpaqnHjxmn48OF1ula2Jqeccor/+8qVtA/eVrl927Zt/scxMTH68ccfNWXKFJWUlCg6Otq/7/vvvz/sOb/55hv9+OOPuummmwK2d+7cWYmJiXrvvfeUk5NT7fPHjh2r//u//9OiRYvUt29fSQeu/77++uv9xwwfPlxTp07ViBEjNGHCBJ1//vn+FbMP53//+59iY2MDfha33nqrxo4dq+XLl+v888/3b1++fLlSUlJ03HHHBbRRm2nZddWyZUtlZ2dr9erV+u2332Sz2eR0OtW2bduQnytUDu3foV6F31SzwvyIESO0Y8cOlZaWqry83L86+sHy8/P9q+HPnz9fzz77rBwOhzp06BDSGgEA9Y+RcgBAyJSUlFQJ5nPnztXChQv1/fff66KLLlK7du00adIkuVyuoz5f5UiyJP+9rQ/eVrnd6/X6H7/11ls699xzNXDgQG3YsMH/JUlut/uw5/z5558lSa+++mrAKGqfPn3UsmVL/fbbbzU+PzExUaNGjdJzzz2niooK7dmzRx988EHABxVTpkzRCy+8oIqKCo0ZM0Zt27bV2LFjg4azQy1ZskRffPFFQF2zZs1SbGysnnrqqSqvpfLDjPq0d+9eDR06VJ9++qneeOMNff7559qwYYP+8Ic/1Oo9P9TWrVurvPcNYeXKlXUO5jfccENAnf/+978lSSkpKdq1a1fQ/rJ8+XJt2LBB6enp2rFjx2HPMXnyZMXFxenyyy8P6OuV4uLi/LdaC+a3334L+cwIAEDtMVIOAKhXNptNN910k2666SZ9/fXXWrhwoR599FGVlZXp73//e4PX8/TTT6tly5aaPn26P8jXReWo8pgxYzR37twjquH666/X3//+d/3rX//Snj17NHr06CqhKCsrS1lZWdq8ebOeeuopzZ49W5s3b9YHH3xQbbvGGP373//WN998EzADQJJuvvlmLVmyRD///LP/NRx33HHavXv3Eb0GSf5zHDriW1ZWpoSEBP/j1atX65tvvtGLL7542EXXaqNTp061XvTOak8++WTQ7RdeeKHmzJmjtWvX1noWRE2mTZumkSNH6vnnn6+ysFy7du1qDPc7duyo9aURAIDQY6QcABBSmzdv9k/Llg4E0H379kmSunXrprlz5+rCCy/Uf//7X/8xldebV4a75cuXH1VYrInb7ZbNZgsI5AdPbz9YTEyMv6Yff/xRq1ev1imnnKLk5GR99tlnVY5/4okn9Je//OWwNZx99tnq2rWrFi1aVGXquiRNnz7dv5J7586dNWPGDN14440B71kwK1asUGpqapVALkkXX3yxPB5PwAchI0aMUHFxsX/0v1J+fr4eeugh/+PY2Fj/+7B3717/aG+7du0UFRUV8LPyeDxVVnqvHA232QL/7KjufW8Kpk6dqjZt2uj+++8PSXsXXHCBevTooT//+c9V1g4YNGiQvv/+e/3vf/+r8jxjjN5//31COQBYiFAOAAgpr9cbENLeffddzZ8/3x/qdu7cqY0bN+qcc87xH5OSkiLpwPXQpaWluvTSS/Xrr7/WS30XXXSRXC6XFixY4K93xowZQY9NSUnxB5mFCxfqySefVFRUlB555BF98MEHWrx4sf/YtWvXKjc3t1b3ro6KitKECRO0YsUKxcfH69RTTw3Yv2bNGj300EP+Kce//vqrPv7444D3LJinnnqq2uv1hw8frmOOOSZgCnteXp4SEhL0f//3f/5zffLJJ3rsscc0YsSIgPdhy5YtMsboww8/VHZ2tiQpPj5egwYN0quvviqPxyPpwOUKh476Dxo0SG3atNH8+fP9P9f33ntP77777uHeqkarQ4cOevXVV7V27VpNmDBBO3fu9O+rqKjQU089pffee0/HHHNMrdqLiorS1KlTtWnTJhUUFATsu/XWW5WYmKjrr78+YMS8vLxcd9xxh7p3766ePXuG5oUBAOrOqmXfAQCRZ+/evaZLly4mMTHRSDLHH3+86dKlS8BX5bZKixcvNkOHDjU9e/Y0ffr0Maeeeqq55557jNvt9h9TXl5uLrnkEpOSkmJSU1PNzJkzzf79+01aWppp3bq1iY2NNWlpaeajjz4ykyZNMp07dzaSTGpqqnn22WfNs88+a1JTU40k07lzZ5OdnW02bdpk0tLSTGxsrGndurXp37+//3xz5swxJ554ojnllFNMenq6WbhwoZFk2rdvby6//HL/ca+++qo58cQTTe/evc3AgQPNt99+69/31ltvmcGDB5ukpCTTt29fM3z4cLNq1apav5dbtmwx0dHR5oknnqiy79VXXzUXXHCB6dGjh0lLSzM9evQwt956q/nll1+qba9///4mLi7O/54cbMeOHSYtLc00b97cSDK9evXy1/rtt9+aUaNGmRNOOMGkpaWZs88+u8rr+Prrr80ZZ5xhunfvbnr27Glee+01/76ioiIzZMgQ07lzZzNkyBDz/PPPm/T0dNOyZUuTlpbmv63Y2rVrzVlnnWU6dOhgzj77bHPDDTeYCy64wP+z3bhxoznrrLNM+/btjSSTlpZmXnzxxVq/n7Vx+eWXm5NOOsnfT4YPHx6wf9++fSYtLc1fQ2pqqklLSzMul6vGdjdu3Ojvq5W1P/LII7Wqadu2beaOO+4wvXr1Mr169TK9e/c2KSkp5rzzzjNz584NuMXe2LFjA+pPS0sze/bs8e+vqKgwHTp0MK1btw5474058DO8+uqrTZcuXUxqaqrp27evOf30001ubq4pLy+vVa0AgPoRZUw1S38CAAAAAIB6xfR1AAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIjFWF1DffD6ftm7dqoSEBEVFRVldDgAAAACgkTPGqKysTJ06dZLNVvNYeKMP5Vu3blXnzp2tLgMAAAAA0MRs3rxZJ5xwQo3HNPpQnpCQIOnAm2G32y2uBgAAAADQ2LlcLnXu3NmfR2vS6EN55ZR1u91OKAcAAAAANJjaXELNQm8AAAAAAFiEUA4AAAAAgEUI5QAAAAAAWKTRX1NeG8YY7d+/X16v1+pSUAfR0dGKiYnhVncAAAAAIlaTD+Uej0fbtm3Tvn37rC4FR6BFixbq2LGj4uLirC4FAAAAAOqsSYdyn8+n4uJiRUdHq1OnToqLi2PUNUIYY+TxeLRz504VFxfr5JNPls3G1RgAAAAAIkuTDuUej0c+n0+dO3dWixYtrC4HddS8eXPFxsbqxx9/lMfjUbNmzawuCQAAAADqhKFFiRHWCMbPDgAAAEAkI9EAAAAAAGARQjkAAAAAABYhlIeAz+dTSUmJvvjiC5WUlMjn8zXo+QcPHqwRI0bU+XmvvPKKXnnllZDXc91116lDhw4aP358yNsGAAAAgMakSS/0FgpFRUUqLCyUy+Xyb7Pb7crMzFRqamq9n7+kpET/+c9/ZIzRTz/9pHbt2tX6uZWB/JJLLglpTYsXLyaQAwAAAEAtMFJ+FIqKilRQUBAQyCXJ5XKpoKBARUVF9V7D888/r//3//6fvF6vXnjhhXo/HwAAAAAgdAjlR8jn86mwsLDGYwoLC+t9Kvs///lP3X777Ro4cKCee+65gH379+/XtGnT1KtXL6Wnp+uMM87QvHnzJEl33HGHCgsLVVhYqIyMDF188cXasGGDBgwYoKioKJWUlEiSpk+fXmUqeklJiUaPHq2BAwcqPT1d5557rr766qt6fZ0AAABouqy+XBSoT0xfP0JOp7PKCPmhXC6XnE6nkpOT66WGL7/8Up06ddKxxx6rMWPG6E9/+pOKi4uVkpIiScrNzdXbb7+ttWvXqmXLlvrwww/1hz/8QdnZ2XrggQf0008/SZKWLFnib3Pp0qX+50vSrFmztG3btirnjYqK0urVqxUVFaW///3vuvTSS7Vx40bFxNClAAAAEDpWXy4K1DdGyo9QWVlZSI87Es8995zGjh0rScrKylJ0dLR/tLy8vFxz587VpEmT1LJlS0nSWWedpT/96U9Hfd709HQtXLhQUVFR/nN/8803+v7774+6bQAAAKBSOFwuCtQ3QvkRSkhICOlxR2LZsmW6+OKLJUnt27dXRkaGP5R/9913qqioUNeuXQOek5eXd9TnjYmJ0SOPPKIhQ4YoPT3dv/L79u3bj7ptAAAAQAqfy0WB+sZc4yOUlJQku91e4xR2u92upKSkejn/6tWrtXPnTl1wwQX+bdu3b9fXX3+tDRs2KDo6+ojarRz9PpjX6w1o7/bbb9ebb76ptWvX+ld7j4qKkjHmiM4JAAAAHCocLhcFGgIj5UfIZrMpMzOzxmMyMzNls9XPW/z888/rmWee0cqVK/1f69atU7NmzfTcc8+pa9euatasmX744YeA5z344IPat2+f/zVU2rdvn7xer39k/+Bp91u2bAloY9WqVRo6dKg/kHs8nnp5jQAAAGi6wuFyUaAhEMqPQmpqqrKysmS32wO22+12ZWVl1dvCE16vV6tWrdLw4cMDticmJuqiiy7S0qVL1axZM02ZMkV//etf/SG8sLBQL7/8slq0aCFJatu2rfbs2SNJGjVqlDZt2qRjjz1WSUlJWr16tSRp06ZN2rBhQ8B5evTooTVr1vjb/de//lUvrxMAAABNVzhcLgo0BKavH6XU1FR169ZNTqdTZWVlSkhIUFJSUr2NkJeWluq8887Tli1blJ2drfnz5/v3LVq0SOvXr9fmzZs1aNAgvfDCC/J6verfv7/atGmjxMRELV261H/8ddddp9GjR2vIkCFKSUnRqaeeKklauHChpkyZoueee079+/fXhRdeqMLCQt1www168skn9fDDD+vGG29Ur1691LNnT5122mmSpOzsbM2ZM0fPPfec//qfm2++WY8//ni9vBcAAABovKy+XBRoKFGmkV8I7HK5lJiYqNLS0ioj2hUVFf5biDVr1syiCnE0+BkCAAA0XpWrr1enPmenAkejphx6KKavAwAAAAhLVl0uCjQkpq8DAAAACFsNfbko0NAI5QAAAADCms1m47ZnaLT4eAkAAAAAAIsQygEAAAAAsAihHAAAAAAAixDKAQAAAACwCKEcAAAAAACLEMoBAAAAALAIoTwC/ec//1FGRoaioqLUvXt3ZWRkBHzV5nYRJSUlysvLq7L9kksu0dy5c0Nf9CFeeeUVvfLKK/V+HgAAAAAIZ4Tyo5CXlyeHwxF0n8PhCBp6Q6F///5auXKlJGnatGlauXJlwFdtlJSU6N57762yPSUlRR06dAhhtcERygEAAACAUH5UoqOjlZubWyWYOxwO5ebmKjo62pK6nn766SN+7ty5czVmzJgQVgMAAADgaPl8PpWUlOiLL75QSUmJfD6f1SUhRGKsLiCS5eTkSJJyc3P9jysDeX5+vn9/Q1myZEnAtPS3335bOTk5at68uSoqKnTGGWdo1qxZWrdunaZOnSpJysjIkCTNmjVLL7/8sgoKCpScnKyVK1fqu+++0w033KD3339fTzzxhN5++2198cUX6tmzp5555hk98MADevfdd7Vnzx4988wzOu200yRJe/bs0dSpU7Vx40Y1b95cPp9Ps2fP1uDBgyVJd9xxhwoLC/3nT0xM1KuvvipJevPNNzVjxgzFxcXJ5/Np3LhxmjhxYgO+iwAAAEB4KSoqUmFhoVwul3+b3W5XZmamUlNTLawMoUAoP0oHB/OZM2fK4/FYEsgPtX//fo0aNUovv/yyhg0bpvLycp122mmaOnWqhg0bpnnz5mno0KEB090HDhyoFi1a+Ld17dpVK1euVFRUlN544w3985//1P79+5WamqpLLrlEjz32mGbMmKFp06Zp6tSpWrFihSRpy5Yt+vrrr7V69WrFxMTogw8+0MUXX6zvvvtOrVq10gMPPKCffvpJ0oEPEipt3LhRo0aN0urVq5WWlqaff/5Zffr0UWJiIqP3AAAAaJKKiopUUFBQZbvL5VJBQYGysrII5hEubKavL1iwQFFRUQEhcffu3bruuuvUt29fZWRk6KyzztKqVausK7IaOTk5iouLk8fjUVxcXIMG8tmzZ/sXeJs9e7Z/e1lZmVwul0pKSiRJzZs3V0FBgdq3b39E57n88ssVHR2t+Ph4nX766fJ6verataskaciQIfrss8/8x5588sl6+eWXFRMT498fGxurdevW1XiOBx54QEOHDlVaWpok6bjjjtOll16qv/zlL0dUMwAAABDJfD6ff4ZpdQoLC5nKHuHCYqR869atmjNnTpXtt912m7777jutWbNG8fHxevXVVzVy5Eh98803DbIYWW05HA5/IPd4PHI4HA0WzKdNm6bx48dL+n36uiS1bt1a06dP14033qiFCxfqyiuv1Pjx49W8efMjOk/Hjh3937do0ULx8fH+xy1btlRpaan/cWxsrP7xj3/4F3Kz2Wzas2ePtm/fXuM5vvzyS23fvt0/pV6SfvnlFzVr1uyIagYAAAAimdPpDJiyHozL5ZLT6azVHZgQnsJipHzy5Mm66667qmzfsGGDhgwZ4g+A5557rsrKyrRmzZqGLrFaB19D7na7lZ+fH3Txt4Ywfvz4gBXf77vvPn3//fe68MILNW/ePKWmpuqHH344orYPXbSupkXsHnroIeXn5+upp57SqlWrtHLlSnXo0EHGmMOe55xzzglYSX7Dhg1au3btEdUMAAAARLKysrKQHofwZHkoX7ZsmWJjYzVixIgq+y6//HK98cYb2r17tyTp2WeflaQap2C73W65XK6Ar/oSbFG3nJwcS4O5JL3wwgsqKyvT8uXLlZycrBkzZmjTpk1q3ry5Xn75ZUkHRq8r7d+/X+Xl5SE7/6pVq9SvXz//9HZJ8ng8AcccfP59+/bJ6/WqZ8+e+vrrrwOO+/LLL5Wfnx+y2gAAAIBIkZCQENLjEJ4sDeV79+7V3Xffrblz5wbdn5eXp4svvlgpKSk6+eSTNWnSJE2ePFmDBg2qts1Zs2YpMTHR/9W5c+f6Kl9erzfoom6Vwdzr9dbbuWty5513ateuXZo0aZJ+/fVXSZIxRl6vV6eccookqW3btpIOrJT+0ksv+VeQD4UePXro888/186dOyVJq1ev1rZt2wKOadu2rfbs2SNJGjVqlDZt2qQ777xT69ev11tvvSVJ+u2335STk6MuXbqErDYAAAAgUiQlJclut9d4jN1uV1JSUgNVhPoQZWozp7ieTJ06VV27dtWkSZNUUlKilJQUrVixwn9NcW5url599VW9/fbbateund577z2VlJRowoQJ1bbpdrvldrv9j10ulzp37qzS0tIqHbqiokLFxcVKSUmJqOuW165dq+zsbK1bt04nnniiP2BX+uyzz7R7925Nnz5dH330kex2u8rKyjRq1ChNmzbNf9xVV12lr776Ss2bN9fixYu1aNEiFRQU6JdfftFZZ52lJ598UldeeaXef/99paWl6eGHH1ZhYaGeeeYZSdK4ceOUmZmpqVOn6r///a/S09O1dOlStWjRQjfddJPWrFmj3r17q2vXrlq6dKkSExN1991365prrtGmTZs0evRotWrVSikpKf42ly9frrvvvls2m01xcXG6/PLLNWXKlGrfi0j9GQIAAAC1Ud3q65VYfT08uVwuJSYmBs2hh7IslK9fv16TJ0/WBx98IJvNViWU79y5U506ddKSJUt01VVX+Z/XtWtX5eXl6eqrr67VeWp6Mwh0kY+fIQAAABo77lMeeeoSyi1bff31119XeXm5hg0bJulAuJKk7Oxs/72s9+/fX2UVweTkZP3rX/+qdSgHAAAAgEiWmpqqbt26yel0qqysTAkJCUpKSgpYpwmRy7JQnpOTE3AtduVI+bx585SRkaEtW7ZIUpVrkbdt23bE99oGAAAAgEhks9m47VkjFbYfrRx//PE677zzNH/+fP8o+rJly/TVV18pKyvL4uoAAAAAADh6lo2UHyw7O9t/L+rs7Gx1795dS5cu1XPPPadp06Zp8ODBatasmSoqKvT000/r4osvtrhiAAAAAACOXliE8nnz5gXd3qZNG/3tb3+r9/NbuAA9jhI/OwAAAACRLGynrzeE2NhYSdK+ffssrgRHqvJnV/mzBAAAAIBIEhYj5VaJjo5Wq1at9NNPP0mSWrRooaioKIurQm0YY7Rv3z799NNPatWqlaKjo60uCQAAAADqrEmHcknq0KGDJPmDOSJLq1at/D9DAAAAAIg0TT6UR0VFqWPHjmrXrp1+++03q8tBHcTGxjJCDgAAACCiNflQXik6OpqABwAAAABoUE16oTcAAAAAAKxEKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAhK28vDw5HI6g+xwOh/Ly8hq2oBAjlAMAAAAAwlZ0dLRyc3OrBHOHw6Hc3FxFR0dbVFloxFhdAAAAAAAA1cnJyZEk5ebm+h9XBvL8/Hz//kgVZYwxVhdRn1wulxITE1VaWiq73W51OQAAAACAI1AZxOPi4uTxeMI6kNclhxLKAQAAAAARIT4+Xh6PR3FxcXK73VaXU6265FCuKQcAAAAAhD2Hw+EP5B6Pp9rF3yINoRwAAAAAENYOvobc7XYrPz8/6OJvkYiF3gAAAAAAYSvYom7BFn+LVIRyAAAAAEDY8nq9QRd1q3zs9XqtKCtkWOgNAAAAAIAQYqE3AAAAAAAiAKEcAAAAAACLEMoBAAAAALAIoRwAAAAAAIsQygEAAAAAsAihHAAAAAAAixDKAQAAAACwCKEcAAAAAACLEMoBAAAAALAIoRwAAAAAAIsQygEAAAAAsAihHAAAAAAAixDKAQAAAACwCKEcAAAAAACLEMoBAAAAALAIoRwAAAAAAIsQygEAAAAAsAihHAAAAAAAixDKAQAAAACwCKEcAAAAAACLEMoBAAAAALBIjNUFAAAAAABwOD6fT06nU2VlZUpISFBSUpJstsgfZyaUAwAAAADCWlFRkQoLC+Vyufzb7Ha7MjMzlZqaamFlRy/yP1YAAAAAADRaRUVFKigoCAjkkuRyuVRQUKCioiKLKgsNQjkAAAAAICz5fD4VFhbWeExhYaF8Pl8DVRR6hHIAAAAAQFhyOp1VRsgP5XK55HQ6G6ii0COUAwAAAADCUllZWUiPC0eEcgAAAABAWEpISAjpceGIUA4AAAAACEtJSUmy2+01HmO325WUlNRAFYUeoRwAAAAAEJZsNpsyMzNrPCYzMzOi71ceuZUDAAAAABq91NRUZWVlVRkxt9vtysrKivj7lMdYXQAAAAAAADVJTU1Vt27d5HQ6VVZWpoSEBCUlJUX0CHklQjkAAAAAIOzZbDYlJydbXUbIRf7HCgAAAAAARChCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWCZtQvmDBAkVFRWnlypUB29evX6/zzz9fQ4cOVbdu3TR06FCVlJRYUiMAAAAAAKEUFquvb926VXPmzKmyfdOmTbrkkku0fPlypaamat++ferXr5+2b9/eKFfdAwAAAAA0LWExUj558mTdddddVbbfc889uvrqq/03g2/RooUKCgrUvXv3hi4RAAAAAICQs3ykfNmyZYqNjdWIESMCtns8Hr322mt65ZVXArb36tWrxvbcbrfcbrf/scvlClmtAAAAAACEkqUj5Xv37tXdd9+tuXPnVtn33Xffye12a/fu3br00ks1aNAg/eEPf9C6detqbHPWrFlKTEz0f3Xu3Lm+ygcAAAAA4KhYGspzcnI0ceJEdezYscq+PXv2SDowhX3evHlavXq1LrvsMg0ZMkRfffVVtW1Onz5dpaWl/q/NmzfXW/0AAAAAABwNy0L5+vXrtW7dOk2cODHo/ujoaEnSNddcoy5dukiSxo8fr+TkZP3lL3+ptt34+HjZ7faALwAAAAAAwpFl15S//vrrKi8v17BhwyRJFRUVkqTs7Gy1atVKM2fOlCQdf/zxAc/r0qWLiouLG7ZYAAAAAADqgWWhPCcnRzk5Of7HJSUlSklJ0bx585SRkSFJOvHEE7Vt27aA5+3YsUODBw9uyFIBAAAAAKgXYXFLtOpMmzZNf//73/3Xl7/77rsqKirSzTffbHFlAAAAAAAcPctviSYdmLK+du1a//fdu3fX0qVLdeONN8rlcikjI8N/bXhhYaH69OljYbUAAAAAAIRGlDHGWF1EfXK5XEpMTFRpaSmLvgEAAAAA6l1dcmhYT18HAAAAAKAxI5QDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWIZQDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWIZQDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWIZQDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWIZQDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgkRirCwAAAAjG5/PJ6XSqrKxMCQkJSkpKks3GeAIAoHEhlAMAgLBTVFSkwsJCuVwu/za73a7MzEylpqZaWBkAAKHFx80AACCsFBUVqaCgICCQS5LL5VJBQYGKioosqgwAgNAjlAMAgLDh8/lUWFhY4zGFhYXy+XwNVBEAAPWLUA4AAMKG0+msMkJ+KJfLJafT2UAVAQBQvwjlAAAgbJSVlYX0OAAAwh2hHAAAhI2EhISQHgcAQLhj9XUAABA2kpKSZLfba5zCbrfblZSU1IBVAdbi9oBA40YoBwAAYcNmsykzM1MFBQXVHpOZmUkgQZPB7QGBxo//0QAAQFhJTU1VVlaW7HZ7wHa73a6srCyCCJoMbg8INA2MlAMAgLCTmpqqbt26MWUXTVZtbw/YrVs3fi+ACEcoBwAAYclmsyk5OdnqMgBL1OX2gPyeAJGNj9UAAACAMMPtAYGmg1AOAAAAhBluDwg0HYRyAAAAIMxU3h6wJtweEGgcCOUAAABAmKm8PWBNuD0g0DjwWwwAAACEIW4PCDQNrL4OAAAAhCluDwg0foRyAAAAIIxxe0CgceMjNgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCJHHMp/++03OZ1OSZLP5wtZQQAAAAAANBV1DuVut1sTJ05Uy5YtNXToUEnShAkTdP3116u8vDzkBQIAAAAA0FjVOZRPmzZNW7Zs0dKlS9WuXTtJ0pNPPqnU1FRNnTo15AUCAAAAANBY1TmUf/LJJ3r11Vd12WWXqXnz5pKkmJgY3X777dq0aVPICwQAAAAAoLGqcyj3er2y2Q48zRgTsG/37t1HXMiCBQsUFRWllStXBt1/++23KyoqSiUlJUd8DgAAAAAAwkmdQ3liYqL+9re/SZKioqIkSXv37tU999yj448//oiK2Lp1q+bMmVPt/g0bNujpp58+orYBAAAAAAhXdQ7ljz76qGbNmqVWrVrp448/VkpKitq0aaOlS5dq/vz5R1TE5MmTdddddwXd5/P5dMstt2jGjBlH1DaAyOHz+VRSUqIvvvhCJSUl3NkBAAAAjV5MXZ9w8skna9OmTfrHP/6hjRs3SpJ69uypsWPHKi4urs4FLFu2TLGxsRoxYkTQ/QsWLNCQIUPUs2fPWrXndrvldrv9j10uV51rAtDwioqKVFhYGPA7a7fblZmZqdTUVAsrAwAAAOpPnUO5JMXFxem6666rsn3fvn1q0aJFrdvZu3ev7r77bi1fvjwgSFfasmWLFi1apDVr1ug///lPrdqcNWuW7r333lrXAMB6RUVFKigoqLLd5XKpoKBAWVlZBHMAAAA0SnWevl6TkSNH1un4nJwcTZw4UR07dgy6f/LkyZo1a1adgv706dNVWlrq/9q8eXOdagLQsHw+nwoLC2s8prCwkKnsAAAAaJTqPFJ+4oknVrtv+/bttW5n/fr1WrdunR588MGg+//9738rJiZGF1xwQZ3qi4+PV3x8fJ2eA8A6TqfzsJeZuFwuOZ1OJScnN0xRAAAAQAOpcyiPj4/XtGnT/I+9Xq+2bNmiZcuW6Y9//GOt23n99ddVXl6uYcOGSZIqKiokSdnZ2WrVqpWio6NVVlamjIwMSdIvv/wiSbryyivVrFkzvfbaazrmmGPqWj6AMFNWVhbS4wAAAIBIEmUOvdn4YVRe33moX3/9VRMnTtSzzz57RIWUlJQoJSVFK1as8Afxg61cuVJDhw5VcXFxnUbLXC6XEhMTVVpaKrvdfkS1Aag/JSUltbrl4bXXXstIOQAAACJCXXJona8pDxbIJemYY47Rd999V9fmADRxSUlJh/2Hym63KykpqYEqAgAAABpOnaevP/PMM1W2lZWVafXq1bLZjmzduOzsbK1du9b/fffu3bV06VL//iuvvFKbNm3yfz9gwADNmzfviM4FILzYbDZlZmYGXX29UmZm5hH/+wIAAACEszpPX2/evLk6dOjwewNRUUpISFCfPn2Ul5enlJSUkBd5NJi+DkQG7lMOAACAxqIuObTOI+UDBgzQihUrjrg4VOXz+eR0OlVWVqaEhAQlJSUxKogmJzU1Vd26deN3AQAAAE1KnUN5TYH8xx9/VJcuXY6qoKaG0UHgdzabjcXcAAAA0KSEdAjquuuuC2VzjV5RUZEKCgqq3KPZ5XKpoKBARUVFFlUGAAAAAGgItRopt9lsioqKqu9amhSfz6fCwsIajyksLFS3bt2YvgsAAAAAjVStQnlaWtphVzs3xmjKlCmhqKlJcDqdVUbID+VyueR0OpnOCwAAAACNVK1C+fTp05Wenl6r41A7ZWVlIT0OAAAAABB5ajUvOisrq1aNffzxx0dVTFOSkJAQ0uMAAAAAAJGnzquvS9KHH36oN998U9u3b9fBtzkvLCzUnDlzQlZcY5aUlCS73V7jFHa73a6kpKQGrAoAAAAA0JDqvILYokWLdMUVV6i4uFhvvPGGjDFyu9166623dOqpp9ZHjY2SzWZTZmZmjcdkZmayyBsAAAAANGJ1Hil/4okn9N///lfHHXechg4dqsWLF0uSdu3axUJvdZSamqqsrCzuUw4AAAAATVSdQ3mLFi103HHHSZK8Xq9/e5s2bbRt27bQVdZEpKamqlu3bnI6nSorK1NCQoKSkpIYIQcAAACAJqBWyW/Tpk3+7/ft26effvpJ0oGA/vLLL0uS3n//fX377bf1UGLjZ7PZlJycrF69eik5OZlADgAAAECSlJeXJ4fDEXSfw+FQXl5ewxaEkKtV+hs3bpz2798vSTr//PM1ePBgbd68WbfeeqtGjx6tuLg4DRs2TBMmTKjXYgEAAACgKYmOjlZubm6VYO5wOJSbm6vo6GiLKkOo1Gr6utPp1JlnnqnTTjtNY8eO9X8a07lzZ3300Uf66KOP1KNHj8MuXAYAAAAAqL2cnBxJUm5urv9xZSDPz8/370fkijIH39OsGlOmTNHcuXO1du1aPffcc/roo4+UkZGhsWPHql+/fg1R5xFzuVxKTExUaWmp7Ha71eUAAAAAQJ1VBvG4uDh5PB4CeZirSw6tVSg/lM/n0zvvvKPnn39eGzdu1IUXXqixY8fq5JNPPuKi6wuhHAAAAEBjEB8fL4/Ho7i4OLndbqvLQQ3qkkOPaEUxm82m8847T4sXL9YHH3wgm82mnj17qn///kdUMAAAAACgeg6Hwx/IPR5PtYu/IfIc8TLfW7du1UMPPaRBgwZpxowZMsaoXbt2oawNAAAAAJq8g68hd7vdys/PD7r4GyJTrRZ6e/zxx3XzzTertLRU//znP/Xcc89p1apV8vl8Gjx4sP7yl79o9OjRatOmTX3XCwAAAABNRrBF3YIt/obIVatQPmfOHL311lt644035Ha7lZaWpvvuu09jxozRCSecUN81AgAAAECT5PV6gy7qVvnY6/VaURZCqFYLvdlsNp100kkaM2aMxowZo9TU1IaoLSRY6A0AAAAA0JDqkkNrNVI+aNAgffjhhyEpDgAAAAAAHFCrhd4KCgrquw4AAAAAAJqcWoXyTp061XcdAAAAAAA0OUd8SzQAAAAAAHB0COUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAACEkby8PDkcjqD7HA6H8vLyGrYgAABQrwjlAACEkejoaOXm5lYJ5g6HQ7m5uYqOjraoMgAAUB9irC4AAAD8LicnR5KUm5vrf1wZyPPz8/37AQBA4xBljDFWF1GfXC6XEhMTVVpaKrvdbnU5AADUSmUQj4uLk8fjIZADABBB6pJDCeUAAISp+Ph4eTwexcXFye12W10OAACopbrkUK4pBwAgDDkcDn8g93g81S7+BgAAIhuhHACAMHPwNeRut1v5+flBF38DAACRj4XeAAAII8EWdQu2+BsAAGgcCOUAAIQRr9cbdFG3ysder9eKsgAAQD1hoTcAAAAAAEKIhd4AAAAAAIgAhHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKEcgAAAAAALEIoBwAAAADAIjFWFwAA+J3P55PT6VRZWZkSEhKUlJQkm43PTwEAABorQjkAhImioiIVFhbK5XL5t9ntdmVmZio1NdXCygAAAFBfGH4BgDBQVFSkgoKCgEAuSS6XSwUFBSoqKrKoMgAAANQnQjkAWMzn86mwsLDGYwoLC+Xz+RqoIgAAADQUQjkAWMzpdFYZIT+Uy+WS0+lsoIoAAADQUAjlAGCxsrKykB4HAACAyEEoBwCLJSQkhPQ4AAAARA5COQBYLCkpSXa7vcZj7Ha7kpKSGqgiAAAANBRCOQBYzGazKTMzs8ZjMjMzuV85AABAI8RfeAAQBlJTU5WVlVVlxNxutysrK4v7lAMAADRSMVYXAAA4IDU1Vd26dZPT6VRZWZkSEhKUlJTECDkAAEAjRigHgDBis9mUnJxsdRkAAABoIAy/AAAAAABgEUbKAQBhxefzMYUfAAA0GYRyAEDYKCoqUmFhoVwul3+b3W5XZmYmi90BAIBGiaEHAEBYKCoqUkFBQUAglySXy6WCggIVFRVZVBkAAED9IZQDACzn8/lUWFhY4zGFhYXy+XwNVBEAAEDDIJQDACzndDqrjJAfyuVyyel0NlBFAAAADYNQDgCwXFlZWUiPAwAAiBSEcgCA5RISEkJ6HAAAQKQglAMALJeUlCS73V7jMXa7XUlJSQ1UEQAAQMMIm1C+YMECRUVFaeXKlZKk/fv368knn9TQoUM1bNgw9evXTzfccIN+/vlnawsFAISczWZTZmZmjcdkZmZyv3IAANDohMVfN1u3btWcOXMCtm3fvl2TJ0/WI488ovfee0+rV69WcXGxRo0aZVGVAID6lJqaqqysrCoj5na7XVlZWdynHAAANEoxVhcgSZMnT9Zdd92liRMn+rfFxcVpwoQJ6t27tyQpPj5ef/zjHzV69Ght27ZNHTt2tKpcAEA9SU1NVbdu3eR0OlVWVqaEhAQlJSUxQg4AABoty0P5smXLFBsbqxEjRgRsb9eunR577LGAbc2aNZMkud3uattzu90B+w93ix0AQHix2WxKTk62ugzL+Xw+PpwAAKAJsDSU7927V3fffbeWL19eY9CutGbNGp1xxhk1/rE2a9Ys3XvvvSGsEgCAhlVUVKTCwsKAD5btdrsyMzOZxg8AQCNj6UfuOTk5mjhxYq2mov/8889atGiRFixYUONx06dPV2lpqf9r8+bNoSoXAIB6V1RUpIKCgiozvVwulwoKClRUVGRRZQAAoD5YFsrXr1+vdevWBVxHXp39+/drzJgxmjlzpvr371/jsfHx8bLb7QFfAABEAp/Pp8LCwhqPKSwslM/na6CKAABAfbNs+vrrr7+u8vJyDRs2TJJUUVEhScrOzlarVq305JNPqmvXrvL5fLr22mt1zjnn6IYbbrCqXAAA6p3T6TzsWigul0tOp5Pr7gEAaCQsC+U5OTnKycnxPy4pKVFKSormzZunjIwM//ZbbrlFSUlJuvPOOyVJ77zzjk488USdeOKJDV0yAAD1qqysLKTHAQCA8BfWy7hOmzZNmzZt0uWXX65PPvlEn3zyiQoKCuR0Oq0uDQCAkEtISAjpcQAAIPxZfks06cCU9bVr1/q/7969u3JycnT//fdLks4444yA48eOHdvgNQIAUN+SkpJkt9trnMJut9uVlJTUgFUBAID6FBahfN68eUG3G2MathAAACxks9mUmZmpgoKCao/JzMzkfuUAADQi/K8OAEAYSU1NVVZWVpW7h9jtdmVlZXGfcgAAGpmwGCkHAAC/S01NVbdu3eR0OlVWVqaEhAQlJSUxQg4AQCNEKAcAIAzZbDZuewYAQBPAR+4AAAAAAFiEUA4AAAAAgEUI5QCAsJCXlyeHwxF0n8PhUF5eXsMWBAAA0AAI5QCAsBAdHa3c3NwqwdzhcCg3N1fR0dEWVQYAAFB/WOgNABAWcnJyJEm5ubn+x5WBPD8/378fAACgMYkyxhiri6hPLpdLiYmJKi0trXLPVwBA+KkM4nFxcfJ4PARyAAAQceqSQwnlAICwEx8fL4/Ho7i4OLndbqvLAQAAqJO65FCuKQcAhBWHw+EP5B6Pp9rF3wAAABoDQjkAIGwcfA252+1Wfn5+0MXfAAAAGgsWegMAhIVgi7oFW/wNAACgMSGUAwDCgtfrDbqoW+Vjr9drRVkAAAD1ioXeAAAAAAAIIRZ6AwAAAAAgAhDKAQAAAACwCKEcAAAAAACLEMoBWC4vL6/aW145HA7l5eU1bEEAAABAAyGUW4ggAhwQHR0d9F7UlbfIio6OtqgyAAAAoH5xSzQLVQYRKfDeuwffqxdoCoLdizrYPasBAACAxoZQbiGCCPC7g38fZs6cKY/Hw+8BAAAAGj3uUx4GKoN4XFwcQQRNXnx8vDwej+Li4uR2u60uBwAAAKgz7lMeYXJycvyBPC4ujkCOJsvhcPh/DzweT7VrLgAAAACNBaE8DBBEgMC1FNxut/Lz84Mu/gYAAAA0JlxTbrFDryGvfCyJEXM0GcHWUgi25gIAAADQ2BDKLUQQAQ7wer1B11KofOz1eq0oCwAAAKh3LPRmoby8PEVHRwcN3g6HQ16vl3uVAwAAAECEqUsOJZQDAAAAABBCrL4OAAAAAEAEIJQDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAACCt5eXlyOBxB9zkcDu5MAgBoVAjlAAAgrERHRys3N7dKMHc4HMrNzVV0dLRFlQEAEHoxVhcAAABwsJycHElSbm6u/3FlIM/Pz/fvBwCgMeA+5QAAICxVBvG4uDh5PB4COQAgYtQlhxLKAQBA2IqPj5fH41FcXJzcbrfV5QAAUCt1yaFcUw4AAMKSw+HwB3KPx1Pt4m8AAEQyQjkAAAg7B19D7na7lZ+fH3TxNwAAIh0LvQEAgLASbFG3YIu/AQDQGBDKAQBAWPF6vUEXdat87PV6rSgLAIB6wUJvAAAAAACEEAu9AQAAAAAQAQjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFiEUA4AAAAAgEUI5QAAAAAAWIRQDgAAAACARQjlAAAAAABYhFAOAAAAAIBFCOUAAAAAAFgkbEL5ggULFBUVpZUrVwZsf/zxx9WvXz8NHjxYF154obZs2WJNgQAAAAAAhFhYhPKtW7dqzpw5Vba/9NJLuvfee7V8+XJ99NFHOvPMMzVy5Ej5fD4LqgQAAAAAILTCIpRPnjxZd911V5XtM2fO1LXXXqvjjjtOknTbbbfpyy+/1Ouvv97QJQIAAAAAEHKWh/Jly5YpNjZWI0aMCNi+e/duffbZZzr99NP92xITE3XKKafonXfeqbY9t9stl8sV8AUAAAAAQDiyNJTv3btXd999t+bOnVtlX3FxsSSpffv2Ads7dOjg3xfMrFmzlJiY6P/q3LlzaIsGAAAAACBELA3lOTk5mjhxojp27Fhl3759+yRJ8fHxAdvj4+P9+4KZPn26SktL/V+bN28ObdEAAAAAAIRIjFUnXr9+vdatW6cHH3ww6P4WLVpIOjAd/WBut1stW7astt34+PgqQR4AAAAAgHBkWSh//fXXVV5ermHDhkmSKioqJEnZ2dlq1aqVfzX2HTt2BDxv+/btOvfccxu2WAAAAAAA6kGUMcZYXYQklZSUKCUlRStWrFBGRoYkqW/fvsrMzNR9990nSXK5XGrTpo1efvlljRw5slbtulwuJSYmqrS0VHa7vb7KBwAAAABAUt1yqOWrr9fknnvu0dNPP61du3ZJkh599FH17NlTF1xwgcWVAQAAAABw9Cybvn6w7OxsrV271v999+7dtXTpUl122WX66aefdO6556pZs2Zq3bq1li1bJpstrD9LAAAAAACgVsJm+np9Yfo6AAAAAKAhNZrp6wAAAAAANGaEcgAAAAAALEIoBwAAAADAIoRyAAAAAAAsQigHAAAAAMAihHIAAAAgDOXl5cnhcATd53A4lJeX17AFAagXhHIAAAAgDEVHRys3N7dKMHc4HMrNzVV0dLRFlQEIpRirCwAAAABQVU5OjiQpNzfX/7gykOfn5/v3A4hsUcYYY3UR9akuN20HAAAAwk1lEI+Li5PH4yGQAxGgLjmUUA4AAACEufj4eHk8HsXFxcntdltdDoDDqEsO5ZpyAAAAIIw5HA5/IPd4PNUu/gYgMhHKAQAAgDB18DXkbrdb+fn5QRd/AxC5WOgNAAAACEPBFnULtvgbgMhGKAcAAADCkNfrDbqoW+Vjr9drRVkAQoyF3gAAAAAACCEWegMAAAAAIAIQygEAAAAAsAihHAAAAAAAixDKAQAAAACwCKEcAAAAQFjKy8ur9p7sDodDeXl5DVsQUA8I5QAAAADCUnR0tHJzc6sE88p7uEdHR1tUGRA63KccAAAAQFiqvCd7bm6u/3FlIA92D3cgEnGfcgAAAABhrTKIx8XFyePxEMgR9uqSQwnlAAAAAMJefHy8PB6P4uLi5Ha7rS4HqFFdcijXlAMAAAAIaw6Hwx/IPR5PtYu/AZGIUA4AAAAgbB18Dbnb7VZ+fn7Qxd+ASMVCbwAAAADCUrBF3YIt/gZEMkI5AAAAgLDk9XqDLupW+djr9VpRFhBSLPQGAAAAAEAIsdAbAAAAAAARgFAOAGEgLy+v2gVrHA6H8vLyGrYgAAAANAhCOQCEgejo6KAryVYucBMdHW1RZQAAAKhPLPQGAGEg2EqywVacBQAAQOPCQm8AEEYqg3hcXJw8Hg+BHAAAIALVJYcSygEgzMTHx8vj8SguLk5ut9vqcgAAAFBHrL4OABHK4XD4A7nH46l28TcAAAA0DoRyAAgTB19D7na7lZ+fH3TxNwAAADQeLPQGAGEg2KJuwRZ/AwAAQONCKAeAMOD1eoMu6lb52Ov1WlEWAAAA6hkLvQEAAAAAEEIs9AYAAAAAQAQglAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWIZQDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWIZQDAAAAAGARQjkAAAAAABYhlAMAAAAAYJEYqwuob8YYSZLL5bK4EgAAAABAU1CZPyvzaE0afSgvKyuTJHXu3NniSgAAAAAATUlZWZkSExNrPCbK1Ca6RzCfz6etW7cqISFBUVFRVpeDGrhcLnXu3FmbN2+W3W63uhxYhH4AiX6A39EXINEPcAD9AFLk9ANjjMrKytSpUyfZbDVfNd7oR8ptNptOOOEEq8tAHdjt9rD+BUPDoB9Aoh/gd/QFSPQDHEA/gBQZ/eBwI+SVWOgNAAAAAACLEMoBAAAAALAIoRxhIz4+XjNmzFB8fLzVpcBC9ANI9AP8jr4AiX6AA+gHkBpnP2j0C70BAAAAABCuGCkHAAAAAMAihHIAAAAAACxCKAcAAAAAwCKE8iaqoKBA5513noYPH64zzjhDo0ePVklJiX+/MUb5+fnq27ev+vfvr6uvvlqlpaX+/f/73/80depUDRkyROnp6erbt68ef/zxgHN8/vnnuuaaa/zH9OzZUzk5OfL5fDXWFopzV8fj8WjatGmKiYkJeL0Hc7lcuv766xUVFVWrNiMdfSF4X1i8eLGGDRumc845RwMGDNDAgQP11ltv1artSEQ/CN4Pxo8frwEDBigjI8P/NWnSpFq1HYnoB8H7QatWrQL6QEZGhk444QSNGzeuVu1HGvpB8H5QVlam2267TQMHDlT//v01YsQIff/997VqOxI1xX5wuNdcafv27brooouUnJx82DYjHf0geD+YM2eOzj77bJ1zzjk6/fTTdc455+iTTz45bNs1vRg0QbGxsaawsNAYY4zX6zXXXHON6datm6moqDDGGPPQQw+Z3r17m3379hljjLnuuuvMRRdd5H++w+Eww4YNM+Xl5cYYY7744gsTHx9vFi9e7D9m1qxZ5vrrrzc+n88YY4zT6TSJiYlm/vz5NdYWinMHU1xcbAYMGGDGjRtnJJni4uIqx6xfv9707dvXjB492jSVXw/6QvC+0L17d/P+++/7Hz/66KMmPj7e7Ny5s8a2IxX9IHg/uPbaa4Nub6zoB8H7QXp6epVt/fr1M6+99lqNbUcq+kHwfnDZZZeZc845x3g8Hv9rOOmkk/zvS2PTFPvB4V6zMcYsX77c9O3b15x//vmmS5cuNbbXGNAPgveD1q1bm02bNvkfT5061bRt29Z4vd4a265O00gdqGLUqFEBjz/++GMjyaxevdrs37/ftG3b1ixcuNC/f+PGjUaS+fzzz40xxixatMi8+eabAW1ceOGF5rzzzvM//uGHH8yOHTsCjunbt6+57bbbqq0rVOcO5osvvjDffvutWbFiRbX/4a5Zs8Zs27bNLF68uMmEcvpC8L6wdu3agMeff/65kWQ+++yzGtuOVPQDQrkx9IPq+sEPP/xQ5TkdO3Y0+/fvr7HtSEU/qNoPtm3bZiSZl156yb9t3759JioqyjzzzDM1th2pmmI/qOk1V3r33XeNy+UyM2bMaBKhnH4QvB8c+nfiv//9byPJ7Nmzp8a2qxNz5GPsiGQvvvhiwONmzZpJktxutz7//HPt3LlTp59+un9/amqqWrZsqXfeeUe9evXShAkTqrTZrFkz/frrr/7HKSkpAftfe+01OZ1OjR8/vtq6QnXuYHr27CnpwFSW6gwYMKDGNhoj+kJwZ555pv/7vXv36pFHHtHQoUPVq1evGtuOVPQDSPSD6hxa89NPP61x48YpOjq6xrYjFf2gKqfTKUlq3769f1vz5s2VmJioVatW6Zprrqmx/UjUFPtBTa+50rBhw2pso7GhHwTvBwf/nbh7924tXLhQ48aNU6tWrWpsuzpcUw5J0po1a9SpUycNHjxYP/zwg6TA/3iioqLUvn17FRcXB32+MUbr1q1TVlZWlX2LFi1SUlKSJk2apH/961/q06dPtXWE+tyoO/pCoEsuuUTt2rXTTz/9pJdffrnR/hF+KPrB72bNmqWMjAydddZZuuWWW7Rjx46QtBsJ6AdVeb1e/eMf/6jxj8XGhn4g/7XDleFcOvCBbWlpaZP5YK8p9oODXzMOoB/8zuv1asCAAerUqZM6dOigJ598sk7tHoxQDrndbs2ZM0cLFixQbGys9u3bJ0mKj48POC4+Pt6/71CLFy9W+/btddNNN1XZd/3118vpdOqxxx7TyJEj9e6771ZbS6jPjbqhL1T1yiuv6Oeff9axxx6r9PT0as/dmNAPfnfKKafo7LPP1nvvvacVK1bI7XZrwIABh/2UvTGgHwS3fPlyJScnq3v37iFtN1zRDw5o166drrjiCj388MMqLS31LzAVExMjr9d7VG1HgqbYDw59zaAfHNoPoqOjtXbtWm3btk1btmzRyJEjZYypddsHI5RDN998s6644gpdeumlkqQWLVpICpyiUfm4ct/BPv/8c91///16+eWXFRNT/RURF110kS666CJNmzZNklRYWBiwku327dtDcu7t27cHtFtYWFjbt6LJoy8E17x5c82fP1+bNm3S4sWLj6iNSEI/+N1dd92lq666SjabTbGxsXr44YfldDr1/PPP17qNSEU/CG7JkiW67rrrjui5kYh+8LvFixfrnHPO0fnnn6+hQ4eqffv2Ovvss9W6detatxGpmmI/OPQ1g35QndatW2v+/Pl666239MYbb1R7XI2O6Ep0NBp33nmn+eMf/xiwbf369UaS+eSTTwK2t2zZ0sydOzdg2/fff2969eplNm7cWKVtt9tdZVt+fr5p0aJFtfWE6tw1qWkxn0pNaaG3SvSF3/l8Pv/qugc78cQTzaRJk+p0jkhDPzi89u3bmzvvvLNO54g09IPgdu/ebVq3bm1KS0vr1Hakoh8c3qmnnmocDkedzhFpmmI/CPaaD9VUFnqrRD/4ndfrNb/99lvANp/PZ2JiYswDDzxQp3NUYqS8CZs9e7Y2b96sBQsWSJI+/fRTffrpp+rdu7fatm2rTz/91H9sUVGR9u7dq3POOce/bevWrbrsssv01FNPqUePHpKkJ554wr//vPPO088//xxwzm3btqlTp07V1hSqc6Nu6AuBfvzxxyqfiHq9Xu3cubPGmiMd/aCq2267LeCx2+3Wrl27lJSUdNRthyv6QfWWLl2qkSNHym63h6zNcEU/qGrt2rWqqKjwP965c6e+/vprXXbZZUfddrhqiv2gutfclNEPAvvBqlWrlJ2dHXD8zp07tX///iP/O/GIojwi3l//+ldz6qmnmjVr1piPP/7YfPzxx2bGjBn++/Y99NBDJi0tzX/fv+uvvz7gvn8///yzOfXUU82DDz7of/7HH39sBg4c6D8mPT3d3HHHHf57Dm7cuNHY7XZz33331VhbKM5dE0bKA9EXqvaF4uJi06xZs4BPX2fOnGlatGhhvv3221q1HWnoB8H/TYiLizMff/yx//E999xj2rZta3766adatR1p6Ac1/9/Qv39/895779WqvUhGPwjeDy688ELz9NNPG2MOjJRdd911ZuLEibVqNxI1xX5wuNd8sKYyUk4/qPqaV6xYYdq2bev/d8Lr9ZqbbrrJdOjQwezatevwb2oQTSN1IIDL5TI2m81IqvJV2dl8Pp+59957zWmnnWbOOOMMM3bs2ID77t1+++1Bn3/wP05vvvmmueCCC0z//v3NWWedZfr06WMeeuihw97XNRTnDsbtdpv09HSTlpZmJJkzzzyzyn0If/zxR5Oenm66detmJJn09HRz66231uZtjUj0heB9oby83Pz5z382p59+uhkyZIjp37+/GT58uPnoo49q+9ZGFPpB9f8mPProo+ass84yGRkZpn///ubCCy80X375ZW3e1ohDP6i+HxhjTFFRkUlJSfH/0dhY0Q+q7wcPPPCAOemkk8ygQYPMoEGDzIwZM6pMYW0smmI/qM1rNsaYdevWmfT0dNOlSxcTHx9v0tPTzcyZM+vw7kYO+kHw17xr1y4zffp0c9ppp5khQ4aYfv36mYsvvrjO0+MPFmXMES4RBwAAAAAAjgrXlAMAAAAAYBFCOQAAAAAAFiGUAwAAAABgEUI5AAAAAAAWIZQDAAAAAGARQjkAAAAAABYhlAMAAAAAYBFCOQAAAAAAFomxugAAAFA/kpOTlZycLEmqqKjQunXrlJaWplatWkmSNmzYoLfffluXXXaZvv32WzVr1sy6YgEAaKII5QAANGIrV66UJJWUlCglJUXz5s1TRkaGJCkjI0MJCQnq1q2bYmNjrSsSAIAmjFAOAEAjlZ2dXeP+8ePHq0OHDnrnnXcapiAAAFAF15QDANBIHS6U9+/fX5dccomioqL8I+r33HOPkpOTlZGRoQceeEBDhw7VySefrDfeeEP//e9/lZWVpW7duulPf/pTQFv79+/XnXfeqT59+ig9PV3nnXeevvzyy3p6ZQAANB6EcgAAmqgePXr4w3ilmTNnavz48fr00081YMAArVixQnfccYcmTJigt956SwUFBVq9erUWLVqk999/3/+83NxcrV27VuvWrdP777+v6667TkOHDlVZWVkDvyoAACILoRwAAFTRvn17nX322ZKkwYMHa8eOHRo4cKAkqU2bNurRo4c+++wzSVJ5ebnmzp2ryZMnKz4+XpI0ZswYVVRUqKCgwJoXAABAhOCacgAAUEXHjh3937do0aLKtpYtW6q0tFSS9N1336miokKzZs3SggUL/Me0b99ee/bsaaCKAQCITIRyAABQRXR09GG3GWMCHj/44IMaOnRovdYFAEBjw/R1AABwVLp27apmzZrp66+/Dti+YMECrVq1yqKqAACIDIRyAABwVJo3b64pU6ZowYIF/unq3377rR555BGdeuqpFlcHAEB4I5QDANDIFRYW6sorr5R04DZpldd9f/XVV8rIyPBv/+c//6nZs2dryZIl2rBhg8aNG6evvvrK/9wrr7xSX331lcaNG6cNGzZoyZIlmj17tiQpPz9fF110kQYOHKj09HRNmjRJzz//vNq0adPwLxgAgAgSZQ69IAwAAAAAADQIRsoBAAAAALAIoRwAAAAAAIsQygEAAAAAsAihHAAAAAAAixDKAQAAAACwCKEcAAAAAACLEMoBAAAAALAIoRwAAAAAAIsQygEAAAAAsAihHAAAAAAAixDKAQAAAACwyP8Hlxyl+Lu/89EAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "    model_metrics['TFT-GRU'] = rolling_test(TFT_GRU, X, y, chunk=(window + predictions), predictions=predictions, sequence_length=sequence_length, model_name='TFT-GRU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "olkznzaHPAhs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24438e2c-90e5-4a55-8bfd-5bd8e75fb124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[40]\tvalid_0's rmse: 0.0558919\n",
            "[50]\tvalid_0's rmse: 0.0346933\n",
            "[60]\tvalid_0's rmse: 0.0247689\n",
            "[70]\tvalid_0's rmse: 0.0163023\n",
            "[80]\tvalid_0's rmse: 0.00937933\n",
            "[90]\tvalid_0's rmse: 0.0076254\n",
            "[100]\tvalid_0's rmse: 0.00773952\n",
            "[110]\tvalid_0's rmse: 0.00773952\n",
            "Early stopping, best iteration is:\n",
            "[94]\tvalid_0's rmse: 0.00740255\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.178588\n",
            "[20]\tvalid_0's rmse: 0.125999\n",
            "[30]\tvalid_0's rmse: 0.0861043\n",
            "[40]\tvalid_0's rmse: 0.0587501\n",
            "[50]\tvalid_0's rmse: 0.039975\n",
            "[60]\tvalid_0's rmse: 0.029596\n",
            "[70]\tvalid_0's rmse: 0.0269145\n",
            "[80]\tvalid_0's rmse: 0.0243196\n",
            "[90]\tvalid_0's rmse: 0.0228443\n",
            "[100]\tvalid_0's rmse: 0.0219747\n",
            "[110]\tvalid_0's rmse: 0.0219747\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 0.0219747\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.218494\n",
            "[20]\tvalid_0's rmse: 0.152705\n",
            "[30]\tvalid_0's rmse: 0.105112\n",
            "[40]\tvalid_0's rmse: 0.0766153\n",
            "[50]\tvalid_0's rmse: 0.0545656\n",
            "[60]\tvalid_0's rmse: 0.037786\n",
            "[70]\tvalid_0's rmse: 0.0314551\n",
            "[80]\tvalid_0's rmse: 0.0262581\n",
            "[90]\tvalid_0's rmse: 0.0213592\n",
            "[100]\tvalid_0's rmse: 0.0202213\n",
            "[110]\tvalid_0's rmse: 0.0202213\n",
            "Early stopping, best iteration is:\n",
            "[93]\tvalid_0's rmse: 0.0202213\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.21326\n",
            "[20]\tvalid_0's rmse: 0.150902\n",
            "[30]\tvalid_0's rmse: 0.106763\n",
            "[40]\tvalid_0's rmse: 0.0775905\n",
            "[50]\tvalid_0's rmse: 0.0563106\n",
            "[60]\tvalid_0's rmse: 0.0429439\n",
            "[70]\tvalid_0's rmse: 0.0358382\n",
            "[80]\tvalid_0's rmse: 0.0301198\n",
            "[90]\tvalid_0's rmse: 0.0250575\n",
            "[100]\tvalid_0's rmse: 0.0219542\n",
            "[110]\tvalid_0's rmse: 0.0219542\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.0219542\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.168559\n",
            "[20]\tvalid_0's rmse: 0.113555\n",
            "[30]\tvalid_0's rmse: 0.0746953\n",
            "[40]\tvalid_0's rmse: 0.0494516\n",
            "[50]\tvalid_0's rmse: 0.0321499\n",
            "[60]\tvalid_0's rmse: 0.0221302\n",
            "[70]\tvalid_0's rmse: 0.0141549\n",
            "[80]\tvalid_0's rmse: 0.00760146\n",
            "[90]\tvalid_0's rmse: 0.00218681\n",
            "[100]\tvalid_0's rmse: 0.000648415\n",
            "[110]\tvalid_0's rmse: 0.000648415\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 6.68621e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.227933\n",
            "[20]\tvalid_0's rmse: 0.168671\n",
            "[30]\tvalid_0's rmse: 0.128462\n",
            "[40]\tvalid_0's rmse: 0.10089\n",
            "[50]\tvalid_0's rmse: 0.0816028\n",
            "[60]\tvalid_0's rmse: 0.0671485\n",
            "[70]\tvalid_0's rmse: 0.0612383\n",
            "[80]\tvalid_0's rmse: 0.0561492\n",
            "[90]\tvalid_0's rmse: 0.0516348\n",
            "[100]\tvalid_0's rmse: 0.0491086\n",
            "[110]\tvalid_0's rmse: 0.0491086\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.0491086\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.238093\n",
            "[20]\tvalid_0's rmse: 0.180264\n",
            "[30]\tvalid_0's rmse: 0.139868\n",
            "[40]\tvalid_0's rmse: 0.113133\n",
            "[50]\tvalid_0's rmse: 0.093519\n",
            "[60]\tvalid_0's rmse: 0.0781693\n",
            "[70]\tvalid_0's rmse: 0.0717669\n",
            "[80]\tvalid_0's rmse: 0.0660309\n",
            "[90]\tvalid_0's rmse: 0.0612506\n",
            "[100]\tvalid_0's rmse: 0.0584145\n",
            "[110]\tvalid_0's rmse: 0.0584145\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.0584145\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.16811\n",
            "[20]\tvalid_0's rmse: 0.156381\n",
            "[30]\tvalid_0's rmse: 0.116644\n",
            "[40]\tvalid_0's rmse: 0.0890888\n",
            "[50]\tvalid_0's rmse: 0.0684284\n",
            "[60]\tvalid_0's rmse: 0.0541404\n",
            "[70]\tvalid_0's rmse: 0.0464251\n",
            "[80]\tvalid_0's rmse: 0.0394729\n",
            "[90]\tvalid_0's rmse: 0.0337178\n",
            "[100]\tvalid_0's rmse: 0.0284874\n",
            "[110]\tvalid_0's rmse: 0.0272437\n",
            "[120]\tvalid_0's rmse: 0.0272437\n",
            "Early stopping, best iteration is:\n",
            "[102]\tvalid_0's rmse: 0.0272437\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.156469\n",
            "[20]\tvalid_0's rmse: 0.119953\n",
            "[30]\tvalid_0's rmse: 0.0834518\n",
            "[40]\tvalid_0's rmse: 0.0590579\n",
            "[50]\tvalid_0's rmse: 0.0404778\n",
            "[60]\tvalid_0's rmse: 0.0301939\n",
            "[70]\tvalid_0's rmse: 0.0241317\n",
            "[80]\tvalid_0's rmse: 0.019351\n",
            "[90]\tvalid_0's rmse: 0.015452\n",
            "[100]\tvalid_0's rmse: 0.012465\n",
            "[110]\tvalid_0's rmse: 0.012465\n",
            "Early stopping, best iteration is:\n",
            "[99]\tvalid_0's rmse: 0.012465\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.183431\n",
            "[20]\tvalid_0's rmse: 0.132614\n",
            "[30]\tvalid_0's rmse: 0.0984828\n",
            "[40]\tvalid_0's rmse: 0.0748727\n",
            "[50]\tvalid_0's rmse: 0.0572017\n",
            "[60]\tvalid_0's rmse: 0.0447624\n",
            "[70]\tvalid_0's rmse: 0.03837\n",
            "[80]\tvalid_0's rmse: 0.0330254\n",
            "[90]\tvalid_0's rmse: 0.0286685\n",
            "[100]\tvalid_0's rmse: 0.0250386\n",
            "[110]\tvalid_0's rmse: 0.0247624\n",
            "[120]\tvalid_0's rmse: 0.0247624\n",
            "Early stopping, best iteration is:\n",
            "[101]\tvalid_0's rmse: 0.0247624\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.31351\n",
            "[20]\tvalid_0's rmse: 0.265451\n",
            "[30]\tvalid_0's rmse: 0.228818\n",
            "[40]\tvalid_0's rmse: 0.20802\n",
            "[50]\tvalid_0's rmse: 0.18894\n",
            "[60]\tvalid_0's rmse: 0.178377\n",
            "[70]\tvalid_0's rmse: 0.172004\n",
            "[80]\tvalid_0's rmse: 0.166877\n",
            "[90]\tvalid_0's rmse: 0.162629\n",
            "[100]\tvalid_0's rmse: 0.157169\n",
            "[110]\tvalid_0's rmse: 0.157169\n",
            "Early stopping, best iteration is:\n",
            "[99]\tvalid_0's rmse: 0.157169\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.313939\n",
            "[20]\tvalid_0's rmse: 0.258319\n",
            "[30]\tvalid_0's rmse: 0.213144\n",
            "[40]\tvalid_0's rmse: 0.178014\n",
            "[50]\tvalid_0's rmse: 0.158014\n",
            "[60]\tvalid_0's rmse: 0.138553\n",
            "[70]\tvalid_0's rmse: 0.129075\n",
            "[80]\tvalid_0's rmse: 0.124588\n",
            "[90]\tvalid_0's rmse: 0.119747\n",
            "[100]\tvalid_0's rmse: 0.113011\n",
            "[110]\tvalid_0's rmse: 0.113011\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's rmse: 0.113011\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.193356\n",
            "[20]\tvalid_0's rmse: 0.136504\n",
            "[30]\tvalid_0's rmse: 0.0904204\n",
            "[40]\tvalid_0's rmse: 0.0578881\n",
            "[50]\tvalid_0's rmse: 0.0440761\n",
            "[60]\tvalid_0's rmse: 0.028454\n",
            "[70]\tvalid_0's rmse: 0.0223512\n",
            "[80]\tvalid_0's rmse: 0.018216\n",
            "[90]\tvalid_0's rmse: 0.0147447\n",
            "[100]\tvalid_0's rmse: 0.0142\n",
            "[110]\tvalid_0's rmse: 0.0142\n",
            "Early stopping, best iteration is:\n",
            "[92]\tvalid_0's rmse: 0.0142\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.193043\n",
            "[20]\tvalid_0's rmse: 0.131948\n",
            "[30]\tvalid_0's rmse: 0.0874411\n",
            "[40]\tvalid_0's rmse: 0.0593589\n",
            "[50]\tvalid_0's rmse: 0.0479793\n",
            "[60]\tvalid_0's rmse: 0.0337021\n",
            "[70]\tvalid_0's rmse: 0.0275156\n",
            "[80]\tvalid_0's rmse: 0.0232578\n",
            "[90]\tvalid_0's rmse: 0.0193942\n",
            "[100]\tvalid_0's rmse: 0.0170109\n",
            "[110]\tvalid_0's rmse: 0.015683\n",
            "[120]\tvalid_0's rmse: 0.01495\n",
            "[130]\tvalid_0's rmse: 0.0145989\n",
            "[140]\tvalid_0's rmse: 0.0145989\n",
            "Early stopping, best iteration is:\n",
            "[127]\tvalid_0's rmse: 0.0145989\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.20753\n",
            "[20]\tvalid_0's rmse: 0.145598\n",
            "[30]\tvalid_0's rmse: 0.102518\n",
            "[40]\tvalid_0's rmse: 0.074359\n",
            "[50]\tvalid_0's rmse: 0.0601387\n",
            "[60]\tvalid_0's rmse: 0.0483885\n",
            "[70]\tvalid_0's rmse: 0.0430134\n",
            "[80]\tvalid_0's rmse: 0.0391229\n",
            "[90]\tvalid_0's rmse: 0.0358801\n",
            "[100]\tvalid_0's rmse: 0.0344354\n",
            "[110]\tvalid_0's rmse: 0.0344354\n",
            "Early stopping, best iteration is:\n",
            "[92]\tvalid_0's rmse: 0.0341493\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.237928\n",
            "[20]\tvalid_0's rmse: 0.175631\n",
            "[30]\tvalid_0's rmse: 0.131441\n",
            "[40]\tvalid_0's rmse: 0.102546\n",
            "[50]\tvalid_0's rmse: 0.0849804\n",
            "[60]\tvalid_0's rmse: 0.0712348\n",
            "[70]\tvalid_0's rmse: 0.0623425\n",
            "[80]\tvalid_0's rmse: 0.0585783\n",
            "[90]\tvalid_0's rmse: 0.0548657\n",
            "[100]\tvalid_0's rmse: 0.0536529\n",
            "[110]\tvalid_0's rmse: 0.0536529\n",
            "Early stopping, best iteration is:\n",
            "[92]\tvalid_0's rmse: 0.0536529\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0931548\n",
            "[20]\tvalid_0's rmse: 0.0506403\n",
            "[30]\tvalid_0's rmse: 0.0229322\n",
            "[40]\tvalid_0's rmse: 0.00661432\n",
            "[50]\tvalid_0's rmse: 0.00394023\n",
            "[60]\tvalid_0's rmse: 0.0144937\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's rmse: 6.22023e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0961404\n",
            "[20]\tvalid_0's rmse: 0.0527376\n",
            "[30]\tvalid_0's rmse: 0.0229156\n",
            "[40]\tvalid_0's rmse: 0.00439414\n",
            "[50]\tvalid_0's rmse: 0.00987234\n",
            "[60]\tvalid_0's rmse: 0.015785\n",
            "Early stopping, best iteration is:\n",
            "[43]\tvalid_0's rmse: 0.000135271\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.113514\n",
            "[20]\tvalid_0's rmse: 0.0654295\n",
            "[30]\tvalid_0's rmse: 0.0279359\n",
            "[40]\tvalid_0's rmse: 0.00595764\n",
            "[50]\tvalid_0's rmse: 0.00690952\n",
            "[60]\tvalid_0's rmse: 0.0138768\n",
            "Early stopping, best iteration is:\n",
            "[43]\tvalid_0's rmse: 0.000305813\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.136326\n",
            "[20]\tvalid_0's rmse: 0.0951594\n",
            "[30]\tvalid_0's rmse: 0.0677535\n",
            "[40]\tvalid_0's rmse: 0.0431222\n",
            "[50]\tvalid_0's rmse: 0.027012\n",
            "[60]\tvalid_0's rmse: 0.0151957\n",
            "[70]\tvalid_0's rmse: 0.00943755\n",
            "[80]\tvalid_0's rmse: 0.00689737\n",
            "[90]\tvalid_0's rmse: 0.00781323\n",
            "[100]\tvalid_0's rmse: 0.00476736\n",
            "[110]\tvalid_0's rmse: 0.00655944\n",
            "[120]\tvalid_0's rmse: 0.00877182\n",
            "Early stopping, best iteration is:\n",
            "[102]\tvalid_0's rmse: 0.00445985\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.139857\n",
            "[20]\tvalid_0's rmse: 0.105255\n",
            "[30]\tvalid_0's rmse: 0.0741946\n",
            "[40]\tvalid_0's rmse: 0.0518861\n",
            "[50]\tvalid_0's rmse: 0.0385434\n",
            "[60]\tvalid_0's rmse: 0.0274875\n",
            "[70]\tvalid_0's rmse: 0.0215505\n",
            "[80]\tvalid_0's rmse: 0.0180463\n",
            "[90]\tvalid_0's rmse: 0.01364\n",
            "[100]\tvalid_0's rmse: 0.0122763\n",
            "[110]\tvalid_0's rmse: 0.0122763\n",
            "Early stopping, best iteration is:\n",
            "[93]\tvalid_0's rmse: 0.0122763\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0605137\n",
            "[20]\tvalid_0's rmse: 0.0289848\n",
            "[30]\tvalid_0's rmse: 0.0101914\n",
            "[40]\tvalid_0's rmse: 0.00355696\n",
            "[50]\tvalid_0's rmse: 0.0123145\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's rmse: 0.000140609\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0798492\n",
            "[20]\tvalid_0's rmse: 0.0509768\n",
            "[30]\tvalid_0's rmse: 0.0332119\n",
            "[40]\tvalid_0's rmse: 0.0254731\n",
            "[50]\tvalid_0's rmse: 0.0155354\n",
            "[60]\tvalid_0's rmse: 0.0108655\n",
            "[70]\tvalid_0's rmse: 0.00593026\n",
            "[80]\tvalid_0's rmse: 0.00259353\n",
            "[90]\tvalid_0's rmse: 0.000512185\n",
            "[100]\tvalid_0's rmse: 0.00292926\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.000134187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.023592\n",
            "[20]\tvalid_0's rmse: 0.0253254\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.0102227\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00184881\n",
            "[20]\tvalid_0's rmse: 0.00308513\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's rmse: 0.000232387\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.502485\n",
            "[20]\tvalid_0's rmse: 0.502485\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.502485\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.511336\n",
            "[20]\tvalid_0's rmse: 0.511336\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.511336\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444358\n",
            "[20]\tvalid_0's rmse: 0.444358\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.444358\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.398313\n",
            "[20]\tvalid_0's rmse: 0.398313\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.398313\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416187\n",
            "[20]\tvalid_0's rmse: 0.416187\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.416187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.42288\n",
            "[20]\tvalid_0's rmse: 0.42288\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.42288\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.496021\n",
            "[20]\tvalid_0's rmse: 0.496021\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.496021\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.374953\n",
            "[20]\tvalid_0's rmse: 0.374953\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.374953\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.490488\n",
            "[20]\tvalid_0's rmse: 0.490488\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.490488\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.505679\n",
            "[20]\tvalid_0's rmse: 0.505679\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.505679\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.367176\n",
            "[20]\tvalid_0's rmse: 0.367176\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.367176\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.464969\n",
            "[20]\tvalid_0's rmse: 0.464969\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.464969\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.461365\n",
            "[20]\tvalid_0's rmse: 0.461365\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.461365\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.441812\n",
            "[20]\tvalid_0's rmse: 0.441812\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.441812\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.365738\n",
            "[20]\tvalid_0's rmse: 0.365738\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.365738\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.308793\n",
            "[20]\tvalid_0's rmse: 0.248726\n",
            "[30]\tvalid_0's rmse: 0.202049\n",
            "[40]\tvalid_0's rmse: 0.165184\n",
            "[50]\tvalid_0's rmse: 0.13569\n",
            "[60]\tvalid_0's rmse: 0.112114\n",
            "[70]\tvalid_0's rmse: 0.0929223\n",
            "[80]\tvalid_0's rmse: 0.0771057\n",
            "[90]\tvalid_0's rmse: 0.063995\n",
            "[100]\tvalid_0's rmse: 0.0616369\n",
            "[110]\tvalid_0's rmse: 0.0594877\n",
            "[120]\tvalid_0's rmse: 0.0594749\n",
            "[130]\tvalid_0's rmse: 0.0575984\n",
            "[140]\tvalid_0's rmse: 0.0575877\n",
            "[150]\tvalid_0's rmse: 0.055911\n",
            "[160]\tvalid_0's rmse: 0.0551262\n",
            "Early stopping, best iteration is:\n",
            "[149]\tvalid_0's rmse: 0.0551177\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.325975\n",
            "[20]\tvalid_0's rmse: 0.265487\n",
            "[30]\tvalid_0's rmse: 0.221083\n",
            "[40]\tvalid_0's rmse: 0.18547\n",
            "[50]\tvalid_0's rmse: 0.156827\n",
            "[60]\tvalid_0's rmse: 0.133843\n",
            "[70]\tvalid_0's rmse: 0.115081\n",
            "[80]\tvalid_0's rmse: 0.108732\n",
            "[90]\tvalid_0's rmse: 0.108879\n",
            "[100]\tvalid_0's rmse: 0.111549\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.108714\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.1653\n",
            "[20]\tvalid_0's rmse: 0.106772\n",
            "[30]\tvalid_0's rmse: 0.0888746\n",
            "[40]\tvalid_0's rmse: 0.0754619\n",
            "[50]\tvalid_0's rmse: 0.0583175\n",
            "[60]\tvalid_0's rmse: 0.0447041\n",
            "[70]\tvalid_0's rmse: 0.0371782\n",
            "[80]\tvalid_0's rmse: 0.0378794\n",
            "[90]\tvalid_0's rmse: 0.0383332\n",
            "[100]\tvalid_0's rmse: 0.0362187\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.0346957\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.153506\n",
            "[20]\tvalid_0's rmse: 0.10009\n",
            "[30]\tvalid_0's rmse: 0.0604565\n",
            "[40]\tvalid_0's rmse: 0.0302806\n",
            "[50]\tvalid_0's rmse: 0.0165342\n",
            "[60]\tvalid_0's rmse: 0.0101459\n",
            "[70]\tvalid_0's rmse: 0.00892348\n",
            "[80]\tvalid_0's rmse: 0.014169\n",
            "Early stopping, best iteration is:\n",
            "[62]\tvalid_0's rmse: 0.00674689\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.239372\n",
            "[20]\tvalid_0's rmse: 0.186548\n",
            "[30]\tvalid_0's rmse: 0.146235\n",
            "[40]\tvalid_0's rmse: 0.116103\n",
            "[50]\tvalid_0's rmse: 0.0972231\n",
            "[60]\tvalid_0's rmse: 0.0912105\n",
            "[70]\tvalid_0's rmse: 0.0891752\n",
            "[80]\tvalid_0's rmse: 0.0872651\n",
            "[90]\tvalid_0's rmse: 0.0857785\n",
            "[100]\tvalid_0's rmse: 0.0819948\n",
            "[110]\tvalid_0's rmse: 0.0779774\n",
            "[120]\tvalid_0's rmse: 0.0741985\n",
            "[130]\tvalid_0's rmse: 0.0705691\n",
            "[140]\tvalid_0's rmse: 0.0672697\n",
            "[150]\tvalid_0's rmse: 0.0641871\n",
            "[160]\tvalid_0's rmse: 0.0612811\n",
            "[170]\tvalid_0's rmse: 0.05936\n",
            "[180]\tvalid_0's rmse: 0.05936\n",
            "Early stopping, best iteration is:\n",
            "[167]\tvalid_0's rmse: 0.0591639\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.230082\n",
            "[20]\tvalid_0's rmse: 0.178364\n",
            "[30]\tvalid_0's rmse: 0.139513\n",
            "[40]\tvalid_0's rmse: 0.109598\n",
            "[50]\tvalid_0's rmse: 0.0955988\n",
            "[60]\tvalid_0's rmse: 0.0925107\n",
            "[70]\tvalid_0's rmse: 0.0863601\n",
            "[80]\tvalid_0's rmse: 0.0841443\n",
            "[90]\tvalid_0's rmse: 0.0825511\n",
            "[100]\tvalid_0's rmse: 0.0781857\n",
            "[110]\tvalid_0's rmse: 0.0744301\n",
            "[120]\tvalid_0's rmse: 0.0731468\n",
            "[130]\tvalid_0's rmse: 0.0674265\n",
            "[140]\tvalid_0's rmse: 0.0664886\n",
            "[150]\tvalid_0's rmse: 0.0630246\n",
            "[160]\tvalid_0's rmse: 0.0603736\n",
            "[170]\tvalid_0's rmse: 0.0578545\n",
            "[180]\tvalid_0's rmse: 0.0565312\n",
            "[190]\tvalid_0's rmse: 0.0565312\n",
            "Early stopping, best iteration is:\n",
            "[176]\tvalid_0's rmse: 0.0565312\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.18189\n",
            "[20]\tvalid_0's rmse: 0.133574\n",
            "[30]\tvalid_0's rmse: 0.0979017\n",
            "[40]\tvalid_0's rmse: 0.0712532\n",
            "[50]\tvalid_0's rmse: 0.0507129\n",
            "[60]\tvalid_0's rmse: 0.0343927\n",
            "[70]\tvalid_0's rmse: 0.0236005\n",
            "[80]\tvalid_0's rmse: 0.0138399\n",
            "[90]\tvalid_0's rmse: 0.00887546\n",
            "[100]\tvalid_0's rmse: 0.00481816\n",
            "[110]\tvalid_0's rmse: 0.00340337\n",
            "[120]\tvalid_0's rmse: 0.00411004\n",
            "[130]\tvalid_0's rmse: 0.000794599\n",
            "[140]\tvalid_0's rmse: 9.95386e-05\n",
            "[150]\tvalid_0's rmse: 0.00218745\n",
            "[160]\tvalid_0's rmse: 0.000272484\n",
            "Early stopping, best iteration is:\n",
            "[140]\tvalid_0's rmse: 9.95386e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.245194\n",
            "[20]\tvalid_0's rmse: 0.196166\n",
            "[30]\tvalid_0's rmse: 0.161422\n",
            "[40]\tvalid_0's rmse: 0.134681\n",
            "[50]\tvalid_0's rmse: 0.113276\n",
            "[60]\tvalid_0's rmse: 0.0959355\n",
            "[70]\tvalid_0's rmse: 0.0812265\n",
            "[80]\tvalid_0's rmse: 0.0680359\n",
            "[90]\tvalid_0's rmse: 0.0630305\n",
            "[100]\tvalid_0's rmse: 0.0607924\n",
            "[110]\tvalid_0's rmse: 0.0571914\n",
            "[120]\tvalid_0's rmse: 0.0556642\n",
            "[130]\tvalid_0's rmse: 0.0539286\n",
            "[140]\tvalid_0's rmse: 0.0502518\n",
            "[150]\tvalid_0's rmse: 0.0487292\n",
            "[160]\tvalid_0's rmse: 0.0475619\n",
            "[170]\tvalid_0's rmse: 0.0459741\n",
            "[180]\tvalid_0's rmse: 0.0448454\n",
            "[190]\tvalid_0's rmse: 0.0448454\n",
            "Early stopping, best iteration is:\n",
            "[175]\tvalid_0's rmse: 0.0441291\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.258523\n",
            "[20]\tvalid_0's rmse: 0.211556\n",
            "[30]\tvalid_0's rmse: 0.177273\n",
            "[40]\tvalid_0's rmse: 0.150448\n",
            "[50]\tvalid_0's rmse: 0.128231\n",
            "[60]\tvalid_0's rmse: 0.11067\n",
            "[70]\tvalid_0's rmse: 0.095752\n",
            "[80]\tvalid_0's rmse: 0.0818558\n",
            "[90]\tvalid_0's rmse: 0.0797663\n",
            "[100]\tvalid_0's rmse: 0.0774648\n",
            "[110]\tvalid_0's rmse: 0.0753141\n",
            "[120]\tvalid_0's rmse: 0.0735491\n",
            "[130]\tvalid_0's rmse: 0.07011\n",
            "[140]\tvalid_0's rmse: 0.068668\n",
            "[150]\tvalid_0's rmse: 0.0674305\n",
            "[160]\tvalid_0's rmse: 0.064464\n",
            "[170]\tvalid_0's rmse: 0.0633097\n",
            "[180]\tvalid_0's rmse: 0.0622448\n",
            "[190]\tvalid_0's rmse: 0.0617065\n",
            "[200]\tvalid_0's rmse: 0.0617065\n",
            "Early stopping, best iteration is:\n",
            "[181]\tvalid_0's rmse: 0.0616193\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.17526\n",
            "[20]\tvalid_0's rmse: 0.12723\n",
            "[30]\tvalid_0's rmse: 0.0914791\n",
            "[40]\tvalid_0's rmse: 0.065375\n",
            "[50]\tvalid_0's rmse: 0.0457643\n",
            "[60]\tvalid_0's rmse: 0.0299114\n",
            "[70]\tvalid_0's rmse: 0.0162203\n",
            "[80]\tvalid_0's rmse: 0.00508201\n",
            "[90]\tvalid_0's rmse: 0.00217207\n",
            "[100]\tvalid_0's rmse: 0.00422138\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.000246316\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.167099\n",
            "[20]\tvalid_0's rmse: 0.121757\n",
            "[30]\tvalid_0's rmse: 0.0878114\n",
            "[40]\tvalid_0's rmse: 0.0694011\n",
            "[50]\tvalid_0's rmse: 0.0505514\n",
            "[60]\tvalid_0's rmse: 0.0365063\n",
            "[70]\tvalid_0's rmse: 0.0309274\n",
            "[80]\tvalid_0's rmse: 0.0289987\n",
            "[90]\tvalid_0's rmse: 0.0285012\n",
            "[100]\tvalid_0's rmse: 0.0254191\n",
            "[110]\tvalid_0's rmse: 0.0247976\n",
            "[120]\tvalid_0's rmse: 0.0245461\n",
            "[130]\tvalid_0's rmse: 0.0217224\n",
            "[140]\tvalid_0's rmse: 0.0196119\n",
            "[150]\tvalid_0's rmse: 0.0190907\n",
            "[160]\tvalid_0's rmse: 0.0180603\n",
            "[170]\tvalid_0's rmse: 0.0155866\n",
            "[180]\tvalid_0's rmse: 0.0146938\n",
            "[190]\tvalid_0's rmse: 0.01476\n",
            "[200]\tvalid_0's rmse: 0.01476\n",
            "Early stopping, best iteration is:\n",
            "[182]\tvalid_0's rmse: 0.0138521\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.195977\n",
            "[20]\tvalid_0's rmse: 0.150849\n",
            "[30]\tvalid_0's rmse: 0.11675\n",
            "[40]\tvalid_0's rmse: 0.0889161\n",
            "[50]\tvalid_0's rmse: 0.0697593\n",
            "[60]\tvalid_0's rmse: 0.0558909\n",
            "[70]\tvalid_0's rmse: 0.0534435\n",
            "[80]\tvalid_0's rmse: 0.0503554\n",
            "[90]\tvalid_0's rmse: 0.0535525\n",
            "Early stopping, best iteration is:\n",
            "[78]\tvalid_0's rmse: 0.0498981\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.327752\n",
            "[20]\tvalid_0's rmse: 0.285444\n",
            "[30]\tvalid_0's rmse: 0.247996\n",
            "[40]\tvalid_0's rmse: 0.220982\n",
            "[50]\tvalid_0's rmse: 0.20557\n",
            "[60]\tvalid_0's rmse: 0.193591\n",
            "[70]\tvalid_0's rmse: 0.183372\n",
            "[80]\tvalid_0's rmse: 0.174981\n",
            "[90]\tvalid_0's rmse: 0.168116\n",
            "[100]\tvalid_0's rmse: 0.167545\n",
            "[110]\tvalid_0's rmse: 0.165109\n",
            "[120]\tvalid_0's rmse: 0.164313\n",
            "[130]\tvalid_0's rmse: 0.162589\n",
            "[140]\tvalid_0's rmse: 0.16038\n",
            "[150]\tvalid_0's rmse: 0.159618\n",
            "[160]\tvalid_0's rmse: 0.159618\n",
            "Early stopping, best iteration is:\n",
            "[141]\tvalid_0's rmse: 0.159618\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.33402\n",
            "[20]\tvalid_0's rmse: 0.289693\n",
            "[30]\tvalid_0's rmse: 0.256518\n",
            "[40]\tvalid_0's rmse: 0.226242\n",
            "[50]\tvalid_0's rmse: 0.207276\n",
            "[60]\tvalid_0's rmse: 0.195206\n",
            "[70]\tvalid_0's rmse: 0.185319\n",
            "[80]\tvalid_0's rmse: 0.173823\n",
            "[90]\tvalid_0's rmse: 0.171367\n",
            "[100]\tvalid_0's rmse: 0.165786\n",
            "[110]\tvalid_0's rmse: 0.164384\n",
            "[120]\tvalid_0's rmse: 0.162976\n",
            "[130]\tvalid_0's rmse: 0.162251\n",
            "[140]\tvalid_0's rmse: 0.157998\n",
            "[150]\tvalid_0's rmse: 0.158757\n",
            "[160]\tvalid_0's rmse: 0.158757\n",
            "Early stopping, best iteration is:\n",
            "[140]\tvalid_0's rmse: 0.157998\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.208135\n",
            "[20]\tvalid_0's rmse: 0.165098\n",
            "[30]\tvalid_0's rmse: 0.127933\n",
            "[40]\tvalid_0's rmse: 0.104377\n",
            "[50]\tvalid_0's rmse: 0.0886746\n",
            "[60]\tvalid_0's rmse: 0.077514\n",
            "[70]\tvalid_0's rmse: 0.067178\n",
            "[80]\tvalid_0's rmse: 0.0566111\n",
            "[90]\tvalid_0's rmse: 0.0512625\n",
            "[100]\tvalid_0's rmse: 0.0496326\n",
            "[110]\tvalid_0's rmse: 0.0450454\n",
            "[120]\tvalid_0's rmse: 0.0419878\n",
            "[130]\tvalid_0's rmse: 0.0403572\n",
            "[140]\tvalid_0's rmse: 0.0400535\n",
            "[150]\tvalid_0's rmse: 0.0400535\n",
            "Early stopping, best iteration is:\n",
            "[132]\tvalid_0's rmse: 0.0389372\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.209294\n",
            "[20]\tvalid_0's rmse: 0.161362\n",
            "[30]\tvalid_0's rmse: 0.124687\n",
            "[40]\tvalid_0's rmse: 0.10104\n",
            "[50]\tvalid_0's rmse: 0.0850303\n",
            "[60]\tvalid_0's rmse: 0.0734829\n",
            "[70]\tvalid_0's rmse: 0.0669063\n",
            "[80]\tvalid_0's rmse: 0.0592194\n",
            "[90]\tvalid_0's rmse: 0.054467\n",
            "[100]\tvalid_0's rmse: 0.0474307\n",
            "[110]\tvalid_0's rmse: 0.0425794\n",
            "[120]\tvalid_0's rmse: 0.0389097\n",
            "[130]\tvalid_0's rmse: 0.0374577\n",
            "[140]\tvalid_0's rmse: 0.030782\n",
            "[150]\tvalid_0's rmse: 0.029906\n",
            "[160]\tvalid_0's rmse: 0.029906\n",
            "Early stopping, best iteration is:\n",
            "[142]\tvalid_0's rmse: 0.029906\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.225739\n",
            "[20]\tvalid_0's rmse: 0.175194\n",
            "[30]\tvalid_0's rmse: 0.139207\n",
            "[40]\tvalid_0's rmse: 0.109516\n",
            "[50]\tvalid_0's rmse: 0.0884628\n",
            "[60]\tvalid_0's rmse: 0.0737978\n",
            "[70]\tvalid_0's rmse: 0.0619416\n",
            "[80]\tvalid_0's rmse: 0.0532102\n",
            "[90]\tvalid_0's rmse: 0.0456732\n",
            "[100]\tvalid_0's rmse: 0.0384726\n",
            "[110]\tvalid_0's rmse: 0.0314205\n",
            "[120]\tvalid_0's rmse: 0.029503\n",
            "[130]\tvalid_0's rmse: 0.0244858\n",
            "[140]\tvalid_0's rmse: 0.0239454\n",
            "[150]\tvalid_0's rmse: 0.0239454\n",
            "Early stopping, best iteration is:\n",
            "[131]\tvalid_0's rmse: 0.0237338\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.250214\n",
            "[20]\tvalid_0's rmse: 0.197154\n",
            "[30]\tvalid_0's rmse: 0.156339\n",
            "[40]\tvalid_0's rmse: 0.126086\n",
            "[50]\tvalid_0's rmse: 0.104199\n",
            "[60]\tvalid_0's rmse: 0.0916572\n",
            "[70]\tvalid_0's rmse: 0.0830578\n",
            "[80]\tvalid_0's rmse: 0.0760037\n",
            "[90]\tvalid_0's rmse: 0.0677326\n",
            "[100]\tvalid_0's rmse: 0.0605636\n",
            "[110]\tvalid_0's rmse: 0.0539038\n",
            "[120]\tvalid_0's rmse: 0.047382\n",
            "[130]\tvalid_0's rmse: 0.0432249\n",
            "[140]\tvalid_0's rmse: 0.0416482\n",
            "[150]\tvalid_0's rmse: 0.0416482\n",
            "Early stopping, best iteration is:\n",
            "[133]\tvalid_0's rmse: 0.0414172\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0925952\n",
            "[20]\tvalid_0's rmse: 0.0414438\n",
            "[30]\tvalid_0's rmse: 0.00488395\n",
            "[40]\tvalid_0's rmse: 0.016413\n",
            "[50]\tvalid_0's rmse: 0.0308892\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's rmse: 0.00153999\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0858186\n",
            "[20]\tvalid_0's rmse: 0.0422722\n",
            "[30]\tvalid_0's rmse: 0.0117482\n",
            "[40]\tvalid_0's rmse: 0.00728159\n",
            "[50]\tvalid_0's rmse: 0.0201068\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's rmse: 0.000105777\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.124151\n",
            "[20]\tvalid_0's rmse: 0.0742574\n",
            "[30]\tvalid_0's rmse: 0.0497964\n",
            "[40]\tvalid_0's rmse: 0.0313466\n",
            "[50]\tvalid_0's rmse: 0.0183552\n",
            "[60]\tvalid_0's rmse: 0.0115046\n",
            "[70]\tvalid_0's rmse: 0.0115622\n",
            "[80]\tvalid_0's rmse: 0.0130629\n",
            "Early stopping, best iteration is:\n",
            "[65]\tvalid_0's rmse: 0.0103338\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.131453\n",
            "[20]\tvalid_0's rmse: 0.0841012\n",
            "[30]\tvalid_0's rmse: 0.0562249\n",
            "[40]\tvalid_0's rmse: 0.0326847\n",
            "[50]\tvalid_0's rmse: 0.023652\n",
            "[60]\tvalid_0's rmse: 0.0204157\n",
            "[70]\tvalid_0's rmse: 0.0215793\n",
            "Early stopping, best iteration is:\n",
            "[55]\tvalid_0's rmse: 0.0145678\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.138008\n",
            "[20]\tvalid_0's rmse: 0.0877501\n",
            "[30]\tvalid_0's rmse: 0.0523011\n",
            "[40]\tvalid_0's rmse: 0.0332908\n",
            "[50]\tvalid_0's rmse: 0.0213392\n",
            "[60]\tvalid_0's rmse: 0.0165198\n",
            "[70]\tvalid_0's rmse: 0.0118001\n",
            "[80]\tvalid_0's rmse: 0.0100147\n",
            "[90]\tvalid_0's rmse: 0.0117403\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 0.00985775\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0360763\n",
            "[20]\tvalid_0's rmse: 0.0052312\n",
            "[30]\tvalid_0's rmse: 0.0246102\n",
            "Early stopping, best iteration is:\n",
            "[19]\tvalid_0's rmse: 0.000846107\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0605901\n",
            "[20]\tvalid_0's rmse: 0.0227361\n",
            "[30]\tvalid_0's rmse: 0.00313601\n",
            "[40]\tvalid_0's rmse: 0.0137702\n",
            "[50]\tvalid_0's rmse: 0.0207396\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's rmse: 8.88279e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0284392\n",
            "[20]\tvalid_0's rmse: 0.039742\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.0131274\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00789437\n",
            "[20]\tvalid_0's rmse: 0.0165157\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.000870445\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.334069\n",
            "[20]\tvalid_0's rmse: 0.232659\n",
            "[30]\tvalid_0's rmse: 0.175252\n",
            "[40]\tvalid_0's rmse: 0.124121\n",
            "[50]\tvalid_0's rmse: 0.0933332\n",
            "[60]\tvalid_0's rmse: 0.0878361\n",
            "[70]\tvalid_0's rmse: 0.0878361\n",
            "Early stopping, best iteration is:\n",
            "[53]\tvalid_0's rmse: 0.0878361\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.339953\n",
            "[20]\tvalid_0's rmse: 0.234151\n",
            "[30]\tvalid_0's rmse: 0.16673\n",
            "[40]\tvalid_0's rmse: 0.12545\n",
            "[50]\tvalid_0's rmse: 0.101833\n",
            "[60]\tvalid_0's rmse: 0.0888252\n",
            "[70]\tvalid_0's rmse: 0.0888252\n",
            "Early stopping, best iteration is:\n",
            "[56]\tvalid_0's rmse: 0.0888252\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.297853\n",
            "[20]\tvalid_0's rmse: 0.204533\n",
            "[30]\tvalid_0's rmse: 0.143476\n",
            "[40]\tvalid_0's rmse: 0.107763\n",
            "[50]\tvalid_0's rmse: 0.0837158\n",
            "[60]\tvalid_0's rmse: 0.0713203\n",
            "[70]\tvalid_0's rmse: 0.0713203\n",
            "Early stopping, best iteration is:\n",
            "[59]\tvalid_0's rmse: 0.0713203\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.270764\n",
            "[20]\tvalid_0's rmse: 0.187974\n",
            "[30]\tvalid_0's rmse: 0.134565\n",
            "[40]\tvalid_0's rmse: 0.101354\n",
            "[50]\tvalid_0's rmse: 0.0810609\n",
            "[60]\tvalid_0's rmse: 0.0684308\n",
            "[70]\tvalid_0's rmse: 0.0652849\n",
            "[80]\tvalid_0's rmse: 0.0652849\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0652849\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.294943\n",
            "[20]\tvalid_0's rmse: 0.211444\n",
            "[30]\tvalid_0's rmse: 0.164309\n",
            "[40]\tvalid_0's rmse: 0.134925\n",
            "[50]\tvalid_0's rmse: 0.112935\n",
            "[60]\tvalid_0's rmse: 0.0994099\n",
            "[70]\tvalid_0's rmse: 0.0912381\n",
            "[80]\tvalid_0's rmse: 0.0912381\n",
            "Early stopping, best iteration is:\n",
            "[64]\tvalid_0's rmse: 0.0912381\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.296553\n",
            "[20]\tvalid_0's rmse: 0.208938\n",
            "[30]\tvalid_0's rmse: 0.160251\n",
            "[40]\tvalid_0's rmse: 0.127258\n",
            "[50]\tvalid_0's rmse: 0.109384\n",
            "[60]\tvalid_0's rmse: 0.0966215\n",
            "[70]\tvalid_0's rmse: 0.0930283\n",
            "[80]\tvalid_0's rmse: 0.0930283\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0930283\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.329771\n",
            "[20]\tvalid_0's rmse: 0.228261\n",
            "[30]\tvalid_0's rmse: 0.15966\n",
            "[40]\tvalid_0's rmse: 0.128857\n",
            "[50]\tvalid_0's rmse: 0.112411\n",
            "[60]\tvalid_0's rmse: 0.0966135\n",
            "[70]\tvalid_0's rmse: 0.0940607\n",
            "[80]\tvalid_0's rmse: 0.0940607\n",
            "Early stopping, best iteration is:\n",
            "[62]\tvalid_0's rmse: 0.0940607\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.237889\n",
            "[20]\tvalid_0's rmse: 0.150231\n",
            "[30]\tvalid_0's rmse: 0.0932165\n",
            "[40]\tvalid_0's rmse: 0.0642158\n",
            "[50]\tvalid_0's rmse: 0.0466544\n",
            "[60]\tvalid_0's rmse: 0.0349405\n",
            "[70]\tvalid_0's rmse: 0.030297\n",
            "[80]\tvalid_0's rmse: 0.030297\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.030297\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.330825\n",
            "[20]\tvalid_0's rmse: 0.237994\n",
            "[30]\tvalid_0's rmse: 0.174328\n",
            "[40]\tvalid_0's rmse: 0.131248\n",
            "[50]\tvalid_0's rmse: 0.112792\n",
            "[60]\tvalid_0's rmse: 0.0983632\n",
            "[70]\tvalid_0's rmse: 0.0955744\n",
            "[80]\tvalid_0's rmse: 0.0955744\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0955744\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.345646\n",
            "[20]\tvalid_0's rmse: 0.244827\n",
            "[30]\tvalid_0's rmse: 0.181866\n",
            "[40]\tvalid_0's rmse: 0.139174\n",
            "[50]\tvalid_0's rmse: 0.115293\n",
            "[60]\tvalid_0's rmse: 0.102599\n",
            "[70]\tvalid_0's rmse: 0.0953866\n",
            "[80]\tvalid_0's rmse: 0.0953866\n",
            "Early stopping, best iteration is:\n",
            "[64]\tvalid_0's rmse: 0.0953866\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.238204\n",
            "[20]\tvalid_0's rmse: 0.153112\n",
            "[30]\tvalid_0's rmse: 0.0952046\n",
            "[40]\tvalid_0's rmse: 0.0658695\n",
            "[50]\tvalid_0's rmse: 0.0422464\n",
            "[60]\tvalid_0's rmse: 0.0286621\n",
            "[70]\tvalid_0's rmse: 0.0253121\n",
            "[80]\tvalid_0's rmse: 0.0253121\n",
            "Early stopping, best iteration is:\n",
            "[65]\tvalid_0's rmse: 0.0253121\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.321538\n",
            "[20]\tvalid_0's rmse: 0.226285\n",
            "[30]\tvalid_0's rmse: 0.164726\n",
            "[40]\tvalid_0's rmse: 0.130298\n",
            "[50]\tvalid_0's rmse: 0.103911\n",
            "[60]\tvalid_0's rmse: 0.0900049\n",
            "[70]\tvalid_0's rmse: 0.0820446\n",
            "[80]\tvalid_0's rmse: 0.0820446\n",
            "Early stopping, best iteration is:\n",
            "[67]\tvalid_0's rmse: 0.0820446\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.320645\n",
            "[20]\tvalid_0's rmse: 0.232098\n",
            "[30]\tvalid_0's rmse: 0.176\n",
            "[40]\tvalid_0's rmse: 0.134514\n",
            "[50]\tvalid_0's rmse: 0.115037\n",
            "[60]\tvalid_0's rmse: 0.099972\n",
            "[70]\tvalid_0's rmse: 0.0886509\n",
            "[80]\tvalid_0's rmse: 0.0886509\n",
            "[90]\tvalid_0's rmse: 0.0886509\n",
            "Early stopping, best iteration is:\n",
            "[70]\tvalid_0's rmse: 0.0886509\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.307045\n",
            "[20]\tvalid_0's rmse: 0.217361\n",
            "[30]\tvalid_0's rmse: 0.160215\n",
            "[40]\tvalid_0's rmse: 0.119756\n",
            "[50]\tvalid_0's rmse: 0.0977549\n",
            "[60]\tvalid_0's rmse: 0.0817179\n",
            "[70]\tvalid_0's rmse: 0.0721701\n",
            "[80]\tvalid_0's rmse: 0.0721701\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's rmse: 0.0721701\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.238857\n",
            "[20]\tvalid_0's rmse: 0.156124\n",
            "[30]\tvalid_0's rmse: 0.10059\n",
            "[40]\tvalid_0's rmse: 0.0665143\n",
            "[50]\tvalid_0's rmse: 0.0454807\n",
            "[60]\tvalid_0's rmse: 0.0318518\n",
            "[70]\tvalid_0's rmse: 0.0194756\n",
            "[80]\tvalid_0's rmse: 0.0189997\n",
            "[90]\tvalid_0's rmse: 0.0189997\n",
            "Early stopping, best iteration is:\n",
            "[71]\tvalid_0's rmse: 0.0189997\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.269858\n",
            "[20]\tvalid_0's rmse: 0.18761\n",
            "[30]\tvalid_0's rmse: 0.13349\n",
            "[40]\tvalid_0's rmse: 0.0994062\n",
            "[50]\tvalid_0's rmse: 0.0761078\n",
            "[60]\tvalid_0's rmse: 0.0615955\n",
            "[70]\tvalid_0's rmse: 0.0525122\n",
            "[80]\tvalid_0's rmse: 0.0502539\n",
            "[90]\tvalid_0's rmse: 0.0502539\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0502539\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.293265\n",
            "[20]\tvalid_0's rmse: 0.212193\n",
            "[30]\tvalid_0's rmse: 0.161696\n",
            "[40]\tvalid_0's rmse: 0.127434\n",
            "[50]\tvalid_0's rmse: 0.10549\n",
            "[60]\tvalid_0's rmse: 0.0923172\n",
            "[70]\tvalid_0's rmse: 0.0838068\n",
            "[80]\tvalid_0's rmse: 0.0826704\n",
            "[90]\tvalid_0's rmse: 0.0826704\n",
            "Early stopping, best iteration is:\n",
            "[71]\tvalid_0's rmse: 0.0826704\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.146343\n",
            "[20]\tvalid_0's rmse: 0.098787\n",
            "[30]\tvalid_0's rmse: 0.0595038\n",
            "[40]\tvalid_0's rmse: 0.0387245\n",
            "[50]\tvalid_0's rmse: 0.0214287\n",
            "[60]\tvalid_0's rmse: 0.0094628\n",
            "[70]\tvalid_0's rmse: 0.0078026\n",
            "[80]\tvalid_0's rmse: 0.00745735\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's rmse: 0.00725864\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.139735\n",
            "[20]\tvalid_0's rmse: 0.0901432\n",
            "[30]\tvalid_0's rmse: 0.0568289\n",
            "[40]\tvalid_0's rmse: 0.0372911\n",
            "[50]\tvalid_0's rmse: 0.0209503\n",
            "[60]\tvalid_0's rmse: 0.00977161\n",
            "[70]\tvalid_0's rmse: 0.00506879\n",
            "[80]\tvalid_0's rmse: 0.00505764\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's rmse: 0.00448777\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.203505\n",
            "[20]\tvalid_0's rmse: 0.134127\n",
            "[30]\tvalid_0's rmse: 0.0910926\n",
            "[40]\tvalid_0's rmse: 0.0618603\n",
            "[50]\tvalid_0's rmse: 0.0420142\n",
            "[60]\tvalid_0's rmse: 0.0301311\n",
            "[70]\tvalid_0's rmse: 0.0219339\n",
            "[80]\tvalid_0's rmse: 0.0205705\n",
            "[90]\tvalid_0's rmse: 0.0205705\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0205705\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.197465\n",
            "[20]\tvalid_0's rmse: 0.13233\n",
            "[30]\tvalid_0's rmse: 0.090302\n",
            "[40]\tvalid_0's rmse: 0.0608235\n",
            "[50]\tvalid_0's rmse: 0.0434096\n",
            "[60]\tvalid_0's rmse: 0.0330668\n",
            "[70]\tvalid_0's rmse: 0.0245144\n",
            "[80]\tvalid_0's rmse: 0.0231872\n",
            "[90]\tvalid_0's rmse: 0.0231872\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0231872\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.163555\n",
            "[20]\tvalid_0's rmse: 0.107332\n",
            "[30]\tvalid_0's rmse: 0.0691263\n",
            "[40]\tvalid_0's rmse: 0.0439938\n",
            "[50]\tvalid_0's rmse: 0.0293771\n",
            "[60]\tvalid_0's rmse: 0.0188738\n",
            "[70]\tvalid_0's rmse: 0.0113477\n",
            "[80]\tvalid_0's rmse: 0.0103855\n",
            "[90]\tvalid_0's rmse: 0.0103855\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.0098597\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.215613\n",
            "[20]\tvalid_0's rmse: 0.153799\n",
            "[30]\tvalid_0's rmse: 0.114726\n",
            "[40]\tvalid_0's rmse: 0.0878815\n",
            "[50]\tvalid_0's rmse: 0.0698794\n",
            "[60]\tvalid_0's rmse: 0.0588803\n",
            "[70]\tvalid_0's rmse: 0.0511879\n",
            "[80]\tvalid_0's rmse: 0.0486417\n",
            "[90]\tvalid_0's rmse: 0.0486417\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.0486417\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.230172\n",
            "[20]\tvalid_0's rmse: 0.167651\n",
            "[30]\tvalid_0's rmse: 0.127974\n",
            "[40]\tvalid_0's rmse: 0.101817\n",
            "[50]\tvalid_0's rmse: 0.0844152\n",
            "[60]\tvalid_0's rmse: 0.0749746\n",
            "[70]\tvalid_0's rmse: 0.0673806\n",
            "[80]\tvalid_0's rmse: 0.0630931\n",
            "[90]\tvalid_0's rmse: 0.0630931\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0630931\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.155593\n",
            "[20]\tvalid_0's rmse: 0.115536\n",
            "[30]\tvalid_0's rmse: 0.0920016\n",
            "[40]\tvalid_0's rmse: 0.0738034\n",
            "[50]\tvalid_0's rmse: 0.0587802\n",
            "[60]\tvalid_0's rmse: 0.046569\n",
            "[70]\tvalid_0's rmse: 0.0392491\n",
            "[80]\tvalid_0's rmse: 0.0341556\n",
            "[90]\tvalid_0's rmse: 0.0341556\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 0.0341556\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.153173\n",
            "[20]\tvalid_0's rmse: 0.121674\n",
            "[30]\tvalid_0's rmse: 0.0849086\n",
            "[40]\tvalid_0's rmse: 0.0612132\n",
            "[50]\tvalid_0's rmse: 0.0470642\n",
            "[60]\tvalid_0's rmse: 0.0377201\n",
            "[70]\tvalid_0's rmse: 0.0310771\n",
            "[80]\tvalid_0's rmse: 0.0281345\n",
            "[90]\tvalid_0's rmse: 0.0281345\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0279127\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.17349\n",
            "[20]\tvalid_0's rmse: 0.119024\n",
            "[30]\tvalid_0's rmse: 0.0849315\n",
            "[40]\tvalid_0's rmse: 0.0613372\n",
            "[50]\tvalid_0's rmse: 0.0456224\n",
            "[60]\tvalid_0's rmse: 0.0361695\n",
            "[70]\tvalid_0's rmse: 0.0294558\n",
            "[80]\tvalid_0's rmse: 0.0260645\n",
            "[90]\tvalid_0's rmse: 0.0260645\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 0.0260645\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.264046\n",
            "[20]\tvalid_0's rmse: 0.187044\n",
            "[30]\tvalid_0's rmse: 0.149719\n",
            "[40]\tvalid_0's rmse: 0.12701\n",
            "[50]\tvalid_0's rmse: 0.115016\n",
            "[60]\tvalid_0's rmse: 0.106918\n",
            "[70]\tvalid_0's rmse: 0.0996743\n",
            "[80]\tvalid_0's rmse: 0.0956471\n",
            "[90]\tvalid_0's rmse: 0.0956471\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0956471\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.274687\n",
            "[20]\tvalid_0's rmse: 0.192915\n",
            "[30]\tvalid_0's rmse: 0.140235\n",
            "[40]\tvalid_0's rmse: 0.111286\n",
            "[50]\tvalid_0's rmse: 0.0976019\n",
            "[60]\tvalid_0's rmse: 0.0893744\n",
            "[70]\tvalid_0's rmse: 0.0837852\n",
            "[80]\tvalid_0's rmse: 0.0805388\n",
            "[90]\tvalid_0's rmse: 0.0805388\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.0805388\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.162736\n",
            "[20]\tvalid_0's rmse: 0.103023\n",
            "[30]\tvalid_0's rmse: 0.0569715\n",
            "[40]\tvalid_0's rmse: 0.0346175\n",
            "[50]\tvalid_0's rmse: 0.0209569\n",
            "[60]\tvalid_0's rmse: 0.0133532\n",
            "[70]\tvalid_0's rmse: 0.00725545\n",
            "[80]\tvalid_0's rmse: 0.00535165\n",
            "[90]\tvalid_0's rmse: 0.00535165\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.00535165\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.172385\n",
            "[20]\tvalid_0's rmse: 0.118273\n",
            "[30]\tvalid_0's rmse: 0.0780338\n",
            "[40]\tvalid_0's rmse: 0.0506217\n",
            "[50]\tvalid_0's rmse: 0.0368252\n",
            "[60]\tvalid_0's rmse: 0.0291441\n",
            "[70]\tvalid_0's rmse: 0.02327\n",
            "[80]\tvalid_0's rmse: 0.0208825\n",
            "[90]\tvalid_0's rmse: 0.0174828\n",
            "[100]\tvalid_0's rmse: 0.0174828\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0174828\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.19809\n",
            "[20]\tvalid_0's rmse: 0.12888\n",
            "[30]\tvalid_0's rmse: 0.0866504\n",
            "[40]\tvalid_0's rmse: 0.0564312\n",
            "[50]\tvalid_0's rmse: 0.0408337\n",
            "[60]\tvalid_0's rmse: 0.0331192\n",
            "[70]\tvalid_0's rmse: 0.0278368\n",
            "[80]\tvalid_0's rmse: 0.0237148\n",
            "[90]\tvalid_0's rmse: 0.0237148\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0237148\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.218239\n",
            "[20]\tvalid_0's rmse: 0.151551\n",
            "[30]\tvalid_0's rmse: 0.102836\n",
            "[40]\tvalid_0's rmse: 0.0757454\n",
            "[50]\tvalid_0's rmse: 0.0578735\n",
            "[60]\tvalid_0's rmse: 0.0505793\n",
            "[70]\tvalid_0's rmse: 0.0446218\n",
            "[80]\tvalid_0's rmse: 0.0410873\n",
            "[90]\tvalid_0's rmse: 0.0410873\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 0.0410873\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.105176\n",
            "[20]\tvalid_0's rmse: 0.0587755\n",
            "[30]\tvalid_0's rmse: 0.029894\n",
            "[40]\tvalid_0's rmse: 0.0119928\n",
            "[50]\tvalid_0's rmse: 0.00142085\n",
            "[60]\tvalid_0's rmse: 0.00616669\n",
            "[70]\tvalid_0's rmse: 0.00969959\n",
            "Early stopping, best iteration is:\n",
            "[52]\tvalid_0's rmse: 0.000139077\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0986446\n",
            "[20]\tvalid_0's rmse: 0.0527294\n",
            "[30]\tvalid_0's rmse: 0.0219282\n",
            "[40]\tvalid_0's rmse: 0.00493865\n",
            "[50]\tvalid_0's rmse: 0.00510262\n",
            "[60]\tvalid_0's rmse: 0.00982645\n",
            "Early stopping, best iteration is:\n",
            "[43]\tvalid_0's rmse: 0.000838401\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.117092\n",
            "[20]\tvalid_0's rmse: 0.0633661\n",
            "[30]\tvalid_0's rmse: 0.0319805\n",
            "[40]\tvalid_0's rmse: 0.0119852\n",
            "[50]\tvalid_0's rmse: 0.000757022\n",
            "[60]\tvalid_0's rmse: 0.00716779\n",
            "Early stopping, best iteration is:\n",
            "[49]\tvalid_0's rmse: 0.000118067\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.134347\n",
            "[20]\tvalid_0's rmse: 0.0897145\n",
            "[30]\tvalid_0's rmse: 0.0600863\n",
            "[40]\tvalid_0's rmse: 0.0421221\n",
            "[50]\tvalid_0's rmse: 0.0306423\n",
            "[60]\tvalid_0's rmse: 0.0257675\n",
            "[70]\tvalid_0's rmse: 0.0210714\n",
            "[80]\tvalid_0's rmse: 0.0130041\n",
            "[90]\tvalid_0's rmse: 0.0124013\n",
            "[100]\tvalid_0's rmse: 0.0128806\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0124013\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.136263\n",
            "[20]\tvalid_0's rmse: 0.0993366\n",
            "[30]\tvalid_0's rmse: 0.069653\n",
            "[40]\tvalid_0's rmse: 0.0516112\n",
            "[50]\tvalid_0's rmse: 0.0385041\n",
            "[60]\tvalid_0's rmse: 0.0307761\n",
            "[70]\tvalid_0's rmse: 0.025211\n",
            "[80]\tvalid_0's rmse: 0.0219304\n",
            "[90]\tvalid_0's rmse: 0.0219304\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 0.0219304\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0563257\n",
            "[20]\tvalid_0's rmse: 0.0212588\n",
            "[30]\tvalid_0's rmse: 0.000851947\n",
            "[40]\tvalid_0's rmse: 0.0129834\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's rmse: 0.000344035\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.054672\n",
            "[20]\tvalid_0's rmse: 0.0247526\n",
            "[30]\tvalid_0's rmse: 0.00854496\n",
            "[40]\tvalid_0's rmse: 0.0068651\n",
            "[50]\tvalid_0's rmse: 0.00103798\n",
            "[60]\tvalid_0's rmse: 0.00616403\n",
            "Early stopping, best iteration is:\n",
            "[48]\tvalid_0's rmse: 0.000267359\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0190044\n",
            "[20]\tvalid_0's rmse: 0.0250277\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's rmse: 0.0110634\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.000780291\n",
            "[20]\tvalid_0's rmse: 0.000272778\n",
            "[30]\tvalid_0's rmse: 0.00280658\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's rmse: 0.000134779\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.502485\n",
            "[20]\tvalid_0's rmse: 0.502485\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.502485\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.511336\n",
            "[20]\tvalid_0's rmse: 0.511336\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.511336\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444358\n",
            "[20]\tvalid_0's rmse: 0.444358\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.444358\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.398313\n",
            "[20]\tvalid_0's rmse: 0.398313\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.398313\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416187\n",
            "[20]\tvalid_0's rmse: 0.416187\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.416187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.464758\n",
            "[20]\tvalid_0's rmse: 0.477105\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.427877\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.438107\n",
            "[20]\tvalid_0's rmse: 0.392346\n",
            "[30]\tvalid_0's rmse: 0.354265\n",
            "[40]\tvalid_0's rmse: 0.322574\n",
            "[50]\tvalid_0's rmse: 0.296202\n",
            "[60]\tvalid_0's rmse: 0.274255\n",
            "[70]\tvalid_0's rmse: 0.255991\n",
            "[80]\tvalid_0's rmse: 0.248904\n",
            "[90]\tvalid_0's rmse: 0.24506\n",
            "[100]\tvalid_0's rmse: 0.241644\n",
            "[110]\tvalid_0's rmse: 0.241644\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.240515\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.302487\n",
            "[20]\tvalid_0's rmse: 0.250073\n",
            "[30]\tvalid_0's rmse: 0.207409\n",
            "[40]\tvalid_0's rmse: 0.173756\n",
            "[50]\tvalid_0's rmse: 0.144821\n",
            "[60]\tvalid_0's rmse: 0.119436\n",
            "[70]\tvalid_0's rmse: 0.0969982\n",
            "[80]\tvalid_0's rmse: 0.0778227\n",
            "[90]\tvalid_0's rmse: 0.0619768\n",
            "[100]\tvalid_0's rmse: 0.047523\n",
            "[110]\tvalid_0's rmse: 0.0401345\n",
            "[120]\tvalid_0's rmse: 0.0401345\n",
            "Early stopping, best iteration is:\n",
            "[106]\tvalid_0's rmse: 0.0401345\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416798\n",
            "[20]\tvalid_0's rmse: 0.364648\n",
            "[30]\tvalid_0's rmse: 0.323936\n",
            "[40]\tvalid_0's rmse: 0.288855\n",
            "[50]\tvalid_0's rmse: 0.259718\n",
            "[60]\tvalid_0's rmse: 0.235451\n",
            "[70]\tvalid_0's rmse: 0.215032\n",
            "[80]\tvalid_0's rmse: 0.197566\n",
            "[90]\tvalid_0's rmse: 0.181905\n",
            "[100]\tvalid_0's rmse: 0.168516\n",
            "[110]\tvalid_0's rmse: 0.15668\n",
            "[120]\tvalid_0's rmse: 0.15668\n",
            "[130]\tvalid_0's rmse: 0.15668\n",
            "Early stopping, best iteration is:\n",
            "[110]\tvalid_0's rmse: 0.15668\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.427433\n",
            "[20]\tvalid_0's rmse: 0.369969\n",
            "[30]\tvalid_0's rmse: 0.327462\n",
            "[40]\tvalid_0's rmse: 0.294706\n",
            "[50]\tvalid_0's rmse: 0.266646\n",
            "[60]\tvalid_0's rmse: 0.241886\n",
            "[70]\tvalid_0's rmse: 0.22078\n",
            "[80]\tvalid_0's rmse: 0.202706\n",
            "[90]\tvalid_0's rmse: 0.186876\n",
            "[100]\tvalid_0's rmse: 0.172838\n",
            "[110]\tvalid_0's rmse: 0.160415\n",
            "[120]\tvalid_0's rmse: 0.152557\n",
            "[130]\tvalid_0's rmse: 0.152557\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 0.152557\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.259657\n",
            "[20]\tvalid_0's rmse: 0.185305\n",
            "[30]\tvalid_0's rmse: 0.134501\n",
            "[40]\tvalid_0's rmse: 0.0955261\n",
            "[50]\tvalid_0's rmse: 0.0647896\n",
            "[60]\tvalid_0's rmse: 0.0498165\n",
            "[70]\tvalid_0's rmse: 0.0362266\n",
            "[80]\tvalid_0's rmse: 0.0237289\n",
            "[90]\tvalid_0's rmse: 0.0159634\n",
            "[100]\tvalid_0's rmse: 0.0159634\n",
            "Early stopping, best iteration is:\n",
            "[88]\tvalid_0's rmse: 0.0159634\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.353531\n",
            "[20]\tvalid_0's rmse: 0.270986\n",
            "[30]\tvalid_0's rmse: 0.214072\n",
            "[40]\tvalid_0's rmse: 0.174068\n",
            "[50]\tvalid_0's rmse: 0.152572\n",
            "[60]\tvalid_0's rmse: 0.138069\n",
            "[70]\tvalid_0's rmse: 0.126131\n",
            "[80]\tvalid_0's rmse: 0.114912\n",
            "[90]\tvalid_0's rmse: 0.105285\n",
            "[100]\tvalid_0's rmse: 0.101176\n",
            "[110]\tvalid_0's rmse: 0.101176\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 0.101176\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.359726\n",
            "[20]\tvalid_0's rmse: 0.291698\n",
            "[30]\tvalid_0's rmse: 0.243243\n",
            "[40]\tvalid_0's rmse: 0.20728\n",
            "[50]\tvalid_0's rmse: 0.180002\n",
            "[60]\tvalid_0's rmse: 0.158693\n",
            "[70]\tvalid_0's rmse: 0.140523\n",
            "[80]\tvalid_0's rmse: 0.127474\n",
            "[90]\tvalid_0's rmse: 0.117794\n",
            "[100]\tvalid_0's rmse: 0.116626\n",
            "[110]\tvalid_0's rmse: 0.116626\n",
            "Early stopping, best iteration is:\n",
            "[91]\tvalid_0's rmse: 0.116626\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.339618\n",
            "[20]\tvalid_0's rmse: 0.259124\n",
            "[30]\tvalid_0's rmse: 0.197608\n",
            "[40]\tvalid_0's rmse: 0.151496\n",
            "[50]\tvalid_0's rmse: 0.119516\n",
            "[60]\tvalid_0's rmse: 0.103653\n",
            "[70]\tvalid_0's rmse: 0.0930349\n",
            "[80]\tvalid_0's rmse: 0.0830732\n",
            "[90]\tvalid_0's rmse: 0.0803496\n",
            "[100]\tvalid_0's rmse: 0.0803496\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0803496\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.261018\n",
            "[20]\tvalid_0's rmse: 0.178254\n",
            "[30]\tvalid_0's rmse: 0.119306\n",
            "[40]\tvalid_0's rmse: 0.076598\n",
            "[50]\tvalid_0's rmse: 0.0493305\n",
            "[60]\tvalid_0's rmse: 0.0377174\n",
            "[70]\tvalid_0's rmse: 0.028378\n",
            "[80]\tvalid_0's rmse: 0.0205622\n",
            "[90]\tvalid_0's rmse: 0.0161393\n",
            "[100]\tvalid_0's rmse: 0.0161393\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.0161393\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.266432\n",
            "[20]\tvalid_0's rmse: 0.19935\n",
            "[30]\tvalid_0's rmse: 0.148573\n",
            "[40]\tvalid_0's rmse: 0.11082\n",
            "[50]\tvalid_0's rmse: 0.0857278\n",
            "[60]\tvalid_0's rmse: 0.0732445\n",
            "[70]\tvalid_0's rmse: 0.0642967\n",
            "[80]\tvalid_0's rmse: 0.056624\n",
            "[90]\tvalid_0's rmse: 0.0547258\n",
            "[100]\tvalid_0's rmse: 0.0547258\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0547258\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.288889\n",
            "[20]\tvalid_0's rmse: 0.209573\n",
            "[30]\tvalid_0's rmse: 0.163655\n",
            "[40]\tvalid_0's rmse: 0.127764\n",
            "[50]\tvalid_0's rmse: 0.101821\n",
            "[60]\tvalid_0's rmse: 0.0909857\n",
            "[70]\tvalid_0's rmse: 0.0840961\n",
            "[80]\tvalid_0's rmse: 0.0783161\n",
            "[90]\tvalid_0's rmse: 0.0778203\n",
            "[100]\tvalid_0's rmse: 0.0778203\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0778203\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.180102\n",
            "[20]\tvalid_0's rmse: 0.114933\n",
            "[30]\tvalid_0's rmse: 0.0720519\n",
            "[40]\tvalid_0's rmse: 0.0423371\n",
            "[50]\tvalid_0's rmse: 0.025609\n",
            "[60]\tvalid_0's rmse: 0.0156576\n",
            "[70]\tvalid_0's rmse: 0.00718753\n",
            "[80]\tvalid_0's rmse: 0.0074459\n",
            "[90]\tvalid_0's rmse: 0.00645478\n",
            "[100]\tvalid_0's rmse: 0.00645478\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.00506014\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.167586\n",
            "[20]\tvalid_0's rmse: 0.112998\n",
            "[30]\tvalid_0's rmse: 0.0735293\n",
            "[40]\tvalid_0's rmse: 0.0462938\n",
            "[50]\tvalid_0's rmse: 0.0318671\n",
            "[60]\tvalid_0's rmse: 0.0284114\n",
            "[70]\tvalid_0's rmse: 0.0257787\n",
            "[80]\tvalid_0's rmse: 0.0240733\n",
            "[90]\tvalid_0's rmse: 0.0241302\n",
            "[100]\tvalid_0's rmse: 0.0241302\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.022855\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.207506\n",
            "[20]\tvalid_0's rmse: 0.13668\n",
            "[30]\tvalid_0's rmse: 0.0913249\n",
            "[40]\tvalid_0's rmse: 0.0642151\n",
            "[50]\tvalid_0's rmse: 0.0425226\n",
            "[60]\tvalid_0's rmse: 0.0331657\n",
            "[70]\tvalid_0's rmse: 0.0271211\n",
            "[80]\tvalid_0's rmse: 0.0215517\n",
            "[90]\tvalid_0's rmse: 0.0207013\n",
            "[100]\tvalid_0's rmse: 0.0207013\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.0207013\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.202836\n",
            "[20]\tvalid_0's rmse: 0.135072\n",
            "[30]\tvalid_0's rmse: 0.0933529\n",
            "[40]\tvalid_0's rmse: 0.0656799\n",
            "[50]\tvalid_0's rmse: 0.0456785\n",
            "[60]\tvalid_0's rmse: 0.0370927\n",
            "[70]\tvalid_0's rmse: 0.0303619\n",
            "[80]\tvalid_0's rmse: 0.0248881\n",
            "[90]\tvalid_0's rmse: 0.022289\n",
            "[100]\tvalid_0's rmse: 0.022289\n",
            "Early stopping, best iteration is:\n",
            "[85]\tvalid_0's rmse: 0.022289\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.161252\n",
            "[20]\tvalid_0's rmse: 0.10022\n",
            "[30]\tvalid_0's rmse: 0.0642583\n",
            "[40]\tvalid_0's rmse: 0.0394792\n",
            "[50]\tvalid_0's rmse: 0.0279328\n",
            "[60]\tvalid_0's rmse: 0.018574\n",
            "[70]\tvalid_0's rmse: 0.0103822\n",
            "[80]\tvalid_0's rmse: 0.00430672\n",
            "[90]\tvalid_0's rmse: 0.000847503\n",
            "[100]\tvalid_0's rmse: 0.000847503\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.000847503\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.217988\n",
            "[20]\tvalid_0's rmse: 0.15474\n",
            "[30]\tvalid_0's rmse: 0.116199\n",
            "[40]\tvalid_0's rmse: 0.0889203\n",
            "[50]\tvalid_0's rmse: 0.0694039\n",
            "[60]\tvalid_0's rmse: 0.0617086\n",
            "[70]\tvalid_0's rmse: 0.0558565\n",
            "[80]\tvalid_0's rmse: 0.0508024\n",
            "[90]\tvalid_0's rmse: 0.0491868\n",
            "[100]\tvalid_0's rmse: 0.0491868\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.0491868\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.22877\n",
            "[20]\tvalid_0's rmse: 0.166523\n",
            "[30]\tvalid_0's rmse: 0.127507\n",
            "[40]\tvalid_0's rmse: 0.100724\n",
            "[50]\tvalid_0's rmse: 0.0806273\n",
            "[60]\tvalid_0's rmse: 0.0721972\n",
            "[70]\tvalid_0's rmse: 0.0656316\n",
            "[80]\tvalid_0's rmse: 0.0603035\n",
            "[90]\tvalid_0's rmse: 0.0580393\n",
            "[100]\tvalid_0's rmse: 0.0580393\n",
            "Early stopping, best iteration is:\n",
            "[85]\tvalid_0's rmse: 0.0580393\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.158702\n",
            "[20]\tvalid_0's rmse: 0.119476\n",
            "[30]\tvalid_0's rmse: 0.0827809\n",
            "[40]\tvalid_0's rmse: 0.0565413\n",
            "[50]\tvalid_0's rmse: 0.0395784\n",
            "[60]\tvalid_0's rmse: 0.0278772\n",
            "[70]\tvalid_0's rmse: 0.019817\n",
            "[80]\tvalid_0's rmse: 0.0133574\n",
            "[90]\tvalid_0's rmse: 0.00693346\n",
            "[100]\tvalid_0's rmse: 0.00693346\n",
            "Early stopping, best iteration is:\n",
            "[89]\tvalid_0's rmse: 0.00693346\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.147758\n",
            "[20]\tvalid_0's rmse: 0.108988\n",
            "[30]\tvalid_0's rmse: 0.0740998\n",
            "[40]\tvalid_0's rmse: 0.05004\n",
            "[50]\tvalid_0's rmse: 0.0342207\n",
            "[60]\tvalid_0's rmse: 0.0257547\n",
            "[70]\tvalid_0's rmse: 0.020277\n",
            "[80]\tvalid_0's rmse: 0.0159565\n",
            "[90]\tvalid_0's rmse: 0.0133611\n",
            "[100]\tvalid_0's rmse: 0.0133611\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.0133611\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.175563\n",
            "[20]\tvalid_0's rmse: 0.120065\n",
            "[30]\tvalid_0's rmse: 0.0863295\n",
            "[40]\tvalid_0's rmse: 0.0620427\n",
            "[50]\tvalid_0's rmse: 0.045555\n",
            "[60]\tvalid_0's rmse: 0.0380698\n",
            "[70]\tvalid_0's rmse: 0.0318922\n",
            "[80]\tvalid_0's rmse: 0.0266938\n",
            "[90]\tvalid_0's rmse: 0.023046\n",
            "[100]\tvalid_0's rmse: 0.023046\n",
            "Early stopping, best iteration is:\n",
            "[89]\tvalid_0's rmse: 0.023046\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.304452\n",
            "[20]\tvalid_0's rmse: 0.252913\n",
            "[30]\tvalid_0's rmse: 0.220157\n",
            "[40]\tvalid_0's rmse: 0.197715\n",
            "[50]\tvalid_0's rmse: 0.179601\n",
            "[60]\tvalid_0's rmse: 0.171752\n",
            "[70]\tvalid_0's rmse: 0.165482\n",
            "[80]\tvalid_0's rmse: 0.160539\n",
            "[90]\tvalid_0's rmse: 0.15602\n",
            "[100]\tvalid_0's rmse: 0.15602\n",
            "Early stopping, best iteration is:\n",
            "[88]\tvalid_0's rmse: 0.15602\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.304585\n",
            "[20]\tvalid_0's rmse: 0.244525\n",
            "[30]\tvalid_0's rmse: 0.196781\n",
            "[40]\tvalid_0's rmse: 0.166198\n",
            "[50]\tvalid_0's rmse: 0.142139\n",
            "[60]\tvalid_0's rmse: 0.128569\n",
            "[70]\tvalid_0's rmse: 0.123387\n",
            "[80]\tvalid_0's rmse: 0.117634\n",
            "[90]\tvalid_0's rmse: 0.112816\n",
            "[100]\tvalid_0's rmse: 0.112816\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.112816\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.183747\n",
            "[20]\tvalid_0's rmse: 0.124095\n",
            "[30]\tvalid_0's rmse: 0.0760617\n",
            "[40]\tvalid_0's rmse: 0.049855\n",
            "[50]\tvalid_0's rmse: 0.0318839\n",
            "[60]\tvalid_0's rmse: 0.0235331\n",
            "[70]\tvalid_0's rmse: 0.0186556\n",
            "[80]\tvalid_0's rmse: 0.0149381\n",
            "[90]\tvalid_0's rmse: 0.0146153\n",
            "[100]\tvalid_0's rmse: 0.0146153\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0146153\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.183189\n",
            "[20]\tvalid_0's rmse: 0.119281\n",
            "[30]\tvalid_0's rmse: 0.073541\n",
            "[40]\tvalid_0's rmse: 0.0508806\n",
            "[50]\tvalid_0's rmse: 0.0375924\n",
            "[60]\tvalid_0's rmse: 0.0308666\n",
            "[70]\tvalid_0's rmse: 0.0251644\n",
            "[80]\tvalid_0's rmse: 0.0211572\n",
            "[90]\tvalid_0's rmse: 0.0173391\n",
            "[100]\tvalid_0's rmse: 0.0165217\n",
            "[110]\tvalid_0's rmse: 0.0157724\n",
            "[120]\tvalid_0's rmse: 0.0149933\n",
            "[130]\tvalid_0's rmse: 0.0149933\n",
            "Early stopping, best iteration is:\n",
            "[112]\tvalid_0's rmse: 0.0149933\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.197033\n",
            "[20]\tvalid_0's rmse: 0.131499\n",
            "[30]\tvalid_0's rmse: 0.090076\n",
            "[40]\tvalid_0's rmse: 0.0679992\n",
            "[50]\tvalid_0's rmse: 0.0522393\n",
            "[60]\tvalid_0's rmse: 0.0425132\n",
            "[70]\tvalid_0's rmse: 0.0382369\n",
            "[80]\tvalid_0's rmse: 0.0350176\n",
            "[90]\tvalid_0's rmse: 0.0346513\n",
            "[100]\tvalid_0's rmse: 0.0346513\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0346513\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.22805\n",
            "[20]\tvalid_0's rmse: 0.162003\n",
            "[30]\tvalid_0's rmse: 0.117389\n",
            "[40]\tvalid_0's rmse: 0.0919494\n",
            "[50]\tvalid_0's rmse: 0.0756425\n",
            "[60]\tvalid_0's rmse: 0.0630265\n",
            "[70]\tvalid_0's rmse: 0.0585347\n",
            "[80]\tvalid_0's rmse: 0.0543166\n",
            "[90]\tvalid_0's rmse: 0.0543166\n",
            "[100]\tvalid_0's rmse: 0.0543166\n",
            "Early stopping, best iteration is:\n",
            "[80]\tvalid_0's rmse: 0.0543166\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0847116\n",
            "[20]\tvalid_0's rmse: 0.038958\n",
            "[30]\tvalid_0's rmse: 0.0120293\n",
            "[40]\tvalid_0's rmse: 0.00154773\n",
            "[50]\tvalid_0's rmse: 0.0117003\n",
            "Early stopping, best iteration is:\n",
            "[38]\tvalid_0's rmse: 0.00141778\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0897922\n",
            "[20]\tvalid_0's rmse: 0.0393742\n",
            "[30]\tvalid_0's rmse: 0.010884\n",
            "[40]\tvalid_0's rmse: 0.00601989\n",
            "[50]\tvalid_0's rmse: 0.0166312\n",
            "Early stopping, best iteration is:\n",
            "[35]\tvalid_0's rmse: 0.000149327\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.103862\n",
            "[20]\tvalid_0's rmse: 0.0524566\n",
            "[30]\tvalid_0's rmse: 0.0150517\n",
            "[40]\tvalid_0's rmse: 0.00529341\n",
            "[50]\tvalid_0's rmse: 0.0142807\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's rmse: 0.000804899\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.128925\n",
            "[20]\tvalid_0's rmse: 0.0859285\n",
            "[30]\tvalid_0's rmse: 0.0596401\n",
            "[40]\tvalid_0's rmse: 0.0330821\n",
            "[50]\tvalid_0's rmse: 0.0184922\n",
            "[60]\tvalid_0's rmse: 0.011506\n",
            "[70]\tvalid_0's rmse: 0.00672172\n",
            "[80]\tvalid_0's rmse: 0.00713134\n",
            "[90]\tvalid_0's rmse: 0.00920787\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.00422931\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.133181\n",
            "[20]\tvalid_0's rmse: 0.0945101\n",
            "[30]\tvalid_0's rmse: 0.0637138\n",
            "[40]\tvalid_0's rmse: 0.0439889\n",
            "[50]\tvalid_0's rmse: 0.0292016\n",
            "[60]\tvalid_0's rmse: 0.0208115\n",
            "[70]\tvalid_0's rmse: 0.016917\n",
            "[80]\tvalid_0's rmse: 0.0123336\n",
            "[90]\tvalid_0's rmse: 0.0107964\n",
            "[100]\tvalid_0's rmse: 0.0107964\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.0107964\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0560303\n",
            "[20]\tvalid_0's rmse: 0.0172677\n",
            "[30]\tvalid_0's rmse: 0.00207217\n",
            "[40]\tvalid_0's rmse: 0.0126469\n",
            "[50]\tvalid_0's rmse: 0.0182583\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's rmse: 0.000746943\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0771966\n",
            "[20]\tvalid_0's rmse: 0.0451606\n",
            "[30]\tvalid_0's rmse: 0.0258388\n",
            "[40]\tvalid_0's rmse: 0.0119466\n",
            "[50]\tvalid_0's rmse: 0.00578394\n",
            "[60]\tvalid_0's rmse: 0.000128063\n",
            "[70]\tvalid_0's rmse: 0.00468404\n",
            "[80]\tvalid_0's rmse: 0.00735701\n",
            "Early stopping, best iteration is:\n",
            "[60]\tvalid_0's rmse: 0.000128063\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0247958\n",
            "[20]\tvalid_0's rmse: 0.0216876\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.0100258\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00235811\n",
            "[20]\tvalid_0's rmse: 0.00325694\n",
            "[30]\tvalid_0's rmse: 0.00135248\n",
            "[40]\tvalid_0's rmse: 0.000645163\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's rmse: 0.000168926\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.502485\n",
            "[20]\tvalid_0's rmse: 0.502485\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.502485\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.511336\n",
            "[20]\tvalid_0's rmse: 0.511336\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.511336\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444358\n",
            "[20]\tvalid_0's rmse: 0.444358\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.444358\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.398313\n",
            "[20]\tvalid_0's rmse: 0.398313\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.398313\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416187\n",
            "[20]\tvalid_0's rmse: 0.416187\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.416187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.42288\n",
            "[20]\tvalid_0's rmse: 0.42288\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.42288\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.496021\n",
            "[20]\tvalid_0's rmse: 0.496021\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.496021\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.374953\n",
            "[20]\tvalid_0's rmse: 0.374953\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.374953\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.490488\n",
            "[20]\tvalid_0's rmse: 0.490488\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.490488\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.505679\n",
            "[20]\tvalid_0's rmse: 0.505679\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.505679\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.367176\n",
            "[20]\tvalid_0's rmse: 0.367176\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.367176\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.464969\n",
            "[20]\tvalid_0's rmse: 0.464969\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.464969\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.461365\n",
            "[20]\tvalid_0's rmse: 0.461365\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.461365\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.441812\n",
            "[20]\tvalid_0's rmse: 0.441812\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.441812\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.365738\n",
            "[20]\tvalid_0's rmse: 0.365738\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.365738\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.298912\n",
            "[20]\tvalid_0's rmse: 0.233926\n",
            "[30]\tvalid_0's rmse: 0.184904\n",
            "[40]\tvalid_0's rmse: 0.147211\n",
            "[50]\tvalid_0's rmse: 0.118008\n",
            "[60]\tvalid_0's rmse: 0.0951235\n",
            "[70]\tvalid_0's rmse: 0.0768057\n",
            "[80]\tvalid_0's rmse: 0.0619827\n",
            "[90]\tvalid_0's rmse: 0.0593864\n",
            "[100]\tvalid_0's rmse: 0.0593765\n",
            "[110]\tvalid_0's rmse: 0.0571611\n",
            "[120]\tvalid_0's rmse: 0.0571716\n",
            "[130]\tvalid_0's rmse: 0.0553117\n",
            "[140]\tvalid_0's rmse: 0.0553042\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 0.0552891\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.3163\n",
            "[20]\tvalid_0's rmse: 0.251126\n",
            "[30]\tvalid_0's rmse: 0.204918\n",
            "[40]\tvalid_0's rmse: 0.16826\n",
            "[50]\tvalid_0's rmse: 0.139836\n",
            "[60]\tvalid_0's rmse: 0.117427\n",
            "[70]\tvalid_0's rmse: 0.109926\n",
            "[80]\tvalid_0's rmse: 0.10983\n",
            "[90]\tvalid_0's rmse: 0.112753\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.108257\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.155927\n",
            "[20]\tvalid_0's rmse: 0.0932797\n",
            "[30]\tvalid_0's rmse: 0.0759457\n",
            "[40]\tvalid_0's rmse: 0.0556052\n",
            "[50]\tvalid_0's rmse: 0.0395384\n",
            "[60]\tvalid_0's rmse: 0.0303964\n",
            "[70]\tvalid_0's rmse: 0.0310867\n",
            "[80]\tvalid_0's rmse: 0.031568\n",
            "[90]\tvalid_0's rmse: 0.0294395\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.0280966\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.144714\n",
            "[20]\tvalid_0's rmse: 0.0872457\n",
            "[30]\tvalid_0's rmse: 0.0459301\n",
            "[40]\tvalid_0's rmse: 0.0153384\n",
            "[50]\tvalid_0's rmse: 0.00815246\n",
            "[60]\tvalid_0's rmse: 0.0109944\n",
            "[70]\tvalid_0's rmse: 0.0133807\n",
            "Early stopping, best iteration is:\n",
            "[53]\tvalid_0's rmse: 0.00684057\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.230911\n",
            "[20]\tvalid_0's rmse: 0.174067\n",
            "[30]\tvalid_0's rmse: 0.132177\n",
            "[40]\tvalid_0's rmse: 0.101325\n",
            "[50]\tvalid_0's rmse: 0.0938121\n",
            "[60]\tvalid_0's rmse: 0.0907259\n",
            "[70]\tvalid_0's rmse: 0.0889866\n",
            "[80]\tvalid_0's rmse: 0.084304\n",
            "[90]\tvalid_0's rmse: 0.0832136\n",
            "[100]\tvalid_0's rmse: 0.0785215\n",
            "[110]\tvalid_0's rmse: 0.0744168\n",
            "[120]\tvalid_0's rmse: 0.0705862\n",
            "[130]\tvalid_0's rmse: 0.067023\n",
            "[140]\tvalid_0's rmse: 0.0635952\n",
            "[150]\tvalid_0's rmse: 0.0623308\n",
            "[160]\tvalid_0's rmse: 0.0623308\n",
            "Early stopping, best iteration is:\n",
            "[147]\tvalid_0's rmse: 0.0613632\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.221589\n",
            "[20]\tvalid_0's rmse: 0.165035\n",
            "[30]\tvalid_0's rmse: 0.12504\n",
            "[40]\tvalid_0's rmse: 0.101198\n",
            "[50]\tvalid_0's rmse: 0.091722\n",
            "[60]\tvalid_0's rmse: 0.088556\n",
            "[70]\tvalid_0's rmse: 0.0862956\n",
            "[80]\tvalid_0's rmse: 0.0805082\n",
            "[90]\tvalid_0's rmse: 0.0760369\n",
            "[100]\tvalid_0's rmse: 0.071631\n",
            "[110]\tvalid_0's rmse: 0.0675666\n",
            "[120]\tvalid_0's rmse: 0.0640692\n",
            "[130]\tvalid_0's rmse: 0.0604649\n",
            "[140]\tvalid_0's rmse: 0.0593096\n",
            "[150]\tvalid_0's rmse: 0.0564123\n",
            "[160]\tvalid_0's rmse: 0.0555773\n",
            "[170]\tvalid_0's rmse: 0.0555773\n",
            "Early stopping, best iteration is:\n",
            "[151]\tvalid_0's rmse: 0.0555595\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.173677\n",
            "[20]\tvalid_0's rmse: 0.121707\n",
            "[30]\tvalid_0's rmse: 0.0853568\n",
            "[40]\tvalid_0's rmse: 0.0586367\n",
            "[50]\tvalid_0's rmse: 0.0384177\n",
            "[60]\tvalid_0's rmse: 0.0220987\n",
            "[70]\tvalid_0's rmse: 0.0106644\n",
            "[80]\tvalid_0's rmse: 0.00564803\n",
            "[90]\tvalid_0's rmse: 0.00389642\n",
            "[100]\tvalid_0's rmse: 0.00210198\n",
            "[110]\tvalid_0's rmse: 0.000590022\n",
            "[120]\tvalid_0's rmse: 0.00144126\n",
            "[130]\tvalid_0's rmse: 0.000181122\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 9.87594e-06\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.237277\n",
            "[20]\tvalid_0's rmse: 0.185397\n",
            "[30]\tvalid_0's rmse: 0.149432\n",
            "[40]\tvalid_0's rmse: 0.12226\n",
            "[50]\tvalid_0's rmse: 0.100347\n",
            "[60]\tvalid_0's rmse: 0.0833068\n",
            "[70]\tvalid_0's rmse: 0.0680902\n",
            "[80]\tvalid_0's rmse: 0.0630169\n",
            "[90]\tvalid_0's rmse: 0.0578866\n",
            "[100]\tvalid_0's rmse: 0.0584969\n",
            "[110]\tvalid_0's rmse: 0.0538222\n",
            "[120]\tvalid_0's rmse: 0.0517233\n",
            "[130]\tvalid_0's rmse: 0.0498259\n",
            "[140]\tvalid_0's rmse: 0.0485031\n",
            "[150]\tvalid_0's rmse: 0.0463862\n",
            "[160]\tvalid_0's rmse: 0.044604\n",
            "[170]\tvalid_0's rmse: 0.044604\n",
            "Early stopping, best iteration is:\n",
            "[156]\tvalid_0's rmse: 0.044604\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.250573\n",
            "[20]\tvalid_0's rmse: 0.200153\n",
            "[30]\tvalid_0's rmse: 0.164226\n",
            "[40]\tvalid_0's rmse: 0.136559\n",
            "[50]\tvalid_0's rmse: 0.115048\n",
            "[60]\tvalid_0's rmse: 0.0968634\n",
            "[70]\tvalid_0's rmse: 0.0811266\n",
            "[80]\tvalid_0's rmse: 0.0783882\n",
            "[90]\tvalid_0's rmse: 0.0761288\n",
            "[100]\tvalid_0's rmse: 0.0739688\n",
            "[110]\tvalid_0's rmse: 0.0698286\n",
            "[120]\tvalid_0's rmse: 0.0681241\n",
            "[130]\tvalid_0's rmse: 0.0666465\n",
            "[140]\tvalid_0's rmse: 0.0651018\n",
            "[150]\tvalid_0's rmse: 0.063649\n",
            "[160]\tvalid_0's rmse: 0.060884\n",
            "[170]\tvalid_0's rmse: 0.060884\n",
            "Early stopping, best iteration is:\n",
            "[158]\tvalid_0's rmse: 0.0607843\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.167387\n",
            "[20]\tvalid_0's rmse: 0.115916\n",
            "[30]\tvalid_0's rmse: 0.0787724\n",
            "[40]\tvalid_0's rmse: 0.0522936\n",
            "[50]\tvalid_0's rmse: 0.0327756\n",
            "[60]\tvalid_0's rmse: 0.0166532\n",
            "[70]\tvalid_0's rmse: 0.00331178\n",
            "[80]\tvalid_0's rmse: 0.00389812\n",
            "[90]\tvalid_0's rmse: 0.00641556\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 5.22579e-06\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.159558\n",
            "[20]\tvalid_0's rmse: 0.111137\n",
            "[30]\tvalid_0's rmse: 0.0760936\n",
            "[40]\tvalid_0's rmse: 0.0577439\n",
            "[50]\tvalid_0's rmse: 0.0397556\n",
            "[60]\tvalid_0's rmse: 0.0289907\n",
            "[70]\tvalid_0's rmse: 0.0265207\n",
            "[80]\tvalid_0's rmse: 0.0261741\n",
            "[90]\tvalid_0's rmse: 0.0222401\n",
            "[100]\tvalid_0's rmse: 0.0219897\n",
            "[110]\tvalid_0's rmse: 0.0217033\n",
            "[120]\tvalid_0's rmse: 0.018687\n",
            "[130]\tvalid_0's rmse: 0.0179411\n",
            "[140]\tvalid_0's rmse: 0.0149105\n",
            "[150]\tvalid_0's rmse: 0.0138273\n",
            "[160]\tvalid_0's rmse: 0.0127469\n",
            "[170]\tvalid_0's rmse: 0.012786\n",
            "Early stopping, best iteration is:\n",
            "[159]\tvalid_0's rmse: 0.0120505\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.189117\n",
            "[20]\tvalid_0's rmse: 0.141174\n",
            "[30]\tvalid_0's rmse: 0.103655\n",
            "[40]\tvalid_0's rmse: 0.0786472\n",
            "[50]\tvalid_0's rmse: 0.0610525\n",
            "[60]\tvalid_0's rmse: 0.054365\n",
            "[70]\tvalid_0's rmse: 0.0544678\n",
            "Early stopping, best iteration is:\n",
            "[57]\tvalid_0's rmse: 0.0512047\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.320819\n",
            "[20]\tvalid_0's rmse: 0.275613\n",
            "[30]\tvalid_0's rmse: 0.236451\n",
            "[40]\tvalid_0's rmse: 0.212808\n",
            "[50]\tvalid_0's rmse: 0.198106\n",
            "[60]\tvalid_0's rmse: 0.185857\n",
            "[70]\tvalid_0's rmse: 0.176289\n",
            "[80]\tvalid_0's rmse: 0.168274\n",
            "[90]\tvalid_0's rmse: 0.167822\n",
            "[100]\tvalid_0's rmse: 0.166771\n",
            "[110]\tvalid_0's rmse: 0.163885\n",
            "[120]\tvalid_0's rmse: 0.161127\n",
            "[130]\tvalid_0's rmse: 0.160384\n",
            "[140]\tvalid_0's rmse: 0.160384\n",
            "Early stopping, best iteration is:\n",
            "[125]\tvalid_0's rmse: 0.160384\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.327019\n",
            "[20]\tvalid_0's rmse: 0.279401\n",
            "[30]\tvalid_0's rmse: 0.245087\n",
            "[40]\tvalid_0's rmse: 0.214387\n",
            "[50]\tvalid_0's rmse: 0.198418\n",
            "[60]\tvalid_0's rmse: 0.186615\n",
            "[70]\tvalid_0's rmse: 0.172561\n",
            "[80]\tvalid_0's rmse: 0.170391\n",
            "[90]\tvalid_0's rmse: 0.164468\n",
            "[100]\tvalid_0's rmse: 0.165283\n",
            "[110]\tvalid_0's rmse: 0.162752\n",
            "[120]\tvalid_0's rmse: 0.159998\n",
            "[130]\tvalid_0's rmse: 0.158804\n",
            "[140]\tvalid_0's rmse: 0.158804\n",
            "Early stopping, best iteration is:\n",
            "[122]\tvalid_0's rmse: 0.15792\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.204725\n",
            "[20]\tvalid_0's rmse: 0.157246\n",
            "[30]\tvalid_0's rmse: 0.118611\n",
            "[40]\tvalid_0's rmse: 0.0962302\n",
            "[50]\tvalid_0's rmse: 0.0817928\n",
            "[60]\tvalid_0's rmse: 0.0695041\n",
            "[70]\tvalid_0's rmse: 0.0571575\n",
            "[80]\tvalid_0's rmse: 0.0521422\n",
            "[90]\tvalid_0's rmse: 0.0494718\n",
            "[100]\tvalid_0's rmse: 0.0438496\n",
            "[110]\tvalid_0's rmse: 0.0424271\n",
            "[120]\tvalid_0's rmse: 0.0408257\n",
            "[130]\tvalid_0's rmse: 0.0408257\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 0.0408257\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.20134\n",
            "[20]\tvalid_0's rmse: 0.149907\n",
            "[30]\tvalid_0's rmse: 0.111978\n",
            "[40]\tvalid_0's rmse: 0.0885535\n",
            "[50]\tvalid_0's rmse: 0.0747123\n",
            "[60]\tvalid_0's rmse: 0.0661546\n",
            "[70]\tvalid_0's rmse: 0.0586574\n",
            "[80]\tvalid_0's rmse: 0.0539878\n",
            "[90]\tvalid_0's rmse: 0.0469835\n",
            "[100]\tvalid_0's rmse: 0.0424331\n",
            "[110]\tvalid_0's rmse: 0.0406234\n",
            "[120]\tvalid_0's rmse: 0.0370102\n",
            "[130]\tvalid_0's rmse: 0.0340512\n",
            "[140]\tvalid_0's rmse: 0.0348775\n",
            "Early stopping, best iteration is:\n",
            "[124]\tvalid_0's rmse: 0.0339681\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.217466\n",
            "[20]\tvalid_0's rmse: 0.163206\n",
            "[30]\tvalid_0's rmse: 0.126001\n",
            "[40]\tvalid_0's rmse: 0.0966167\n",
            "[50]\tvalid_0's rmse: 0.0784932\n",
            "[60]\tvalid_0's rmse: 0.0632589\n",
            "[70]\tvalid_0's rmse: 0.051956\n",
            "[80]\tvalid_0's rmse: 0.043746\n",
            "[90]\tvalid_0's rmse: 0.0358069\n",
            "[100]\tvalid_0's rmse: 0.0280263\n",
            "[110]\tvalid_0's rmse: 0.0230433\n",
            "[120]\tvalid_0's rmse: 0.0216886\n",
            "[130]\tvalid_0's rmse: 0.0216886\n",
            "Early stopping, best iteration is:\n",
            "[113]\tvalid_0's rmse: 0.0212598\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.241571\n",
            "[20]\tvalid_0's rmse: 0.18351\n",
            "[30]\tvalid_0's rmse: 0.141596\n",
            "[40]\tvalid_0's rmse: 0.112082\n",
            "[50]\tvalid_0's rmse: 0.0935532\n",
            "[60]\tvalid_0's rmse: 0.0831732\n",
            "[70]\tvalid_0's rmse: 0.0751287\n",
            "[80]\tvalid_0's rmse: 0.0661235\n",
            "[90]\tvalid_0's rmse: 0.0579927\n",
            "[100]\tvalid_0's rmse: 0.049862\n",
            "[110]\tvalid_0's rmse: 0.0426503\n",
            "[120]\tvalid_0's rmse: 0.0404205\n",
            "[130]\tvalid_0's rmse: 0.0404205\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 0.0397071\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0840021\n",
            "[20]\tvalid_0's rmse: 0.0287084\n",
            "[30]\tvalid_0's rmse: 0.00607254\n",
            "[40]\tvalid_0's rmse: 0.0271818\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's rmse: 0.00137674\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0776961\n",
            "[20]\tvalid_0's rmse: 0.0312698\n",
            "[30]\tvalid_0's rmse: 0.0023098\n",
            "[40]\tvalid_0's rmse: 0.0165998\n",
            "[50]\tvalid_0's rmse: 0.0296481\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's rmse: 0.00119377\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.115732\n",
            "[20]\tvalid_0's rmse: 0.0686139\n",
            "[30]\tvalid_0's rmse: 0.0431673\n",
            "[40]\tvalid_0's rmse: 0.0244\n",
            "[50]\tvalid_0's rmse: 0.0109452\n",
            "[60]\tvalid_0's rmse: 0.0142294\n",
            "[70]\tvalid_0's rmse: 0.015932\n",
            "Early stopping, best iteration is:\n",
            "[50]\tvalid_0's rmse: 0.0109452\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.123289\n",
            "[20]\tvalid_0's rmse: 0.0756483\n",
            "[30]\tvalid_0's rmse: 0.0444531\n",
            "[40]\tvalid_0's rmse: 0.0201891\n",
            "[50]\tvalid_0's rmse: 0.0234235\n",
            "[60]\tvalid_0's rmse: 0.0207767\n",
            "Early stopping, best iteration is:\n",
            "[44]\tvalid_0's rmse: 0.0159989\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.129717\n",
            "[20]\tvalid_0's rmse: 0.075771\n",
            "[30]\tvalid_0's rmse: 0.0447611\n",
            "[40]\tvalid_0's rmse: 0.0262172\n",
            "[50]\tvalid_0's rmse: 0.0170229\n",
            "[60]\tvalid_0's rmse: 0.0152819\n",
            "[70]\tvalid_0's rmse: 0.0153291\n",
            "[80]\tvalid_0's rmse: 0.0140136\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0129414\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0283145\n",
            "[20]\tvalid_0's rmse: 0.0123337\n",
            "[30]\tvalid_0's rmse: 0.0314426\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's rmse: 0.00217251\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0558058\n",
            "[20]\tvalid_0's rmse: 0.015392\n",
            "[30]\tvalid_0's rmse: 0.00454896\n",
            "[40]\tvalid_0's rmse: 0.00806167\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's rmse: 8.35839e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0303313\n",
            "[20]\tvalid_0's rmse: 0.0424748\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.0133414\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00910219\n",
            "[20]\tvalid_0's rmse: 0.0184014\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.000582763\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.351881\n",
            "[20]\tvalid_0's rmse: 0.248817\n",
            "[30]\tvalid_0's rmse: 0.192271\n",
            "[40]\tvalid_0's rmse: 0.147419\n",
            "[50]\tvalid_0's rmse: 0.109146\n",
            "[60]\tvalid_0's rmse: 0.0874527\n",
            "[70]\tvalid_0's rmse: 0.0874527\n",
            "[80]\tvalid_0's rmse: 0.0874527\n",
            "Early stopping, best iteration is:\n",
            "[60]\tvalid_0's rmse: 0.0874527\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.358079\n",
            "[20]\tvalid_0's rmse: 0.256611\n",
            "[30]\tvalid_0's rmse: 0.188665\n",
            "[40]\tvalid_0's rmse: 0.14345\n",
            "[50]\tvalid_0's rmse: 0.117185\n",
            "[60]\tvalid_0's rmse: 0.0964553\n",
            "[70]\tvalid_0's rmse: 0.0881888\n",
            "[80]\tvalid_0's rmse: 0.0881888\n",
            "Early stopping, best iteration is:\n",
            "[64]\tvalid_0's rmse: 0.0881888\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.313348\n",
            "[20]\tvalid_0's rmse: 0.224209\n",
            "[30]\tvalid_0's rmse: 0.165225\n",
            "[40]\tvalid_0's rmse: 0.123623\n",
            "[50]\tvalid_0's rmse: 0.0967543\n",
            "[60]\tvalid_0's rmse: 0.0811834\n",
            "[70]\tvalid_0's rmse: 0.0724356\n",
            "[80]\tvalid_0's rmse: 0.0724356\n",
            "Early stopping, best iteration is:\n",
            "[67]\tvalid_0's rmse: 0.0724356\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.284255\n",
            "[20]\tvalid_0's rmse: 0.205317\n",
            "[30]\tvalid_0's rmse: 0.152634\n",
            "[40]\tvalid_0's rmse: 0.116223\n",
            "[50]\tvalid_0's rmse: 0.094162\n",
            "[60]\tvalid_0's rmse: 0.0786825\n",
            "[70]\tvalid_0's rmse: 0.0671404\n",
            "[80]\tvalid_0's rmse: 0.0654118\n",
            "[90]\tvalid_0's rmse: 0.0654118\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0654118\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.307955\n",
            "[20]\tvalid_0's rmse: 0.232597\n",
            "[30]\tvalid_0's rmse: 0.18042\n",
            "[40]\tvalid_0's rmse: 0.148346\n",
            "[50]\tvalid_0's rmse: 0.127808\n",
            "[60]\tvalid_0's rmse: 0.111206\n",
            "[70]\tvalid_0's rmse: 0.0966772\n",
            "[80]\tvalid_0's rmse: 0.0935741\n",
            "[90]\tvalid_0's rmse: 0.0935741\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.0935741\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.309914\n",
            "[20]\tvalid_0's rmse: 0.227607\n",
            "[30]\tvalid_0's rmse: 0.179864\n",
            "[40]\tvalid_0's rmse: 0.143219\n",
            "[50]\tvalid_0's rmse: 0.118825\n",
            "[60]\tvalid_0's rmse: 0.105678\n",
            "[70]\tvalid_0's rmse: 0.0954088\n",
            "[80]\tvalid_0's rmse: 0.0944912\n",
            "[90]\tvalid_0's rmse: 0.0944912\n",
            "Early stopping, best iteration is:\n",
            "[71]\tvalid_0's rmse: 0.0944912\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.347355\n",
            "[20]\tvalid_0's rmse: 0.248919\n",
            "[30]\tvalid_0's rmse: 0.183672\n",
            "[40]\tvalid_0's rmse: 0.140781\n",
            "[50]\tvalid_0's rmse: 0.125631\n",
            "[60]\tvalid_0's rmse: 0.110995\n",
            "[70]\tvalid_0's rmse: 0.0956522\n",
            "[80]\tvalid_0's rmse: 0.093897\n",
            "[90]\tvalid_0's rmse: 0.093897\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.093897\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.249538\n",
            "[20]\tvalid_0's rmse: 0.170873\n",
            "[30]\tvalid_0's rmse: 0.114834\n",
            "[40]\tvalid_0's rmse: 0.079073\n",
            "[50]\tvalid_0's rmse: 0.0560421\n",
            "[60]\tvalid_0's rmse: 0.0434056\n",
            "[70]\tvalid_0's rmse: 0.0340039\n",
            "[80]\tvalid_0's rmse: 0.0311391\n",
            "[90]\tvalid_0's rmse: 0.0311391\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0311391\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.346306\n",
            "[20]\tvalid_0's rmse: 0.256544\n",
            "[30]\tvalid_0's rmse: 0.195679\n",
            "[40]\tvalid_0's rmse: 0.148951\n",
            "[50]\tvalid_0's rmse: 0.121589\n",
            "[60]\tvalid_0's rmse: 0.107989\n",
            "[70]\tvalid_0's rmse: 0.095956\n",
            "[80]\tvalid_0's rmse: 0.0956692\n",
            "[90]\tvalid_0's rmse: 0.0956692\n",
            "Early stopping, best iteration is:\n",
            "[71]\tvalid_0's rmse: 0.0956692\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.362706\n",
            "[20]\tvalid_0's rmse: 0.266221\n",
            "[30]\tvalid_0's rmse: 0.201732\n",
            "[40]\tvalid_0's rmse: 0.154097\n",
            "[50]\tvalid_0's rmse: 0.131311\n",
            "[60]\tvalid_0's rmse: 0.111481\n",
            "[70]\tvalid_0's rmse: 0.0997462\n",
            "[80]\tvalid_0's rmse: 0.0950474\n",
            "[90]\tvalid_0's rmse: 0.0950474\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.0950474\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.25189\n",
            "[20]\tvalid_0's rmse: 0.171298\n",
            "[30]\tvalid_0's rmse: 0.116581\n",
            "[40]\tvalid_0's rmse: 0.078821\n",
            "[50]\tvalid_0's rmse: 0.0540749\n",
            "[60]\tvalid_0's rmse: 0.0403437\n",
            "[70]\tvalid_0's rmse: 0.0285254\n",
            "[80]\tvalid_0's rmse: 0.0263794\n",
            "[90]\tvalid_0's rmse: 0.0263794\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.0263794\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.335138\n",
            "[20]\tvalid_0's rmse: 0.246032\n",
            "[30]\tvalid_0's rmse: 0.186264\n",
            "[40]\tvalid_0's rmse: 0.146006\n",
            "[50]\tvalid_0's rmse: 0.118259\n",
            "[60]\tvalid_0's rmse: 0.0996465\n",
            "[70]\tvalid_0's rmse: 0.0878922\n",
            "[80]\tvalid_0's rmse: 0.082955\n",
            "[90]\tvalid_0's rmse: 0.082955\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.082955\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.335529\n",
            "[20]\tvalid_0's rmse: 0.25125\n",
            "[30]\tvalid_0's rmse: 0.194302\n",
            "[40]\tvalid_0's rmse: 0.155869\n",
            "[50]\tvalid_0's rmse: 0.125068\n",
            "[60]\tvalid_0's rmse: 0.111844\n",
            "[70]\tvalid_0's rmse: 0.0978582\n",
            "[80]\tvalid_0's rmse: 0.0900589\n",
            "[90]\tvalid_0's rmse: 0.0900589\n",
            "Early stopping, best iteration is:\n",
            "[79]\tvalid_0's rmse: 0.0900589\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.321299\n",
            "[20]\tvalid_0's rmse: 0.235378\n",
            "[30]\tvalid_0's rmse: 0.179523\n",
            "[40]\tvalid_0's rmse: 0.138694\n",
            "[50]\tvalid_0's rmse: 0.109071\n",
            "[60]\tvalid_0's rmse: 0.0923163\n",
            "[70]\tvalid_0's rmse: 0.0808157\n",
            "[80]\tvalid_0's rmse: 0.0710834\n",
            "[90]\tvalid_0's rmse: 0.0710834\n",
            "Early stopping, best iteration is:\n",
            "[79]\tvalid_0's rmse: 0.0710834\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.252276\n",
            "[20]\tvalid_0's rmse: 0.17374\n",
            "[30]\tvalid_0's rmse: 0.119112\n",
            "[40]\tvalid_0's rmse: 0.080101\n",
            "[50]\tvalid_0's rmse: 0.0575145\n",
            "[60]\tvalid_0's rmse: 0.0409343\n",
            "[70]\tvalid_0's rmse: 0.0301657\n",
            "[80]\tvalid_0's rmse: 0.0194936\n",
            "[90]\tvalid_0's rmse: 0.0194936\n",
            "[100]\tvalid_0's rmse: 0.0194936\n",
            "Early stopping, best iteration is:\n",
            "[80]\tvalid_0's rmse: 0.0194936\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.282747\n",
            "[20]\tvalid_0's rmse: 0.203972\n",
            "[30]\tvalid_0's rmse: 0.151503\n",
            "[40]\tvalid_0's rmse: 0.115438\n",
            "[50]\tvalid_0's rmse: 0.0889559\n",
            "[60]\tvalid_0's rmse: 0.0735208\n",
            "[70]\tvalid_0's rmse: 0.0634273\n",
            "[80]\tvalid_0's rmse: 0.0541167\n",
            "[90]\tvalid_0's rmse: 0.0515473\n",
            "[100]\tvalid_0's rmse: 0.0515473\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0515473\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.305271\n",
            "[20]\tvalid_0's rmse: 0.231099\n",
            "[30]\tvalid_0's rmse: 0.178251\n",
            "[40]\tvalid_0's rmse: 0.143527\n",
            "[50]\tvalid_0's rmse: 0.116125\n",
            "[60]\tvalid_0's rmse: 0.101393\n",
            "[70]\tvalid_0's rmse: 0.090182\n",
            "[80]\tvalid_0's rmse: 0.0828095\n",
            "[90]\tvalid_0's rmse: 0.0818293\n",
            "[100]\tvalid_0's rmse: 0.0818293\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0818293\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.153416\n",
            "[20]\tvalid_0's rmse: 0.110663\n",
            "[30]\tvalid_0's rmse: 0.0733456\n",
            "[40]\tvalid_0's rmse: 0.0473845\n",
            "[50]\tvalid_0's rmse: 0.032647\n",
            "[60]\tvalid_0's rmse: 0.0187117\n",
            "[70]\tvalid_0's rmse: 0.00895144\n",
            "[80]\tvalid_0's rmse: 0.00642002\n",
            "[90]\tvalid_0's rmse: 0.00626702\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 0.00616789\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.149921\n",
            "[20]\tvalid_0's rmse: 0.101376\n",
            "[30]\tvalid_0's rmse: 0.068455\n",
            "[40]\tvalid_0's rmse: 0.0467488\n",
            "[50]\tvalid_0's rmse: 0.0278406\n",
            "[60]\tvalid_0's rmse: 0.0169835\n",
            "[70]\tvalid_0's rmse: 0.00766964\n",
            "[80]\tvalid_0's rmse: 0.00376885\n",
            "[90]\tvalid_0's rmse: 0.00314826\n",
            "[100]\tvalid_0's rmse: 0.00314826\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.00266542\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.213001\n",
            "[20]\tvalid_0's rmse: 0.148947\n",
            "[30]\tvalid_0's rmse: 0.105501\n",
            "[40]\tvalid_0's rmse: 0.0755926\n",
            "[50]\tvalid_0's rmse: 0.0528007\n",
            "[60]\tvalid_0's rmse: 0.0396324\n",
            "[70]\tvalid_0's rmse: 0.0297506\n",
            "[80]\tvalid_0's rmse: 0.022717\n",
            "[90]\tvalid_0's rmse: 0.020658\n",
            "[100]\tvalid_0's rmse: 0.020658\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.020658\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.207449\n",
            "[20]\tvalid_0's rmse: 0.14597\n",
            "[30]\tvalid_0's rmse: 0.1038\n",
            "[40]\tvalid_0's rmse: 0.0741627\n",
            "[50]\tvalid_0's rmse: 0.0541674\n",
            "[60]\tvalid_0's rmse: 0.0419719\n",
            "[70]\tvalid_0's rmse: 0.0322555\n",
            "[80]\tvalid_0's rmse: 0.0244325\n",
            "[90]\tvalid_0's rmse: 0.0226069\n",
            "[100]\tvalid_0's rmse: 0.0226069\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.0226069\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.171776\n",
            "[20]\tvalid_0's rmse: 0.118041\n",
            "[30]\tvalid_0's rmse: 0.0804317\n",
            "[40]\tvalid_0's rmse: 0.0557643\n",
            "[50]\tvalid_0's rmse: 0.0385443\n",
            "[60]\tvalid_0's rmse: 0.0266542\n",
            "[70]\tvalid_0's rmse: 0.0181339\n",
            "[80]\tvalid_0's rmse: 0.011968\n",
            "[90]\tvalid_0's rmse: 0.0116356\n",
            "[100]\tvalid_0's rmse: 0.0116356\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.0111747\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.225135\n",
            "[20]\tvalid_0's rmse: 0.167963\n",
            "[30]\tvalid_0's rmse: 0.12762\n",
            "[40]\tvalid_0's rmse: 0.0995917\n",
            "[50]\tvalid_0's rmse: 0.0801581\n",
            "[60]\tvalid_0's rmse: 0.0669198\n",
            "[70]\tvalid_0's rmse: 0.0580138\n",
            "[80]\tvalid_0's rmse: 0.0511847\n",
            "[90]\tvalid_0's rmse: 0.0485185\n",
            "[100]\tvalid_0's rmse: 0.0485185\n",
            "Early stopping, best iteration is:\n",
            "[85]\tvalid_0's rmse: 0.0485185\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.239677\n",
            "[20]\tvalid_0's rmse: 0.180836\n",
            "[30]\tvalid_0's rmse: 0.140982\n",
            "[40]\tvalid_0's rmse: 0.114393\n",
            "[50]\tvalid_0's rmse: 0.0952884\n",
            "[60]\tvalid_0's rmse: 0.0824221\n",
            "[70]\tvalid_0's rmse: 0.0746576\n",
            "[80]\tvalid_0's rmse: 0.0679323\n",
            "[90]\tvalid_0's rmse: 0.0632626\n",
            "[100]\tvalid_0's rmse: 0.0632626\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.0632626\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.164125\n",
            "[20]\tvalid_0's rmse: 0.110195\n",
            "[30]\tvalid_0's rmse: 0.0875597\n",
            "[40]\tvalid_0's rmse: 0.0605263\n",
            "[50]\tvalid_0's rmse: 0.0423368\n",
            "[60]\tvalid_0's rmse: 0.0301046\n",
            "[70]\tvalid_0's rmse: 0.0219084\n",
            "[80]\tvalid_0's rmse: 0.0156675\n",
            "[90]\tvalid_0's rmse: 0.0129911\n",
            "[100]\tvalid_0's rmse: 0.0129911\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.0127249\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.161228\n",
            "[20]\tvalid_0's rmse: 0.132406\n",
            "[30]\tvalid_0's rmse: 0.0963414\n",
            "[40]\tvalid_0's rmse: 0.0704958\n",
            "[50]\tvalid_0's rmse: 0.0534165\n",
            "[60]\tvalid_0's rmse: 0.0423475\n",
            "[70]\tvalid_0's rmse: 0.0342221\n",
            "[80]\tvalid_0's rmse: 0.0280718\n",
            "[90]\tvalid_0's rmse: 0.0248545\n",
            "[100]\tvalid_0's rmse: 0.0248545\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.0242455\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.180514\n",
            "[20]\tvalid_0's rmse: 0.128198\n",
            "[30]\tvalid_0's rmse: 0.0938768\n",
            "[40]\tvalid_0's rmse: 0.0698749\n",
            "[50]\tvalid_0's rmse: 0.0532298\n",
            "[60]\tvalid_0's rmse: 0.041576\n",
            "[70]\tvalid_0's rmse: 0.0341314\n",
            "[80]\tvalid_0's rmse: 0.0281046\n",
            "[90]\tvalid_0's rmse: 0.0249728\n",
            "[100]\tvalid_0's rmse: 0.0249728\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.0249728\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.27703\n",
            "[20]\tvalid_0's rmse: 0.199415\n",
            "[30]\tvalid_0's rmse: 0.162278\n",
            "[40]\tvalid_0's rmse: 0.139241\n",
            "[50]\tvalid_0's rmse: 0.124001\n",
            "[60]\tvalid_0's rmse: 0.113956\n",
            "[70]\tvalid_0's rmse: 0.106885\n",
            "[80]\tvalid_0's rmse: 0.10181\n",
            "[90]\tvalid_0's rmse: 0.0978664\n",
            "[100]\tvalid_0's rmse: 0.0978664\n",
            "Early stopping, best iteration is:\n",
            "[85]\tvalid_0's rmse: 0.0978664\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.28923\n",
            "[20]\tvalid_0's rmse: 0.212794\n",
            "[30]\tvalid_0's rmse: 0.158184\n",
            "[40]\tvalid_0's rmse: 0.124275\n",
            "[50]\tvalid_0's rmse: 0.106482\n",
            "[60]\tvalid_0's rmse: 0.0946939\n",
            "[70]\tvalid_0's rmse: 0.0880749\n",
            "[80]\tvalid_0's rmse: 0.0827244\n",
            "[90]\tvalid_0's rmse: 0.0799394\n",
            "[100]\tvalid_0's rmse: 0.0799394\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.0799394\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.186335\n",
            "[20]\tvalid_0's rmse: 0.126018\n",
            "[30]\tvalid_0's rmse: 0.0758775\n",
            "[40]\tvalid_0's rmse: 0.0511161\n",
            "[50]\tvalid_0's rmse: 0.034817\n",
            "[60]\tvalid_0's rmse: 0.0230886\n",
            "[70]\tvalid_0's rmse: 0.0167212\n",
            "[80]\tvalid_0's rmse: 0.0123372\n",
            "[90]\tvalid_0's rmse: 0.0107368\n",
            "[100]\tvalid_0's rmse: 0.0107368\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0105344\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.183281\n",
            "[20]\tvalid_0's rmse: 0.127645\n",
            "[30]\tvalid_0's rmse: 0.0907933\n",
            "[40]\tvalid_0's rmse: 0.0624732\n",
            "[50]\tvalid_0's rmse: 0.0505628\n",
            "[60]\tvalid_0's rmse: 0.0418797\n",
            "[70]\tvalid_0's rmse: 0.0353536\n",
            "[80]\tvalid_0's rmse: 0.0309203\n",
            "[90]\tvalid_0's rmse: 0.025439\n",
            "[100]\tvalid_0's rmse: 0.0241039\n",
            "[110]\tvalid_0's rmse: 0.0241039\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's rmse: 0.0241039\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.206751\n",
            "[20]\tvalid_0's rmse: 0.144212\n",
            "[30]\tvalid_0's rmse: 0.102683\n",
            "[40]\tvalid_0's rmse: 0.0700772\n",
            "[50]\tvalid_0's rmse: 0.0518047\n",
            "[60]\tvalid_0's rmse: 0.040878\n",
            "[70]\tvalid_0's rmse: 0.0343628\n",
            "[80]\tvalid_0's rmse: 0.0294474\n",
            "[90]\tvalid_0's rmse: 0.025759\n",
            "[100]\tvalid_0's rmse: 0.025759\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.025759\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.229256\n",
            "[20]\tvalid_0's rmse: 0.16738\n",
            "[30]\tvalid_0's rmse: 0.121842\n",
            "[40]\tvalid_0's rmse: 0.0889127\n",
            "[50]\tvalid_0's rmse: 0.0687354\n",
            "[60]\tvalid_0's rmse: 0.0581084\n",
            "[70]\tvalid_0's rmse: 0.0518296\n",
            "[80]\tvalid_0's rmse: 0.0469875\n",
            "[90]\tvalid_0's rmse: 0.0423376\n",
            "[100]\tvalid_0's rmse: 0.0423376\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.0423376\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.113797\n",
            "[20]\tvalid_0's rmse: 0.0666982\n",
            "[30]\tvalid_0's rmse: 0.0368428\n",
            "[40]\tvalid_0's rmse: 0.0172288\n",
            "[50]\tvalid_0's rmse: 0.00467613\n",
            "[60]\tvalid_0's rmse: 0.00421988\n",
            "[70]\tvalid_0's rmse: 0.00885221\n",
            "Early stopping, best iteration is:\n",
            "[55]\tvalid_0's rmse: 0.000343323\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.10183\n",
            "[20]\tvalid_0's rmse: 0.0635351\n",
            "[30]\tvalid_0's rmse: 0.0318979\n",
            "[40]\tvalid_0's rmse: 0.0132636\n",
            "[50]\tvalid_0's rmse: 0.00125807\n",
            "[60]\tvalid_0's rmse: 0.00739549\n",
            "[70]\tvalid_0's rmse: 0.0117929\n",
            "Early stopping, best iteration is:\n",
            "[51]\tvalid_0's rmse: 7.1955e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.123986\n",
            "[20]\tvalid_0's rmse: 0.0739879\n",
            "[30]\tvalid_0's rmse: 0.0410771\n",
            "[40]\tvalid_0's rmse: 0.0173965\n",
            "[50]\tvalid_0's rmse: 0.00477214\n",
            "[60]\tvalid_0's rmse: 0.00246277\n",
            "[70]\tvalid_0's rmse: 0.00839267\n",
            "Early stopping, best iteration is:\n",
            "[55]\tvalid_0's rmse: 0.000194059\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.142158\n",
            "[20]\tvalid_0's rmse: 0.0979038\n",
            "[30]\tvalid_0's rmse: 0.0725443\n",
            "[40]\tvalid_0's rmse: 0.052256\n",
            "[50]\tvalid_0's rmse: 0.0387768\n",
            "[60]\tvalid_0's rmse: 0.0297054\n",
            "[70]\tvalid_0's rmse: 0.0234799\n",
            "[80]\tvalid_0's rmse: 0.0203097\n",
            "[90]\tvalid_0's rmse: 0.0158672\n",
            "[100]\tvalid_0's rmse: 0.0130896\n",
            "[110]\tvalid_0's rmse: 0.0132892\n",
            "Early stopping, best iteration is:\n",
            "[96]\tvalid_0's rmse: 0.0129091\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.14472\n",
            "[20]\tvalid_0's rmse: 0.10925\n",
            "[30]\tvalid_0's rmse: 0.0794427\n",
            "[40]\tvalid_0's rmse: 0.0577776\n",
            "[50]\tvalid_0's rmse: 0.0441577\n",
            "[60]\tvalid_0's rmse: 0.0344845\n",
            "[70]\tvalid_0's rmse: 0.0290037\n",
            "[80]\tvalid_0's rmse: 0.0249604\n",
            "[90]\tvalid_0's rmse: 0.0210073\n",
            "[100]\tvalid_0's rmse: 0.0210073\n",
            "Early stopping, best iteration is:\n",
            "[89]\tvalid_0's rmse: 0.0210073\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0609992\n",
            "[20]\tvalid_0's rmse: 0.0276758\n",
            "[30]\tvalid_0's rmse: 0.00635912\n",
            "[40]\tvalid_0's rmse: 0.00744389\n",
            "[50]\tvalid_0's rmse: 0.0148874\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's rmse: 0.00106901\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0601363\n",
            "[20]\tvalid_0's rmse: 0.0298807\n",
            "[30]\tvalid_0's rmse: 0.0124551\n",
            "[40]\tvalid_0's rmse: 0.0110052\n",
            "[50]\tvalid_0's rmse: 0.00209542\n",
            "[60]\tvalid_0's rmse: 0.00516273\n",
            "[70]\tvalid_0's rmse: 0.0046584\n",
            "Early stopping, best iteration is:\n",
            "[51]\tvalid_0's rmse: 0.000268019\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0172876\n",
            "[20]\tvalid_0's rmse: 0.0214313\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's rmse: 0.0111453\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.000975861\n",
            "[20]\tvalid_0's rmse: 0.00027488\n",
            "[30]\tvalid_0's rmse: 0.00100083\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's rmse: 6.94507e-06\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.502485\n",
            "[20]\tvalid_0's rmse: 0.502485\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.502485\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.511336\n",
            "[20]\tvalid_0's rmse: 0.511336\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.511336\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444358\n",
            "[20]\tvalid_0's rmse: 0.444358\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.444358\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.398313\n",
            "[20]\tvalid_0's rmse: 0.398313\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.398313\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416187\n",
            "[20]\tvalid_0's rmse: 0.416187\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.416187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.460328\n",
            "[20]\tvalid_0's rmse: 0.477079\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.427253\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444548\n",
            "[20]\tvalid_0's rmse: 0.4032\n",
            "[30]\tvalid_0's rmse: 0.367978\n",
            "[40]\tvalid_0's rmse: 0.337973\n",
            "[50]\tvalid_0's rmse: 0.312413\n",
            "[60]\tvalid_0's rmse: 0.290639\n",
            "[70]\tvalid_0's rmse: 0.27209\n",
            "[80]\tvalid_0's rmse: 0.256289\n",
            "[90]\tvalid_0's rmse: 0.249951\n",
            "[100]\tvalid_0's rmse: 0.246441\n",
            "[110]\tvalid_0's rmse: 0.243099\n",
            "[120]\tvalid_0's rmse: 0.240544\n",
            "[130]\tvalid_0's rmse: 0.240544\n",
            "Early stopping, best iteration is:\n",
            "[114]\tvalid_0's rmse: 0.240544\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.310239\n",
            "[20]\tvalid_0's rmse: 0.263204\n",
            "[30]\tvalid_0's rmse: 0.22559\n",
            "[40]\tvalid_0's rmse: 0.189951\n",
            "[50]\tvalid_0's rmse: 0.162028\n",
            "[60]\tvalid_0's rmse: 0.138131\n",
            "[70]\tvalid_0's rmse: 0.116566\n",
            "[80]\tvalid_0's rmse: 0.0971581\n",
            "[90]\tvalid_0's rmse: 0.0802153\n",
            "[100]\tvalid_0's rmse: 0.0654487\n",
            "[110]\tvalid_0's rmse: 0.0525774\n",
            "[120]\tvalid_0's rmse: 0.0413616\n",
            "[130]\tvalid_0's rmse: 0.0392943\n",
            "[140]\tvalid_0's rmse: 0.0392943\n",
            "Early stopping, best iteration is:\n",
            "[122]\tvalid_0's rmse: 0.0392943\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.424826\n",
            "[20]\tvalid_0's rmse: 0.376309\n",
            "[30]\tvalid_0's rmse: 0.338718\n",
            "[40]\tvalid_0's rmse: 0.305675\n",
            "[50]\tvalid_0's rmse: 0.27766\n",
            "[60]\tvalid_0's rmse: 0.253701\n",
            "[70]\tvalid_0's rmse: 0.232838\n",
            "[80]\tvalid_0's rmse: 0.214949\n",
            "[90]\tvalid_0's rmse: 0.199043\n",
            "[100]\tvalid_0's rmse: 0.185216\n",
            "[110]\tvalid_0's rmse: 0.172654\n",
            "[120]\tvalid_0's rmse: 0.161561\n",
            "[130]\tvalid_0's rmse: 0.155551\n",
            "[140]\tvalid_0's rmse: 0.155551\n",
            "Early stopping, best iteration is:\n",
            "[127]\tvalid_0's rmse: 0.155551\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.436084\n",
            "[20]\tvalid_0's rmse: 0.382569\n",
            "[30]\tvalid_0's rmse: 0.342478\n",
            "[40]\tvalid_0's rmse: 0.31088\n",
            "[50]\tvalid_0's rmse: 0.284333\n",
            "[60]\tvalid_0's rmse: 0.260826\n",
            "[70]\tvalid_0's rmse: 0.239798\n",
            "[80]\tvalid_0's rmse: 0.221508\n",
            "[90]\tvalid_0's rmse: 0.205549\n",
            "[100]\tvalid_0's rmse: 0.191343\n",
            "[110]\tvalid_0's rmse: 0.178569\n",
            "[120]\tvalid_0's rmse: 0.16708\n",
            "[130]\tvalid_0's rmse: 0.156741\n",
            "[140]\tvalid_0's rmse: 0.152913\n",
            "[150]\tvalid_0's rmse: 0.152913\n",
            "Early stopping, best iteration is:\n",
            "[134]\tvalid_0's rmse: 0.152913\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.271136\n",
            "[20]\tvalid_0's rmse: 0.201319\n",
            "[30]\tvalid_0's rmse: 0.152202\n",
            "[40]\tvalid_0's rmse: 0.113631\n",
            "[50]\tvalid_0's rmse: 0.0827058\n",
            "[60]\tvalid_0's rmse: 0.0613203\n",
            "[70]\tvalid_0's rmse: 0.0487766\n",
            "[80]\tvalid_0's rmse: 0.0370774\n",
            "[90]\tvalid_0's rmse: 0.0260814\n",
            "[100]\tvalid_0's rmse: 0.0174969\n",
            "[110]\tvalid_0's rmse: 0.0162133\n",
            "[120]\tvalid_0's rmse: 0.0162133\n",
            "Early stopping, best iteration is:\n",
            "[101]\tvalid_0's rmse: 0.0162133\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.367533\n",
            "[20]\tvalid_0's rmse: 0.289846\n",
            "[30]\tvalid_0's rmse: 0.234041\n",
            "[40]\tvalid_0's rmse: 0.193226\n",
            "[50]\tvalid_0's rmse: 0.165044\n",
            "[60]\tvalid_0's rmse: 0.150856\n",
            "[70]\tvalid_0's rmse: 0.138587\n",
            "[80]\tvalid_0's rmse: 0.127825\n",
            "[90]\tvalid_0's rmse: 0.117783\n",
            "[100]\tvalid_0's rmse: 0.109002\n",
            "[110]\tvalid_0's rmse: 0.10143\n",
            "[120]\tvalid_0's rmse: 0.10143\n",
            "[130]\tvalid_0's rmse: 0.10143\n",
            "Early stopping, best iteration is:\n",
            "[110]\tvalid_0's rmse: 0.10143\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.370476\n",
            "[20]\tvalid_0's rmse: 0.306671\n",
            "[30]\tvalid_0's rmse: 0.260013\n",
            "[40]\tvalid_0's rmse: 0.22424\n",
            "[50]\tvalid_0's rmse: 0.196425\n",
            "[60]\tvalid_0's rmse: 0.174387\n",
            "[70]\tvalid_0's rmse: 0.156432\n",
            "[80]\tvalid_0's rmse: 0.139455\n",
            "[90]\tvalid_0's rmse: 0.128776\n",
            "[100]\tvalid_0's rmse: 0.119053\n",
            "[110]\tvalid_0's rmse: 0.115711\n",
            "[120]\tvalid_0's rmse: 0.115711\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 0.115711\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.350506\n",
            "[20]\tvalid_0's rmse: 0.276187\n",
            "[30]\tvalid_0's rmse: 0.219007\n",
            "[40]\tvalid_0's rmse: 0.171843\n",
            "[50]\tvalid_0's rmse: 0.137668\n",
            "[60]\tvalid_0's rmse: 0.113943\n",
            "[70]\tvalid_0's rmse: 0.104236\n",
            "[80]\tvalid_0's rmse: 0.0939613\n",
            "[90]\tvalid_0's rmse: 0.0860814\n",
            "[100]\tvalid_0's rmse: 0.0817176\n",
            "[110]\tvalid_0's rmse: 0.0817176\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 0.0817176\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.275359\n",
            "[20]\tvalid_0's rmse: 0.196423\n",
            "[30]\tvalid_0's rmse: 0.141533\n",
            "[40]\tvalid_0's rmse: 0.0952696\n",
            "[50]\tvalid_0's rmse: 0.0644124\n",
            "[60]\tvalid_0's rmse: 0.0471963\n",
            "[70]\tvalid_0's rmse: 0.0374374\n",
            "[80]\tvalid_0's rmse: 0.0293052\n",
            "[90]\tvalid_0's rmse: 0.0225497\n",
            "[100]\tvalid_0's rmse: 0.0164402\n",
            "[110]\tvalid_0's rmse: 0.0164402\n",
            "Early stopping, best iteration is:\n",
            "[99]\tvalid_0's rmse: 0.0164402\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.279683\n",
            "[20]\tvalid_0's rmse: 0.212399\n",
            "[30]\tvalid_0's rmse: 0.16684\n",
            "[40]\tvalid_0's rmse: 0.129455\n",
            "[50]\tvalid_0's rmse: 0.100672\n",
            "[60]\tvalid_0's rmse: 0.0829221\n",
            "[70]\tvalid_0's rmse: 0.0734714\n",
            "[80]\tvalid_0's rmse: 0.0653891\n",
            "[90]\tvalid_0's rmse: 0.0587201\n",
            "[100]\tvalid_0's rmse: 0.0552138\n",
            "[110]\tvalid_0's rmse: 0.0552138\n",
            "Early stopping, best iteration is:\n",
            "[96]\tvalid_0's rmse: 0.0552138\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.301404\n",
            "[20]\tvalid_0's rmse: 0.226588\n",
            "[30]\tvalid_0's rmse: 0.178445\n",
            "[40]\tvalid_0's rmse: 0.145642\n",
            "[50]\tvalid_0's rmse: 0.1176\n",
            "[60]\tvalid_0's rmse: 0.097284\n",
            "[70]\tvalid_0's rmse: 0.0901465\n",
            "[80]\tvalid_0's rmse: 0.0842425\n",
            "[90]\tvalid_0's rmse: 0.0791288\n",
            "[100]\tvalid_0's rmse: 0.0771685\n",
            "[110]\tvalid_0's rmse: 0.0771685\n",
            "Early stopping, best iteration is:\n",
            "[93]\tvalid_0's rmse: 0.0771685\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.188066\n",
            "[20]\tvalid_0's rmse: 0.131547\n",
            "[30]\tvalid_0's rmse: 0.0872858\n",
            "[40]\tvalid_0's rmse: 0.0558919\n",
            "[50]\tvalid_0's rmse: 0.0346933\n",
            "[60]\tvalid_0's rmse: 0.0247689\n",
            "[70]\tvalid_0's rmse: 0.0163023\n",
            "[80]\tvalid_0's rmse: 0.00937933\n",
            "[90]\tvalid_0's rmse: 0.0076254\n",
            "[100]\tvalid_0's rmse: 0.00773952\n",
            "[110]\tvalid_0's rmse: 0.00773952\n",
            "Early stopping, best iteration is:\n",
            "[94]\tvalid_0's rmse: 0.00740255\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.178588\n",
            "[20]\tvalid_0's rmse: 0.125999\n",
            "[30]\tvalid_0's rmse: 0.0861043\n",
            "[40]\tvalid_0's rmse: 0.0587501\n",
            "[50]\tvalid_0's rmse: 0.039975\n",
            "[60]\tvalid_0's rmse: 0.029596\n",
            "[70]\tvalid_0's rmse: 0.0269145\n",
            "[80]\tvalid_0's rmse: 0.0243196\n",
            "[90]\tvalid_0's rmse: 0.0228443\n",
            "[100]\tvalid_0's rmse: 0.0219747\n",
            "[110]\tvalid_0's rmse: 0.0219747\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 0.0219747\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.218494\n",
            "[20]\tvalid_0's rmse: 0.152705\n",
            "[30]\tvalid_0's rmse: 0.105112\n",
            "[40]\tvalid_0's rmse: 0.0766153\n",
            "[50]\tvalid_0's rmse: 0.0545656\n",
            "[60]\tvalid_0's rmse: 0.037786\n",
            "[70]\tvalid_0's rmse: 0.0314551\n",
            "[80]\tvalid_0's rmse: 0.0262581\n",
            "[90]\tvalid_0's rmse: 0.0213592\n",
            "[100]\tvalid_0's rmse: 0.0202213\n",
            "[110]\tvalid_0's rmse: 0.0202213\n",
            "Early stopping, best iteration is:\n",
            "[93]\tvalid_0's rmse: 0.0202213\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.21326\n",
            "[20]\tvalid_0's rmse: 0.150902\n",
            "[30]\tvalid_0's rmse: 0.106763\n",
            "[40]\tvalid_0's rmse: 0.0775905\n",
            "[50]\tvalid_0's rmse: 0.0563106\n",
            "[60]\tvalid_0's rmse: 0.0429439\n",
            "[70]\tvalid_0's rmse: 0.0358382\n",
            "[80]\tvalid_0's rmse: 0.0301198\n",
            "[90]\tvalid_0's rmse: 0.0250575\n",
            "[100]\tvalid_0's rmse: 0.0219542\n",
            "[110]\tvalid_0's rmse: 0.0219542\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.0219542\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.168559\n",
            "[20]\tvalid_0's rmse: 0.113555\n",
            "[30]\tvalid_0's rmse: 0.0746953\n",
            "[40]\tvalid_0's rmse: 0.0494516\n",
            "[50]\tvalid_0's rmse: 0.0321499\n",
            "[60]\tvalid_0's rmse: 0.0221302\n",
            "[70]\tvalid_0's rmse: 0.0141549\n",
            "[80]\tvalid_0's rmse: 0.00760146\n",
            "[90]\tvalid_0's rmse: 0.00218681\n",
            "[100]\tvalid_0's rmse: 0.000648415\n",
            "[110]\tvalid_0's rmse: 0.000648415\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 6.68621e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.227933\n",
            "[20]\tvalid_0's rmse: 0.168671\n",
            "[30]\tvalid_0's rmse: 0.128462\n",
            "[40]\tvalid_0's rmse: 0.10089\n",
            "[50]\tvalid_0's rmse: 0.0816028\n",
            "[60]\tvalid_0's rmse: 0.0671485\n",
            "[70]\tvalid_0's rmse: 0.0612383\n",
            "[80]\tvalid_0's rmse: 0.0561492\n",
            "[90]\tvalid_0's rmse: 0.0516348\n",
            "[100]\tvalid_0's rmse: 0.0491086\n",
            "[110]\tvalid_0's rmse: 0.0491086\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.0491086\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.238093\n",
            "[20]\tvalid_0's rmse: 0.180264\n",
            "[30]\tvalid_0's rmse: 0.139868\n",
            "[40]\tvalid_0's rmse: 0.113133\n",
            "[50]\tvalid_0's rmse: 0.093519\n",
            "[60]\tvalid_0's rmse: 0.0781693\n",
            "[70]\tvalid_0's rmse: 0.0717669\n",
            "[80]\tvalid_0's rmse: 0.0660309\n",
            "[90]\tvalid_0's rmse: 0.0612506\n",
            "[100]\tvalid_0's rmse: 0.0584145\n",
            "[110]\tvalid_0's rmse: 0.0584145\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.0584145\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.16811\n",
            "[20]\tvalid_0's rmse: 0.156381\n",
            "[30]\tvalid_0's rmse: 0.116644\n",
            "[40]\tvalid_0's rmse: 0.0890888\n",
            "[50]\tvalid_0's rmse: 0.0684284\n",
            "[60]\tvalid_0's rmse: 0.0541404\n",
            "[70]\tvalid_0's rmse: 0.0464251\n",
            "[80]\tvalid_0's rmse: 0.0394729\n",
            "[90]\tvalid_0's rmse: 0.0337178\n",
            "[100]\tvalid_0's rmse: 0.0284874\n",
            "[110]\tvalid_0's rmse: 0.0272437\n",
            "[120]\tvalid_0's rmse: 0.0272437\n",
            "Early stopping, best iteration is:\n",
            "[102]\tvalid_0's rmse: 0.0272437\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.156469\n",
            "[20]\tvalid_0's rmse: 0.119953\n",
            "[30]\tvalid_0's rmse: 0.0834518\n",
            "[40]\tvalid_0's rmse: 0.0590579\n",
            "[50]\tvalid_0's rmse: 0.0404778\n",
            "[60]\tvalid_0's rmse: 0.0301939\n",
            "[70]\tvalid_0's rmse: 0.0241317\n",
            "[80]\tvalid_0's rmse: 0.019351\n",
            "[90]\tvalid_0's rmse: 0.015452\n",
            "[100]\tvalid_0's rmse: 0.012465\n",
            "[110]\tvalid_0's rmse: 0.012465\n",
            "Early stopping, best iteration is:\n",
            "[99]\tvalid_0's rmse: 0.012465\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.183431\n",
            "[20]\tvalid_0's rmse: 0.132614\n",
            "[30]\tvalid_0's rmse: 0.0984828\n",
            "[40]\tvalid_0's rmse: 0.0748727\n",
            "[50]\tvalid_0's rmse: 0.0572017\n",
            "[60]\tvalid_0's rmse: 0.0447624\n",
            "[70]\tvalid_0's rmse: 0.03837\n",
            "[80]\tvalid_0's rmse: 0.0330254\n",
            "[90]\tvalid_0's rmse: 0.0286685\n",
            "[100]\tvalid_0's rmse: 0.0250386\n",
            "[110]\tvalid_0's rmse: 0.0247624\n",
            "[120]\tvalid_0's rmse: 0.0247624\n",
            "Early stopping, best iteration is:\n",
            "[101]\tvalid_0's rmse: 0.0247624\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.31351\n",
            "[20]\tvalid_0's rmse: 0.265451\n",
            "[30]\tvalid_0's rmse: 0.228818\n",
            "[40]\tvalid_0's rmse: 0.20802\n",
            "[50]\tvalid_0's rmse: 0.18894\n",
            "[60]\tvalid_0's rmse: 0.178377\n",
            "[70]\tvalid_0's rmse: 0.172004\n",
            "[80]\tvalid_0's rmse: 0.166877\n",
            "[90]\tvalid_0's rmse: 0.162629\n",
            "[100]\tvalid_0's rmse: 0.157169\n",
            "[110]\tvalid_0's rmse: 0.157169\n",
            "Early stopping, best iteration is:\n",
            "[99]\tvalid_0's rmse: 0.157169\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.313939\n",
            "[20]\tvalid_0's rmse: 0.258319\n",
            "[30]\tvalid_0's rmse: 0.213144\n",
            "[40]\tvalid_0's rmse: 0.178014\n",
            "[50]\tvalid_0's rmse: 0.158014\n",
            "[60]\tvalid_0's rmse: 0.138553\n",
            "[70]\tvalid_0's rmse: 0.129075\n",
            "[80]\tvalid_0's rmse: 0.124588\n",
            "[90]\tvalid_0's rmse: 0.119747\n",
            "[100]\tvalid_0's rmse: 0.113011\n",
            "[110]\tvalid_0's rmse: 0.113011\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's rmse: 0.113011\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.193356\n",
            "[20]\tvalid_0's rmse: 0.136504\n",
            "[30]\tvalid_0's rmse: 0.0904204\n",
            "[40]\tvalid_0's rmse: 0.0578881\n",
            "[50]\tvalid_0's rmse: 0.0440761\n",
            "[60]\tvalid_0's rmse: 0.028454\n",
            "[70]\tvalid_0's rmse: 0.0223512\n",
            "[80]\tvalid_0's rmse: 0.018216\n",
            "[90]\tvalid_0's rmse: 0.0147447\n",
            "[100]\tvalid_0's rmse: 0.0142\n",
            "[110]\tvalid_0's rmse: 0.0142\n",
            "Early stopping, best iteration is:\n",
            "[92]\tvalid_0's rmse: 0.0142\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.193043\n",
            "[20]\tvalid_0's rmse: 0.131948\n",
            "[30]\tvalid_0's rmse: 0.0874411\n",
            "[40]\tvalid_0's rmse: 0.0593589\n",
            "[50]\tvalid_0's rmse: 0.0479793\n",
            "[60]\tvalid_0's rmse: 0.0337021\n",
            "[70]\tvalid_0's rmse: 0.0275156\n",
            "[80]\tvalid_0's rmse: 0.0232578\n",
            "[90]\tvalid_0's rmse: 0.0193942\n",
            "[100]\tvalid_0's rmse: 0.0170109\n",
            "[110]\tvalid_0's rmse: 0.015683\n",
            "[120]\tvalid_0's rmse: 0.01495\n",
            "[130]\tvalid_0's rmse: 0.0145989\n",
            "[140]\tvalid_0's rmse: 0.0145989\n",
            "Early stopping, best iteration is:\n",
            "[127]\tvalid_0's rmse: 0.0145989\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.20753\n",
            "[20]\tvalid_0's rmse: 0.145598\n",
            "[30]\tvalid_0's rmse: 0.102518\n",
            "[40]\tvalid_0's rmse: 0.074359\n",
            "[50]\tvalid_0's rmse: 0.0601387\n",
            "[60]\tvalid_0's rmse: 0.0483885\n",
            "[70]\tvalid_0's rmse: 0.0430134\n",
            "[80]\tvalid_0's rmse: 0.0391229\n",
            "[90]\tvalid_0's rmse: 0.0358801\n",
            "[100]\tvalid_0's rmse: 0.0344354\n",
            "[110]\tvalid_0's rmse: 0.0344354\n",
            "Early stopping, best iteration is:\n",
            "[92]\tvalid_0's rmse: 0.0341493\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.237928\n",
            "[20]\tvalid_0's rmse: 0.175631\n",
            "[30]\tvalid_0's rmse: 0.131441\n",
            "[40]\tvalid_0's rmse: 0.102546\n",
            "[50]\tvalid_0's rmse: 0.0849804\n",
            "[60]\tvalid_0's rmse: 0.0712348\n",
            "[70]\tvalid_0's rmse: 0.0623425\n",
            "[80]\tvalid_0's rmse: 0.0585783\n",
            "[90]\tvalid_0's rmse: 0.0548657\n",
            "[100]\tvalid_0's rmse: 0.0536529\n",
            "[110]\tvalid_0's rmse: 0.0536529\n",
            "Early stopping, best iteration is:\n",
            "[92]\tvalid_0's rmse: 0.0536529\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0931548\n",
            "[20]\tvalid_0's rmse: 0.0506403\n",
            "[30]\tvalid_0's rmse: 0.0229322\n",
            "[40]\tvalid_0's rmse: 0.00661432\n",
            "[50]\tvalid_0's rmse: 0.00394023\n",
            "[60]\tvalid_0's rmse: 0.0144937\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's rmse: 6.22023e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0961404\n",
            "[20]\tvalid_0's rmse: 0.0527376\n",
            "[30]\tvalid_0's rmse: 0.0229156\n",
            "[40]\tvalid_0's rmse: 0.00439414\n",
            "[50]\tvalid_0's rmse: 0.00987234\n",
            "[60]\tvalid_0's rmse: 0.015785\n",
            "Early stopping, best iteration is:\n",
            "[43]\tvalid_0's rmse: 0.000135271\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.113514\n",
            "[20]\tvalid_0's rmse: 0.0654295\n",
            "[30]\tvalid_0's rmse: 0.0279359\n",
            "[40]\tvalid_0's rmse: 0.00595764\n",
            "[50]\tvalid_0's rmse: 0.00690952\n",
            "[60]\tvalid_0's rmse: 0.0138768\n",
            "Early stopping, best iteration is:\n",
            "[43]\tvalid_0's rmse: 0.000305813\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.136326\n",
            "[20]\tvalid_0's rmse: 0.0951594\n",
            "[30]\tvalid_0's rmse: 0.0677535\n",
            "[40]\tvalid_0's rmse: 0.0431222\n",
            "[50]\tvalid_0's rmse: 0.027012\n",
            "[60]\tvalid_0's rmse: 0.0151957\n",
            "[70]\tvalid_0's rmse: 0.00943755\n",
            "[80]\tvalid_0's rmse: 0.00689737\n",
            "[90]\tvalid_0's rmse: 0.00781323\n",
            "[100]\tvalid_0's rmse: 0.00476736\n",
            "[110]\tvalid_0's rmse: 0.00655944\n",
            "[120]\tvalid_0's rmse: 0.00877182\n",
            "Early stopping, best iteration is:\n",
            "[102]\tvalid_0's rmse: 0.00445985\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.139857\n",
            "[20]\tvalid_0's rmse: 0.105255\n",
            "[30]\tvalid_0's rmse: 0.0741946\n",
            "[40]\tvalid_0's rmse: 0.0518861\n",
            "[50]\tvalid_0's rmse: 0.0385434\n",
            "[60]\tvalid_0's rmse: 0.0274875\n",
            "[70]\tvalid_0's rmse: 0.0215505\n",
            "[80]\tvalid_0's rmse: 0.0180463\n",
            "[90]\tvalid_0's rmse: 0.01364\n",
            "[100]\tvalid_0's rmse: 0.0122763\n",
            "[110]\tvalid_0's rmse: 0.0122763\n",
            "Early stopping, best iteration is:\n",
            "[93]\tvalid_0's rmse: 0.0122763\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0605137\n",
            "[20]\tvalid_0's rmse: 0.0289848\n",
            "[30]\tvalid_0's rmse: 0.0101914\n",
            "[40]\tvalid_0's rmse: 0.00355696\n",
            "[50]\tvalid_0's rmse: 0.0123145\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's rmse: 0.000140609\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0798492\n",
            "[20]\tvalid_0's rmse: 0.0509768\n",
            "[30]\tvalid_0's rmse: 0.0332119\n",
            "[40]\tvalid_0's rmse: 0.0254731\n",
            "[50]\tvalid_0's rmse: 0.0155354\n",
            "[60]\tvalid_0's rmse: 0.0108655\n",
            "[70]\tvalid_0's rmse: 0.00593026\n",
            "[80]\tvalid_0's rmse: 0.00259353\n",
            "[90]\tvalid_0's rmse: 0.000512185\n",
            "[100]\tvalid_0's rmse: 0.00292926\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.000134187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.023592\n",
            "[20]\tvalid_0's rmse: 0.0253254\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.0102227\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00184881\n",
            "[20]\tvalid_0's rmse: 0.00308513\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's rmse: 0.000232387\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.502485\n",
            "[20]\tvalid_0's rmse: 0.502485\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.502485\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.511336\n",
            "[20]\tvalid_0's rmse: 0.511336\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.511336\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444358\n",
            "[20]\tvalid_0's rmse: 0.444358\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.444358\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.398313\n",
            "[20]\tvalid_0's rmse: 0.398313\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.398313\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416187\n",
            "[20]\tvalid_0's rmse: 0.416187\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.416187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.42288\n",
            "[20]\tvalid_0's rmse: 0.42288\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.42288\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.496021\n",
            "[20]\tvalid_0's rmse: 0.496021\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.496021\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.374953\n",
            "[20]\tvalid_0's rmse: 0.374953\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.374953\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.490488\n",
            "[20]\tvalid_0's rmse: 0.490488\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.490488\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.505679\n",
            "[20]\tvalid_0's rmse: 0.505679\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.505679\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.367176\n",
            "[20]\tvalid_0's rmse: 0.367176\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.367176\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.464969\n",
            "[20]\tvalid_0's rmse: 0.464969\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.464969\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.461365\n",
            "[20]\tvalid_0's rmse: 0.461365\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.461365\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.441812\n",
            "[20]\tvalid_0's rmse: 0.441812\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.441812\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.365738\n",
            "[20]\tvalid_0's rmse: 0.365738\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.365738\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.308793\n",
            "[20]\tvalid_0's rmse: 0.248726\n",
            "[30]\tvalid_0's rmse: 0.202049\n",
            "[40]\tvalid_0's rmse: 0.165184\n",
            "[50]\tvalid_0's rmse: 0.13569\n",
            "[60]\tvalid_0's rmse: 0.112114\n",
            "[70]\tvalid_0's rmse: 0.0929223\n",
            "[80]\tvalid_0's rmse: 0.0771057\n",
            "[90]\tvalid_0's rmse: 0.063995\n",
            "[100]\tvalid_0's rmse: 0.0616369\n",
            "[110]\tvalid_0's rmse: 0.0594877\n",
            "[120]\tvalid_0's rmse: 0.0594749\n",
            "[130]\tvalid_0's rmse: 0.0575984\n",
            "[140]\tvalid_0's rmse: 0.0575877\n",
            "[150]\tvalid_0's rmse: 0.055911\n",
            "[160]\tvalid_0's rmse: 0.0551262\n",
            "Early stopping, best iteration is:\n",
            "[149]\tvalid_0's rmse: 0.0551177\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.325975\n",
            "[20]\tvalid_0's rmse: 0.265487\n",
            "[30]\tvalid_0's rmse: 0.221083\n",
            "[40]\tvalid_0's rmse: 0.18547\n",
            "[50]\tvalid_0's rmse: 0.156827\n",
            "[60]\tvalid_0's rmse: 0.133843\n",
            "[70]\tvalid_0's rmse: 0.115081\n",
            "[80]\tvalid_0's rmse: 0.108732\n",
            "[90]\tvalid_0's rmse: 0.108879\n",
            "[100]\tvalid_0's rmse: 0.111549\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.108714\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.1653\n",
            "[20]\tvalid_0's rmse: 0.106772\n",
            "[30]\tvalid_0's rmse: 0.0888746\n",
            "[40]\tvalid_0's rmse: 0.0754619\n",
            "[50]\tvalid_0's rmse: 0.0583175\n",
            "[60]\tvalid_0's rmse: 0.0447041\n",
            "[70]\tvalid_0's rmse: 0.0371782\n",
            "[80]\tvalid_0's rmse: 0.0378794\n",
            "[90]\tvalid_0's rmse: 0.0383332\n",
            "[100]\tvalid_0's rmse: 0.0362187\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.0346957\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.153506\n",
            "[20]\tvalid_0's rmse: 0.10009\n",
            "[30]\tvalid_0's rmse: 0.0604565\n",
            "[40]\tvalid_0's rmse: 0.0302806\n",
            "[50]\tvalid_0's rmse: 0.0165342\n",
            "[60]\tvalid_0's rmse: 0.0101459\n",
            "[70]\tvalid_0's rmse: 0.00892348\n",
            "[80]\tvalid_0's rmse: 0.014169\n",
            "Early stopping, best iteration is:\n",
            "[62]\tvalid_0's rmse: 0.00674689\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.239372\n",
            "[20]\tvalid_0's rmse: 0.186548\n",
            "[30]\tvalid_0's rmse: 0.146235\n",
            "[40]\tvalid_0's rmse: 0.116103\n",
            "[50]\tvalid_0's rmse: 0.0972231\n",
            "[60]\tvalid_0's rmse: 0.0912105\n",
            "[70]\tvalid_0's rmse: 0.0891752\n",
            "[80]\tvalid_0's rmse: 0.0872651\n",
            "[90]\tvalid_0's rmse: 0.0857785\n",
            "[100]\tvalid_0's rmse: 0.0819948\n",
            "[110]\tvalid_0's rmse: 0.0779774\n",
            "[120]\tvalid_0's rmse: 0.0741985\n",
            "[130]\tvalid_0's rmse: 0.0705691\n",
            "[140]\tvalid_0's rmse: 0.0672697\n",
            "[150]\tvalid_0's rmse: 0.0641871\n",
            "[160]\tvalid_0's rmse: 0.0612811\n",
            "[170]\tvalid_0's rmse: 0.05936\n",
            "[180]\tvalid_0's rmse: 0.05936\n",
            "Early stopping, best iteration is:\n",
            "[167]\tvalid_0's rmse: 0.0591639\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.230082\n",
            "[20]\tvalid_0's rmse: 0.178364\n",
            "[30]\tvalid_0's rmse: 0.139513\n",
            "[40]\tvalid_0's rmse: 0.109598\n",
            "[50]\tvalid_0's rmse: 0.0955988\n",
            "[60]\tvalid_0's rmse: 0.0925107\n",
            "[70]\tvalid_0's rmse: 0.0863601\n",
            "[80]\tvalid_0's rmse: 0.0841443\n",
            "[90]\tvalid_0's rmse: 0.0825511\n",
            "[100]\tvalid_0's rmse: 0.0781857\n",
            "[110]\tvalid_0's rmse: 0.0744301\n",
            "[120]\tvalid_0's rmse: 0.0731468\n",
            "[130]\tvalid_0's rmse: 0.0674265\n",
            "[140]\tvalid_0's rmse: 0.0664886\n",
            "[150]\tvalid_0's rmse: 0.0630246\n",
            "[160]\tvalid_0's rmse: 0.0603736\n",
            "[170]\tvalid_0's rmse: 0.0578545\n",
            "[180]\tvalid_0's rmse: 0.0565312\n",
            "[190]\tvalid_0's rmse: 0.0565312\n",
            "Early stopping, best iteration is:\n",
            "[176]\tvalid_0's rmse: 0.0565312\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.18189\n",
            "[20]\tvalid_0's rmse: 0.133574\n",
            "[30]\tvalid_0's rmse: 0.0979017\n",
            "[40]\tvalid_0's rmse: 0.0712532\n",
            "[50]\tvalid_0's rmse: 0.0507129\n",
            "[60]\tvalid_0's rmse: 0.0343927\n",
            "[70]\tvalid_0's rmse: 0.0236005\n",
            "[80]\tvalid_0's rmse: 0.0138399\n",
            "[90]\tvalid_0's rmse: 0.00887546\n",
            "[100]\tvalid_0's rmse: 0.00481816\n",
            "[110]\tvalid_0's rmse: 0.00340337\n",
            "[120]\tvalid_0's rmse: 0.00411004\n",
            "[130]\tvalid_0's rmse: 0.000794599\n",
            "[140]\tvalid_0's rmse: 9.95386e-05\n",
            "[150]\tvalid_0's rmse: 0.00218745\n",
            "[160]\tvalid_0's rmse: 0.000272484\n",
            "Early stopping, best iteration is:\n",
            "[140]\tvalid_0's rmse: 9.95386e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.245194\n",
            "[20]\tvalid_0's rmse: 0.196166\n",
            "[30]\tvalid_0's rmse: 0.161422\n",
            "[40]\tvalid_0's rmse: 0.134681\n",
            "[50]\tvalid_0's rmse: 0.113276\n",
            "[60]\tvalid_0's rmse: 0.0959355\n",
            "[70]\tvalid_0's rmse: 0.0812265\n",
            "[80]\tvalid_0's rmse: 0.0680359\n",
            "[90]\tvalid_0's rmse: 0.0630305\n",
            "[100]\tvalid_0's rmse: 0.0607924\n",
            "[110]\tvalid_0's rmse: 0.0571914\n",
            "[120]\tvalid_0's rmse: 0.0556642\n",
            "[130]\tvalid_0's rmse: 0.0539286\n",
            "[140]\tvalid_0's rmse: 0.0502518\n",
            "[150]\tvalid_0's rmse: 0.0487292\n",
            "[160]\tvalid_0's rmse: 0.0475619\n",
            "[170]\tvalid_0's rmse: 0.0459741\n",
            "[180]\tvalid_0's rmse: 0.0448454\n",
            "[190]\tvalid_0's rmse: 0.0448454\n",
            "Early stopping, best iteration is:\n",
            "[175]\tvalid_0's rmse: 0.0441291\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.258523\n",
            "[20]\tvalid_0's rmse: 0.211556\n",
            "[30]\tvalid_0's rmse: 0.177273\n",
            "[40]\tvalid_0's rmse: 0.150448\n",
            "[50]\tvalid_0's rmse: 0.128231\n",
            "[60]\tvalid_0's rmse: 0.11067\n",
            "[70]\tvalid_0's rmse: 0.095752\n",
            "[80]\tvalid_0's rmse: 0.0818558\n",
            "[90]\tvalid_0's rmse: 0.0797663\n",
            "[100]\tvalid_0's rmse: 0.0774648\n",
            "[110]\tvalid_0's rmse: 0.0753141\n",
            "[120]\tvalid_0's rmse: 0.0735491\n",
            "[130]\tvalid_0's rmse: 0.07011\n",
            "[140]\tvalid_0's rmse: 0.068668\n",
            "[150]\tvalid_0's rmse: 0.0674305\n",
            "[160]\tvalid_0's rmse: 0.064464\n",
            "[170]\tvalid_0's rmse: 0.0633097\n",
            "[180]\tvalid_0's rmse: 0.0622448\n",
            "[190]\tvalid_0's rmse: 0.0617065\n",
            "[200]\tvalid_0's rmse: 0.0617065\n",
            "Early stopping, best iteration is:\n",
            "[181]\tvalid_0's rmse: 0.0616193\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.17526\n",
            "[20]\tvalid_0's rmse: 0.12723\n",
            "[30]\tvalid_0's rmse: 0.0914791\n",
            "[40]\tvalid_0's rmse: 0.065375\n",
            "[50]\tvalid_0's rmse: 0.0457643\n",
            "[60]\tvalid_0's rmse: 0.0299114\n",
            "[70]\tvalid_0's rmse: 0.0162203\n",
            "[80]\tvalid_0's rmse: 0.00508201\n",
            "[90]\tvalid_0's rmse: 0.00217207\n",
            "[100]\tvalid_0's rmse: 0.00422138\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.000246316\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.167099\n",
            "[20]\tvalid_0's rmse: 0.121757\n",
            "[30]\tvalid_0's rmse: 0.0878114\n",
            "[40]\tvalid_0's rmse: 0.0694011\n",
            "[50]\tvalid_0's rmse: 0.0505514\n",
            "[60]\tvalid_0's rmse: 0.0365063\n",
            "[70]\tvalid_0's rmse: 0.0309274\n",
            "[80]\tvalid_0's rmse: 0.0289987\n",
            "[90]\tvalid_0's rmse: 0.0285012\n",
            "[100]\tvalid_0's rmse: 0.0254191\n",
            "[110]\tvalid_0's rmse: 0.0247976\n",
            "[120]\tvalid_0's rmse: 0.0245461\n",
            "[130]\tvalid_0's rmse: 0.0217224\n",
            "[140]\tvalid_0's rmse: 0.0196119\n",
            "[150]\tvalid_0's rmse: 0.0190907\n",
            "[160]\tvalid_0's rmse: 0.0180603\n",
            "[170]\tvalid_0's rmse: 0.0155866\n",
            "[180]\tvalid_0's rmse: 0.0146938\n",
            "[190]\tvalid_0's rmse: 0.01476\n",
            "[200]\tvalid_0's rmse: 0.01476\n",
            "Early stopping, best iteration is:\n",
            "[182]\tvalid_0's rmse: 0.0138521\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.195977\n",
            "[20]\tvalid_0's rmse: 0.150849\n",
            "[30]\tvalid_0's rmse: 0.11675\n",
            "[40]\tvalid_0's rmse: 0.0889161\n",
            "[50]\tvalid_0's rmse: 0.0697593\n",
            "[60]\tvalid_0's rmse: 0.0558909\n",
            "[70]\tvalid_0's rmse: 0.0534435\n",
            "[80]\tvalid_0's rmse: 0.0503554\n",
            "[90]\tvalid_0's rmse: 0.0535525\n",
            "Early stopping, best iteration is:\n",
            "[78]\tvalid_0's rmse: 0.0498981\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.327752\n",
            "[20]\tvalid_0's rmse: 0.285444\n",
            "[30]\tvalid_0's rmse: 0.247996\n",
            "[40]\tvalid_0's rmse: 0.220982\n",
            "[50]\tvalid_0's rmse: 0.20557\n",
            "[60]\tvalid_0's rmse: 0.193591\n",
            "[70]\tvalid_0's rmse: 0.183372\n",
            "[80]\tvalid_0's rmse: 0.174981\n",
            "[90]\tvalid_0's rmse: 0.168116\n",
            "[100]\tvalid_0's rmse: 0.167545\n",
            "[110]\tvalid_0's rmse: 0.165109\n",
            "[120]\tvalid_0's rmse: 0.164313\n",
            "[130]\tvalid_0's rmse: 0.162589\n",
            "[140]\tvalid_0's rmse: 0.16038\n",
            "[150]\tvalid_0's rmse: 0.159618\n",
            "[160]\tvalid_0's rmse: 0.159618\n",
            "Early stopping, best iteration is:\n",
            "[141]\tvalid_0's rmse: 0.159618\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.33402\n",
            "[20]\tvalid_0's rmse: 0.289693\n",
            "[30]\tvalid_0's rmse: 0.256518\n",
            "[40]\tvalid_0's rmse: 0.226242\n",
            "[50]\tvalid_0's rmse: 0.207276\n",
            "[60]\tvalid_0's rmse: 0.195206\n",
            "[70]\tvalid_0's rmse: 0.185319\n",
            "[80]\tvalid_0's rmse: 0.173823\n",
            "[90]\tvalid_0's rmse: 0.171367\n",
            "[100]\tvalid_0's rmse: 0.165786\n",
            "[110]\tvalid_0's rmse: 0.164384\n",
            "[120]\tvalid_0's rmse: 0.162976\n",
            "[130]\tvalid_0's rmse: 0.162251\n",
            "[140]\tvalid_0's rmse: 0.157998\n",
            "[150]\tvalid_0's rmse: 0.158757\n",
            "[160]\tvalid_0's rmse: 0.158757\n",
            "Early stopping, best iteration is:\n",
            "[140]\tvalid_0's rmse: 0.157998\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.208135\n",
            "[20]\tvalid_0's rmse: 0.165098\n",
            "[30]\tvalid_0's rmse: 0.127933\n",
            "[40]\tvalid_0's rmse: 0.104377\n",
            "[50]\tvalid_0's rmse: 0.0886746\n",
            "[60]\tvalid_0's rmse: 0.077514\n",
            "[70]\tvalid_0's rmse: 0.067178\n",
            "[80]\tvalid_0's rmse: 0.0566111\n",
            "[90]\tvalid_0's rmse: 0.0512625\n",
            "[100]\tvalid_0's rmse: 0.0496326\n",
            "[110]\tvalid_0's rmse: 0.0450454\n",
            "[120]\tvalid_0's rmse: 0.0419878\n",
            "[130]\tvalid_0's rmse: 0.0403572\n",
            "[140]\tvalid_0's rmse: 0.0400535\n",
            "[150]\tvalid_0's rmse: 0.0400535\n",
            "Early stopping, best iteration is:\n",
            "[132]\tvalid_0's rmse: 0.0389372\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.209294\n",
            "[20]\tvalid_0's rmse: 0.161362\n",
            "[30]\tvalid_0's rmse: 0.124687\n",
            "[40]\tvalid_0's rmse: 0.10104\n",
            "[50]\tvalid_0's rmse: 0.0850303\n",
            "[60]\tvalid_0's rmse: 0.0734829\n",
            "[70]\tvalid_0's rmse: 0.0669063\n",
            "[80]\tvalid_0's rmse: 0.0592194\n",
            "[90]\tvalid_0's rmse: 0.054467\n",
            "[100]\tvalid_0's rmse: 0.0474307\n",
            "[110]\tvalid_0's rmse: 0.0425794\n",
            "[120]\tvalid_0's rmse: 0.0389097\n",
            "[130]\tvalid_0's rmse: 0.0374577\n",
            "[140]\tvalid_0's rmse: 0.030782\n",
            "[150]\tvalid_0's rmse: 0.029906\n",
            "[160]\tvalid_0's rmse: 0.029906\n",
            "Early stopping, best iteration is:\n",
            "[142]\tvalid_0's rmse: 0.029906\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.225739\n",
            "[20]\tvalid_0's rmse: 0.175194\n",
            "[30]\tvalid_0's rmse: 0.139207\n",
            "[40]\tvalid_0's rmse: 0.109516\n",
            "[50]\tvalid_0's rmse: 0.0884628\n",
            "[60]\tvalid_0's rmse: 0.0737978\n",
            "[70]\tvalid_0's rmse: 0.0619416\n",
            "[80]\tvalid_0's rmse: 0.0532102\n",
            "[90]\tvalid_0's rmse: 0.0456732\n",
            "[100]\tvalid_0's rmse: 0.0384726\n",
            "[110]\tvalid_0's rmse: 0.0314205\n",
            "[120]\tvalid_0's rmse: 0.029503\n",
            "[130]\tvalid_0's rmse: 0.0244858\n",
            "[140]\tvalid_0's rmse: 0.0239454\n",
            "[150]\tvalid_0's rmse: 0.0239454\n",
            "Early stopping, best iteration is:\n",
            "[131]\tvalid_0's rmse: 0.0237338\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.250214\n",
            "[20]\tvalid_0's rmse: 0.197154\n",
            "[30]\tvalid_0's rmse: 0.156339\n",
            "[40]\tvalid_0's rmse: 0.126086\n",
            "[50]\tvalid_0's rmse: 0.104199\n",
            "[60]\tvalid_0's rmse: 0.0916572\n",
            "[70]\tvalid_0's rmse: 0.0830578\n",
            "[80]\tvalid_0's rmse: 0.0760037\n",
            "[90]\tvalid_0's rmse: 0.0677326\n",
            "[100]\tvalid_0's rmse: 0.0605636\n",
            "[110]\tvalid_0's rmse: 0.0539038\n",
            "[120]\tvalid_0's rmse: 0.047382\n",
            "[130]\tvalid_0's rmse: 0.0432249\n",
            "[140]\tvalid_0's rmse: 0.0416482\n",
            "[150]\tvalid_0's rmse: 0.0416482\n",
            "Early stopping, best iteration is:\n",
            "[133]\tvalid_0's rmse: 0.0414172\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0925952\n",
            "[20]\tvalid_0's rmse: 0.0414438\n",
            "[30]\tvalid_0's rmse: 0.00488395\n",
            "[40]\tvalid_0's rmse: 0.016413\n",
            "[50]\tvalid_0's rmse: 0.0308892\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's rmse: 0.00153999\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0858186\n",
            "[20]\tvalid_0's rmse: 0.0422722\n",
            "[30]\tvalid_0's rmse: 0.0117482\n",
            "[40]\tvalid_0's rmse: 0.00728159\n",
            "[50]\tvalid_0's rmse: 0.0201068\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's rmse: 0.000105777\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.124151\n",
            "[20]\tvalid_0's rmse: 0.0742574\n",
            "[30]\tvalid_0's rmse: 0.0497964\n",
            "[40]\tvalid_0's rmse: 0.0313466\n",
            "[50]\tvalid_0's rmse: 0.0183552\n",
            "[60]\tvalid_0's rmse: 0.0115046\n",
            "[70]\tvalid_0's rmse: 0.0115622\n",
            "[80]\tvalid_0's rmse: 0.0130629\n",
            "Early stopping, best iteration is:\n",
            "[65]\tvalid_0's rmse: 0.0103338\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.131453\n",
            "[20]\tvalid_0's rmse: 0.0841012\n",
            "[30]\tvalid_0's rmse: 0.0562249\n",
            "[40]\tvalid_0's rmse: 0.0326847\n",
            "[50]\tvalid_0's rmse: 0.023652\n",
            "[60]\tvalid_0's rmse: 0.0204157\n",
            "[70]\tvalid_0's rmse: 0.0215793\n",
            "Early stopping, best iteration is:\n",
            "[55]\tvalid_0's rmse: 0.0145678\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.138008\n",
            "[20]\tvalid_0's rmse: 0.0877501\n",
            "[30]\tvalid_0's rmse: 0.0523011\n",
            "[40]\tvalid_0's rmse: 0.0332908\n",
            "[50]\tvalid_0's rmse: 0.0213392\n",
            "[60]\tvalid_0's rmse: 0.0165198\n",
            "[70]\tvalid_0's rmse: 0.0118001\n",
            "[80]\tvalid_0's rmse: 0.0100147\n",
            "[90]\tvalid_0's rmse: 0.0117403\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 0.00985775\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0360763\n",
            "[20]\tvalid_0's rmse: 0.0052312\n",
            "[30]\tvalid_0's rmse: 0.0246102\n",
            "Early stopping, best iteration is:\n",
            "[19]\tvalid_0's rmse: 0.000846107\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0605901\n",
            "[20]\tvalid_0's rmse: 0.0227361\n",
            "[30]\tvalid_0's rmse: 0.00313601\n",
            "[40]\tvalid_0's rmse: 0.0137702\n",
            "[50]\tvalid_0's rmse: 0.0207396\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's rmse: 8.88279e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0284392\n",
            "[20]\tvalid_0's rmse: 0.039742\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.0131274\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00789437\n",
            "[20]\tvalid_0's rmse: 0.0165157\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.000870445\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.334069\n",
            "[20]\tvalid_0's rmse: 0.232659\n",
            "[30]\tvalid_0's rmse: 0.175252\n",
            "[40]\tvalid_0's rmse: 0.124121\n",
            "[50]\tvalid_0's rmse: 0.0933332\n",
            "[60]\tvalid_0's rmse: 0.0878361\n",
            "[70]\tvalid_0's rmse: 0.0878361\n",
            "Early stopping, best iteration is:\n",
            "[53]\tvalid_0's rmse: 0.0878361\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.339953\n",
            "[20]\tvalid_0's rmse: 0.234151\n",
            "[30]\tvalid_0's rmse: 0.16673\n",
            "[40]\tvalid_0's rmse: 0.12545\n",
            "[50]\tvalid_0's rmse: 0.101833\n",
            "[60]\tvalid_0's rmse: 0.0888252\n",
            "[70]\tvalid_0's rmse: 0.0888252\n",
            "Early stopping, best iteration is:\n",
            "[56]\tvalid_0's rmse: 0.0888252\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.297853\n",
            "[20]\tvalid_0's rmse: 0.204533\n",
            "[30]\tvalid_0's rmse: 0.143476\n",
            "[40]\tvalid_0's rmse: 0.107763\n",
            "[50]\tvalid_0's rmse: 0.0837158\n",
            "[60]\tvalid_0's rmse: 0.0713203\n",
            "[70]\tvalid_0's rmse: 0.0713203\n",
            "Early stopping, best iteration is:\n",
            "[59]\tvalid_0's rmse: 0.0713203\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.270764\n",
            "[20]\tvalid_0's rmse: 0.187974\n",
            "[30]\tvalid_0's rmse: 0.134565\n",
            "[40]\tvalid_0's rmse: 0.101354\n",
            "[50]\tvalid_0's rmse: 0.0810609\n",
            "[60]\tvalid_0's rmse: 0.0684308\n",
            "[70]\tvalid_0's rmse: 0.0652849\n",
            "[80]\tvalid_0's rmse: 0.0652849\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0652849\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.294943\n",
            "[20]\tvalid_0's rmse: 0.211444\n",
            "[30]\tvalid_0's rmse: 0.164309\n",
            "[40]\tvalid_0's rmse: 0.134925\n",
            "[50]\tvalid_0's rmse: 0.112935\n",
            "[60]\tvalid_0's rmse: 0.0994099\n",
            "[70]\tvalid_0's rmse: 0.0912381\n",
            "[80]\tvalid_0's rmse: 0.0912381\n",
            "Early stopping, best iteration is:\n",
            "[64]\tvalid_0's rmse: 0.0912381\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.296553\n",
            "[20]\tvalid_0's rmse: 0.208938\n",
            "[30]\tvalid_0's rmse: 0.160251\n",
            "[40]\tvalid_0's rmse: 0.127258\n",
            "[50]\tvalid_0's rmse: 0.109384\n",
            "[60]\tvalid_0's rmse: 0.0966215\n",
            "[70]\tvalid_0's rmse: 0.0930283\n",
            "[80]\tvalid_0's rmse: 0.0930283\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0930283\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.329771\n",
            "[20]\tvalid_0's rmse: 0.228261\n",
            "[30]\tvalid_0's rmse: 0.15966\n",
            "[40]\tvalid_0's rmse: 0.128857\n",
            "[50]\tvalid_0's rmse: 0.112411\n",
            "[60]\tvalid_0's rmse: 0.0966135\n",
            "[70]\tvalid_0's rmse: 0.0940607\n",
            "[80]\tvalid_0's rmse: 0.0940607\n",
            "Early stopping, best iteration is:\n",
            "[62]\tvalid_0's rmse: 0.0940607\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.237889\n",
            "[20]\tvalid_0's rmse: 0.150231\n",
            "[30]\tvalid_0's rmse: 0.0932165\n",
            "[40]\tvalid_0's rmse: 0.0642158\n",
            "[50]\tvalid_0's rmse: 0.0466544\n",
            "[60]\tvalid_0's rmse: 0.0349405\n",
            "[70]\tvalid_0's rmse: 0.030297\n",
            "[80]\tvalid_0's rmse: 0.030297\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.030297\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.330825\n",
            "[20]\tvalid_0's rmse: 0.237994\n",
            "[30]\tvalid_0's rmse: 0.174328\n",
            "[40]\tvalid_0's rmse: 0.131248\n",
            "[50]\tvalid_0's rmse: 0.112792\n",
            "[60]\tvalid_0's rmse: 0.0983632\n",
            "[70]\tvalid_0's rmse: 0.0955744\n",
            "[80]\tvalid_0's rmse: 0.0955744\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0955744\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.345646\n",
            "[20]\tvalid_0's rmse: 0.244827\n",
            "[30]\tvalid_0's rmse: 0.181866\n",
            "[40]\tvalid_0's rmse: 0.139174\n",
            "[50]\tvalid_0's rmse: 0.115293\n",
            "[60]\tvalid_0's rmse: 0.102599\n",
            "[70]\tvalid_0's rmse: 0.0953866\n",
            "[80]\tvalid_0's rmse: 0.0953866\n",
            "Early stopping, best iteration is:\n",
            "[64]\tvalid_0's rmse: 0.0953866\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.238204\n",
            "[20]\tvalid_0's rmse: 0.153112\n",
            "[30]\tvalid_0's rmse: 0.0952046\n",
            "[40]\tvalid_0's rmse: 0.0658695\n",
            "[50]\tvalid_0's rmse: 0.0422464\n",
            "[60]\tvalid_0's rmse: 0.0286621\n",
            "[70]\tvalid_0's rmse: 0.0253121\n",
            "[80]\tvalid_0's rmse: 0.0253121\n",
            "Early stopping, best iteration is:\n",
            "[65]\tvalid_0's rmse: 0.0253121\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.321538\n",
            "[20]\tvalid_0's rmse: 0.226285\n",
            "[30]\tvalid_0's rmse: 0.164726\n",
            "[40]\tvalid_0's rmse: 0.130298\n",
            "[50]\tvalid_0's rmse: 0.103911\n",
            "[60]\tvalid_0's rmse: 0.0900049\n",
            "[70]\tvalid_0's rmse: 0.0820446\n",
            "[80]\tvalid_0's rmse: 0.0820446\n",
            "Early stopping, best iteration is:\n",
            "[67]\tvalid_0's rmse: 0.0820446\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.320645\n",
            "[20]\tvalid_0's rmse: 0.232098\n",
            "[30]\tvalid_0's rmse: 0.176\n",
            "[40]\tvalid_0's rmse: 0.134514\n",
            "[50]\tvalid_0's rmse: 0.115037\n",
            "[60]\tvalid_0's rmse: 0.099972\n",
            "[70]\tvalid_0's rmse: 0.0886509\n",
            "[80]\tvalid_0's rmse: 0.0886509\n",
            "[90]\tvalid_0's rmse: 0.0886509\n",
            "Early stopping, best iteration is:\n",
            "[70]\tvalid_0's rmse: 0.0886509\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.307045\n",
            "[20]\tvalid_0's rmse: 0.217361\n",
            "[30]\tvalid_0's rmse: 0.160215\n",
            "[40]\tvalid_0's rmse: 0.119756\n",
            "[50]\tvalid_0's rmse: 0.0977549\n",
            "[60]\tvalid_0's rmse: 0.0817179\n",
            "[70]\tvalid_0's rmse: 0.0721701\n",
            "[80]\tvalid_0's rmse: 0.0721701\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's rmse: 0.0721701\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.238857\n",
            "[20]\tvalid_0's rmse: 0.156124\n",
            "[30]\tvalid_0's rmse: 0.10059\n",
            "[40]\tvalid_0's rmse: 0.0665143\n",
            "[50]\tvalid_0's rmse: 0.0454807\n",
            "[60]\tvalid_0's rmse: 0.0318518\n",
            "[70]\tvalid_0's rmse: 0.0194756\n",
            "[80]\tvalid_0's rmse: 0.0189997\n",
            "[90]\tvalid_0's rmse: 0.0189997\n",
            "Early stopping, best iteration is:\n",
            "[71]\tvalid_0's rmse: 0.0189997\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.269858\n",
            "[20]\tvalid_0's rmse: 0.18761\n",
            "[30]\tvalid_0's rmse: 0.13349\n",
            "[40]\tvalid_0's rmse: 0.0994062\n",
            "[50]\tvalid_0's rmse: 0.0761078\n",
            "[60]\tvalid_0's rmse: 0.0615955\n",
            "[70]\tvalid_0's rmse: 0.0525122\n",
            "[80]\tvalid_0's rmse: 0.0502539\n",
            "[90]\tvalid_0's rmse: 0.0502539\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0502539\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.293265\n",
            "[20]\tvalid_0's rmse: 0.212193\n",
            "[30]\tvalid_0's rmse: 0.161696\n",
            "[40]\tvalid_0's rmse: 0.127434\n",
            "[50]\tvalid_0's rmse: 0.10549\n",
            "[60]\tvalid_0's rmse: 0.0923172\n",
            "[70]\tvalid_0's rmse: 0.0838068\n",
            "[80]\tvalid_0's rmse: 0.0826704\n",
            "[90]\tvalid_0's rmse: 0.0826704\n",
            "Early stopping, best iteration is:\n",
            "[71]\tvalid_0's rmse: 0.0826704\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.146343\n",
            "[20]\tvalid_0's rmse: 0.098787\n",
            "[30]\tvalid_0's rmse: 0.0595038\n",
            "[40]\tvalid_0's rmse: 0.0387245\n",
            "[50]\tvalid_0's rmse: 0.0214287\n",
            "[60]\tvalid_0's rmse: 0.0094628\n",
            "[70]\tvalid_0's rmse: 0.0078026\n",
            "[80]\tvalid_0's rmse: 0.00745735\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's rmse: 0.00725864\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.139735\n",
            "[20]\tvalid_0's rmse: 0.0901432\n",
            "[30]\tvalid_0's rmse: 0.0568289\n",
            "[40]\tvalid_0's rmse: 0.0372911\n",
            "[50]\tvalid_0's rmse: 0.0209503\n",
            "[60]\tvalid_0's rmse: 0.00977161\n",
            "[70]\tvalid_0's rmse: 0.00506879\n",
            "[80]\tvalid_0's rmse: 0.00505764\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's rmse: 0.00448777\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.203505\n",
            "[20]\tvalid_0's rmse: 0.134127\n",
            "[30]\tvalid_0's rmse: 0.0910926\n",
            "[40]\tvalid_0's rmse: 0.0618603\n",
            "[50]\tvalid_0's rmse: 0.0420142\n",
            "[60]\tvalid_0's rmse: 0.0301311\n",
            "[70]\tvalid_0's rmse: 0.0219339\n",
            "[80]\tvalid_0's rmse: 0.0205705\n",
            "[90]\tvalid_0's rmse: 0.0205705\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0205705\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.197465\n",
            "[20]\tvalid_0's rmse: 0.13233\n",
            "[30]\tvalid_0's rmse: 0.090302\n",
            "[40]\tvalid_0's rmse: 0.0608235\n",
            "[50]\tvalid_0's rmse: 0.0434096\n",
            "[60]\tvalid_0's rmse: 0.0330668\n",
            "[70]\tvalid_0's rmse: 0.0245144\n",
            "[80]\tvalid_0's rmse: 0.0231872\n",
            "[90]\tvalid_0's rmse: 0.0231872\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's rmse: 0.0231872\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.163555\n",
            "[20]\tvalid_0's rmse: 0.107332\n",
            "[30]\tvalid_0's rmse: 0.0691263\n",
            "[40]\tvalid_0's rmse: 0.0439938\n",
            "[50]\tvalid_0's rmse: 0.0293771\n",
            "[60]\tvalid_0's rmse: 0.0188738\n",
            "[70]\tvalid_0's rmse: 0.0113477\n",
            "[80]\tvalid_0's rmse: 0.0103855\n",
            "[90]\tvalid_0's rmse: 0.0103855\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.0098597\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.215613\n",
            "[20]\tvalid_0's rmse: 0.153799\n",
            "[30]\tvalid_0's rmse: 0.114726\n",
            "[40]\tvalid_0's rmse: 0.0878815\n",
            "[50]\tvalid_0's rmse: 0.0698794\n",
            "[60]\tvalid_0's rmse: 0.0588803\n",
            "[70]\tvalid_0's rmse: 0.0511879\n",
            "[80]\tvalid_0's rmse: 0.0486417\n",
            "[90]\tvalid_0's rmse: 0.0486417\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.0486417\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.230172\n",
            "[20]\tvalid_0's rmse: 0.167651\n",
            "[30]\tvalid_0's rmse: 0.127974\n",
            "[40]\tvalid_0's rmse: 0.101817\n",
            "[50]\tvalid_0's rmse: 0.0844152\n",
            "[60]\tvalid_0's rmse: 0.0749746\n",
            "[70]\tvalid_0's rmse: 0.0673806\n",
            "[80]\tvalid_0's rmse: 0.0630931\n",
            "[90]\tvalid_0's rmse: 0.0630931\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0630931\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.155593\n",
            "[20]\tvalid_0's rmse: 0.115536\n",
            "[30]\tvalid_0's rmse: 0.0920016\n",
            "[40]\tvalid_0's rmse: 0.0738034\n",
            "[50]\tvalid_0's rmse: 0.0587802\n",
            "[60]\tvalid_0's rmse: 0.046569\n",
            "[70]\tvalid_0's rmse: 0.0392491\n",
            "[80]\tvalid_0's rmse: 0.0341556\n",
            "[90]\tvalid_0's rmse: 0.0341556\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 0.0341556\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.153173\n",
            "[20]\tvalid_0's rmse: 0.121674\n",
            "[30]\tvalid_0's rmse: 0.0849086\n",
            "[40]\tvalid_0's rmse: 0.0612132\n",
            "[50]\tvalid_0's rmse: 0.0470642\n",
            "[60]\tvalid_0's rmse: 0.0377201\n",
            "[70]\tvalid_0's rmse: 0.0310771\n",
            "[80]\tvalid_0's rmse: 0.0281345\n",
            "[90]\tvalid_0's rmse: 0.0281345\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0279127\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.17349\n",
            "[20]\tvalid_0's rmse: 0.119024\n",
            "[30]\tvalid_0's rmse: 0.0849315\n",
            "[40]\tvalid_0's rmse: 0.0613372\n",
            "[50]\tvalid_0's rmse: 0.0456224\n",
            "[60]\tvalid_0's rmse: 0.0361695\n",
            "[70]\tvalid_0's rmse: 0.0294558\n",
            "[80]\tvalid_0's rmse: 0.0260645\n",
            "[90]\tvalid_0's rmse: 0.0260645\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 0.0260645\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.264046\n",
            "[20]\tvalid_0's rmse: 0.187044\n",
            "[30]\tvalid_0's rmse: 0.149719\n",
            "[40]\tvalid_0's rmse: 0.12701\n",
            "[50]\tvalid_0's rmse: 0.115016\n",
            "[60]\tvalid_0's rmse: 0.106918\n",
            "[70]\tvalid_0's rmse: 0.0996743\n",
            "[80]\tvalid_0's rmse: 0.0956471\n",
            "[90]\tvalid_0's rmse: 0.0956471\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0956471\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.274687\n",
            "[20]\tvalid_0's rmse: 0.192915\n",
            "[30]\tvalid_0's rmse: 0.140235\n",
            "[40]\tvalid_0's rmse: 0.111286\n",
            "[50]\tvalid_0's rmse: 0.0976019\n",
            "[60]\tvalid_0's rmse: 0.0893744\n",
            "[70]\tvalid_0's rmse: 0.0837852\n",
            "[80]\tvalid_0's rmse: 0.0805388\n",
            "[90]\tvalid_0's rmse: 0.0805388\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.0805388\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.162736\n",
            "[20]\tvalid_0's rmse: 0.103023\n",
            "[30]\tvalid_0's rmse: 0.0569715\n",
            "[40]\tvalid_0's rmse: 0.0346175\n",
            "[50]\tvalid_0's rmse: 0.0209569\n",
            "[60]\tvalid_0's rmse: 0.0133532\n",
            "[70]\tvalid_0's rmse: 0.00725545\n",
            "[80]\tvalid_0's rmse: 0.00535165\n",
            "[90]\tvalid_0's rmse: 0.00535165\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.00535165\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.172385\n",
            "[20]\tvalid_0's rmse: 0.118273\n",
            "[30]\tvalid_0's rmse: 0.0780338\n",
            "[40]\tvalid_0's rmse: 0.0506217\n",
            "[50]\tvalid_0's rmse: 0.0368252\n",
            "[60]\tvalid_0's rmse: 0.0291441\n",
            "[70]\tvalid_0's rmse: 0.02327\n",
            "[80]\tvalid_0's rmse: 0.0208825\n",
            "[90]\tvalid_0's rmse: 0.0174828\n",
            "[100]\tvalid_0's rmse: 0.0174828\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0174828\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.19809\n",
            "[20]\tvalid_0's rmse: 0.12888\n",
            "[30]\tvalid_0's rmse: 0.0866504\n",
            "[40]\tvalid_0's rmse: 0.0564312\n",
            "[50]\tvalid_0's rmse: 0.0408337\n",
            "[60]\tvalid_0's rmse: 0.0331192\n",
            "[70]\tvalid_0's rmse: 0.0278368\n",
            "[80]\tvalid_0's rmse: 0.0237148\n",
            "[90]\tvalid_0's rmse: 0.0237148\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's rmse: 0.0237148\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.218239\n",
            "[20]\tvalid_0's rmse: 0.151551\n",
            "[30]\tvalid_0's rmse: 0.102836\n",
            "[40]\tvalid_0's rmse: 0.0757454\n",
            "[50]\tvalid_0's rmse: 0.0578735\n",
            "[60]\tvalid_0's rmse: 0.0505793\n",
            "[70]\tvalid_0's rmse: 0.0446218\n",
            "[80]\tvalid_0's rmse: 0.0410873\n",
            "[90]\tvalid_0's rmse: 0.0410873\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's rmse: 0.0410873\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.105176\n",
            "[20]\tvalid_0's rmse: 0.0587755\n",
            "[30]\tvalid_0's rmse: 0.029894\n",
            "[40]\tvalid_0's rmse: 0.0119928\n",
            "[50]\tvalid_0's rmse: 0.00142085\n",
            "[60]\tvalid_0's rmse: 0.00616669\n",
            "[70]\tvalid_0's rmse: 0.00969959\n",
            "Early stopping, best iteration is:\n",
            "[52]\tvalid_0's rmse: 0.000139077\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0986446\n",
            "[20]\tvalid_0's rmse: 0.0527294\n",
            "[30]\tvalid_0's rmse: 0.0219282\n",
            "[40]\tvalid_0's rmse: 0.00493865\n",
            "[50]\tvalid_0's rmse: 0.00510262\n",
            "[60]\tvalid_0's rmse: 0.00982645\n",
            "Early stopping, best iteration is:\n",
            "[43]\tvalid_0's rmse: 0.000838401\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.117092\n",
            "[20]\tvalid_0's rmse: 0.0633661\n",
            "[30]\tvalid_0's rmse: 0.0319805\n",
            "[40]\tvalid_0's rmse: 0.0119852\n",
            "[50]\tvalid_0's rmse: 0.000757022\n",
            "[60]\tvalid_0's rmse: 0.00716779\n",
            "Early stopping, best iteration is:\n",
            "[49]\tvalid_0's rmse: 0.000118067\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.134347\n",
            "[20]\tvalid_0's rmse: 0.0897145\n",
            "[30]\tvalid_0's rmse: 0.0600863\n",
            "[40]\tvalid_0's rmse: 0.0421221\n",
            "[50]\tvalid_0's rmse: 0.0306423\n",
            "[60]\tvalid_0's rmse: 0.0257675\n",
            "[70]\tvalid_0's rmse: 0.0210714\n",
            "[80]\tvalid_0's rmse: 0.0130041\n",
            "[90]\tvalid_0's rmse: 0.0124013\n",
            "[100]\tvalid_0's rmse: 0.0128806\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0124013\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.136263\n",
            "[20]\tvalid_0's rmse: 0.0993366\n",
            "[30]\tvalid_0's rmse: 0.069653\n",
            "[40]\tvalid_0's rmse: 0.0516112\n",
            "[50]\tvalid_0's rmse: 0.0385041\n",
            "[60]\tvalid_0's rmse: 0.0307761\n",
            "[70]\tvalid_0's rmse: 0.025211\n",
            "[80]\tvalid_0's rmse: 0.0219304\n",
            "[90]\tvalid_0's rmse: 0.0219304\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 0.0219304\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0563257\n",
            "[20]\tvalid_0's rmse: 0.0212588\n",
            "[30]\tvalid_0's rmse: 0.000851947\n",
            "[40]\tvalid_0's rmse: 0.0129834\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's rmse: 0.000344035\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.054672\n",
            "[20]\tvalid_0's rmse: 0.0247526\n",
            "[30]\tvalid_0's rmse: 0.00854496\n",
            "[40]\tvalid_0's rmse: 0.0068651\n",
            "[50]\tvalid_0's rmse: 0.00103798\n",
            "[60]\tvalid_0's rmse: 0.00616403\n",
            "Early stopping, best iteration is:\n",
            "[48]\tvalid_0's rmse: 0.000267359\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0190044\n",
            "[20]\tvalid_0's rmse: 0.0250277\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's rmse: 0.0110634\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.000780291\n",
            "[20]\tvalid_0's rmse: 0.000272778\n",
            "[30]\tvalid_0's rmse: 0.00280658\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's rmse: 0.000134779\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.502485\n",
            "[20]\tvalid_0's rmse: 0.502485\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.502485\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.511336\n",
            "[20]\tvalid_0's rmse: 0.511336\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.511336\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444358\n",
            "[20]\tvalid_0's rmse: 0.444358\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.444358\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.398313\n",
            "[20]\tvalid_0's rmse: 0.398313\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.398313\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416187\n",
            "[20]\tvalid_0's rmse: 0.416187\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.416187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.464758\n",
            "[20]\tvalid_0's rmse: 0.477105\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.427877\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.438107\n",
            "[20]\tvalid_0's rmse: 0.392346\n",
            "[30]\tvalid_0's rmse: 0.354265\n",
            "[40]\tvalid_0's rmse: 0.322574\n",
            "[50]\tvalid_0's rmse: 0.296202\n",
            "[60]\tvalid_0's rmse: 0.274255\n",
            "[70]\tvalid_0's rmse: 0.255991\n",
            "[80]\tvalid_0's rmse: 0.248904\n",
            "[90]\tvalid_0's rmse: 0.24506\n",
            "[100]\tvalid_0's rmse: 0.241644\n",
            "[110]\tvalid_0's rmse: 0.241644\n",
            "Early stopping, best iteration is:\n",
            "[97]\tvalid_0's rmse: 0.240515\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.302487\n",
            "[20]\tvalid_0's rmse: 0.250073\n",
            "[30]\tvalid_0's rmse: 0.207409\n",
            "[40]\tvalid_0's rmse: 0.173756\n",
            "[50]\tvalid_0's rmse: 0.144821\n",
            "[60]\tvalid_0's rmse: 0.119436\n",
            "[70]\tvalid_0's rmse: 0.0969982\n",
            "[80]\tvalid_0's rmse: 0.0778227\n",
            "[90]\tvalid_0's rmse: 0.0619768\n",
            "[100]\tvalid_0's rmse: 0.047523\n",
            "[110]\tvalid_0's rmse: 0.0401345\n",
            "[120]\tvalid_0's rmse: 0.0401345\n",
            "Early stopping, best iteration is:\n",
            "[106]\tvalid_0's rmse: 0.0401345\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416798\n",
            "[20]\tvalid_0's rmse: 0.364648\n",
            "[30]\tvalid_0's rmse: 0.323936\n",
            "[40]\tvalid_0's rmse: 0.288855\n",
            "[50]\tvalid_0's rmse: 0.259718\n",
            "[60]\tvalid_0's rmse: 0.235451\n",
            "[70]\tvalid_0's rmse: 0.215032\n",
            "[80]\tvalid_0's rmse: 0.197566\n",
            "[90]\tvalid_0's rmse: 0.181905\n",
            "[100]\tvalid_0's rmse: 0.168516\n",
            "[110]\tvalid_0's rmse: 0.15668\n",
            "[120]\tvalid_0's rmse: 0.15668\n",
            "[130]\tvalid_0's rmse: 0.15668\n",
            "Early stopping, best iteration is:\n",
            "[110]\tvalid_0's rmse: 0.15668\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.427433\n",
            "[20]\tvalid_0's rmse: 0.369969\n",
            "[30]\tvalid_0's rmse: 0.327462\n",
            "[40]\tvalid_0's rmse: 0.294706\n",
            "[50]\tvalid_0's rmse: 0.266646\n",
            "[60]\tvalid_0's rmse: 0.241886\n",
            "[70]\tvalid_0's rmse: 0.22078\n",
            "[80]\tvalid_0's rmse: 0.202706\n",
            "[90]\tvalid_0's rmse: 0.186876\n",
            "[100]\tvalid_0's rmse: 0.172838\n",
            "[110]\tvalid_0's rmse: 0.160415\n",
            "[120]\tvalid_0's rmse: 0.152557\n",
            "[130]\tvalid_0's rmse: 0.152557\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 0.152557\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.259657\n",
            "[20]\tvalid_0's rmse: 0.185305\n",
            "[30]\tvalid_0's rmse: 0.134501\n",
            "[40]\tvalid_0's rmse: 0.0955261\n",
            "[50]\tvalid_0's rmse: 0.0647896\n",
            "[60]\tvalid_0's rmse: 0.0498165\n",
            "[70]\tvalid_0's rmse: 0.0362266\n",
            "[80]\tvalid_0's rmse: 0.0237289\n",
            "[90]\tvalid_0's rmse: 0.0159634\n",
            "[100]\tvalid_0's rmse: 0.0159634\n",
            "Early stopping, best iteration is:\n",
            "[88]\tvalid_0's rmse: 0.0159634\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.353531\n",
            "[20]\tvalid_0's rmse: 0.270986\n",
            "[30]\tvalid_0's rmse: 0.214072\n",
            "[40]\tvalid_0's rmse: 0.174068\n",
            "[50]\tvalid_0's rmse: 0.152572\n",
            "[60]\tvalid_0's rmse: 0.138069\n",
            "[70]\tvalid_0's rmse: 0.126131\n",
            "[80]\tvalid_0's rmse: 0.114912\n",
            "[90]\tvalid_0's rmse: 0.105285\n",
            "[100]\tvalid_0's rmse: 0.101176\n",
            "[110]\tvalid_0's rmse: 0.101176\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 0.101176\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.359726\n",
            "[20]\tvalid_0's rmse: 0.291698\n",
            "[30]\tvalid_0's rmse: 0.243243\n",
            "[40]\tvalid_0's rmse: 0.20728\n",
            "[50]\tvalid_0's rmse: 0.180002\n",
            "[60]\tvalid_0's rmse: 0.158693\n",
            "[70]\tvalid_0's rmse: 0.140523\n",
            "[80]\tvalid_0's rmse: 0.127474\n",
            "[90]\tvalid_0's rmse: 0.117794\n",
            "[100]\tvalid_0's rmse: 0.116626\n",
            "[110]\tvalid_0's rmse: 0.116626\n",
            "Early stopping, best iteration is:\n",
            "[91]\tvalid_0's rmse: 0.116626\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.339618\n",
            "[20]\tvalid_0's rmse: 0.259124\n",
            "[30]\tvalid_0's rmse: 0.197608\n",
            "[40]\tvalid_0's rmse: 0.151496\n",
            "[50]\tvalid_0's rmse: 0.119516\n",
            "[60]\tvalid_0's rmse: 0.103653\n",
            "[70]\tvalid_0's rmse: 0.0930349\n",
            "[80]\tvalid_0's rmse: 0.0830732\n",
            "[90]\tvalid_0's rmse: 0.0803496\n",
            "[100]\tvalid_0's rmse: 0.0803496\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0803496\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.261018\n",
            "[20]\tvalid_0's rmse: 0.178254\n",
            "[30]\tvalid_0's rmse: 0.119306\n",
            "[40]\tvalid_0's rmse: 0.076598\n",
            "[50]\tvalid_0's rmse: 0.0493305\n",
            "[60]\tvalid_0's rmse: 0.0377174\n",
            "[70]\tvalid_0's rmse: 0.028378\n",
            "[80]\tvalid_0's rmse: 0.0205622\n",
            "[90]\tvalid_0's rmse: 0.0161393\n",
            "[100]\tvalid_0's rmse: 0.0161393\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.0161393\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.266432\n",
            "[20]\tvalid_0's rmse: 0.19935\n",
            "[30]\tvalid_0's rmse: 0.148573\n",
            "[40]\tvalid_0's rmse: 0.11082\n",
            "[50]\tvalid_0's rmse: 0.0857278\n",
            "[60]\tvalid_0's rmse: 0.0732445\n",
            "[70]\tvalid_0's rmse: 0.0642967\n",
            "[80]\tvalid_0's rmse: 0.056624\n",
            "[90]\tvalid_0's rmse: 0.0547258\n",
            "[100]\tvalid_0's rmse: 0.0547258\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.0547258\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.288889\n",
            "[20]\tvalid_0's rmse: 0.209573\n",
            "[30]\tvalid_0's rmse: 0.163655\n",
            "[40]\tvalid_0's rmse: 0.127764\n",
            "[50]\tvalid_0's rmse: 0.101821\n",
            "[60]\tvalid_0's rmse: 0.0909857\n",
            "[70]\tvalid_0's rmse: 0.0840961\n",
            "[80]\tvalid_0's rmse: 0.0783161\n",
            "[90]\tvalid_0's rmse: 0.0778203\n",
            "[100]\tvalid_0's rmse: 0.0778203\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0778203\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.180102\n",
            "[20]\tvalid_0's rmse: 0.114933\n",
            "[30]\tvalid_0's rmse: 0.0720519\n",
            "[40]\tvalid_0's rmse: 0.0423371\n",
            "[50]\tvalid_0's rmse: 0.025609\n",
            "[60]\tvalid_0's rmse: 0.0156576\n",
            "[70]\tvalid_0's rmse: 0.00718753\n",
            "[80]\tvalid_0's rmse: 0.0074459\n",
            "[90]\tvalid_0's rmse: 0.00645478\n",
            "[100]\tvalid_0's rmse: 0.00645478\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.00506014\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.167586\n",
            "[20]\tvalid_0's rmse: 0.112998\n",
            "[30]\tvalid_0's rmse: 0.0735293\n",
            "[40]\tvalid_0's rmse: 0.0462938\n",
            "[50]\tvalid_0's rmse: 0.0318671\n",
            "[60]\tvalid_0's rmse: 0.0284114\n",
            "[70]\tvalid_0's rmse: 0.0257787\n",
            "[80]\tvalid_0's rmse: 0.0240733\n",
            "[90]\tvalid_0's rmse: 0.0241302\n",
            "[100]\tvalid_0's rmse: 0.0241302\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.022855\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.207506\n",
            "[20]\tvalid_0's rmse: 0.13668\n",
            "[30]\tvalid_0's rmse: 0.0913249\n",
            "[40]\tvalid_0's rmse: 0.0642151\n",
            "[50]\tvalid_0's rmse: 0.0425226\n",
            "[60]\tvalid_0's rmse: 0.0331657\n",
            "[70]\tvalid_0's rmse: 0.0271211\n",
            "[80]\tvalid_0's rmse: 0.0215517\n",
            "[90]\tvalid_0's rmse: 0.0207013\n",
            "[100]\tvalid_0's rmse: 0.0207013\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.0207013\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.202836\n",
            "[20]\tvalid_0's rmse: 0.135072\n",
            "[30]\tvalid_0's rmse: 0.0933529\n",
            "[40]\tvalid_0's rmse: 0.0656799\n",
            "[50]\tvalid_0's rmse: 0.0456785\n",
            "[60]\tvalid_0's rmse: 0.0370927\n",
            "[70]\tvalid_0's rmse: 0.0303619\n",
            "[80]\tvalid_0's rmse: 0.0248881\n",
            "[90]\tvalid_0's rmse: 0.022289\n",
            "[100]\tvalid_0's rmse: 0.022289\n",
            "Early stopping, best iteration is:\n",
            "[85]\tvalid_0's rmse: 0.022289\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.161252\n",
            "[20]\tvalid_0's rmse: 0.10022\n",
            "[30]\tvalid_0's rmse: 0.0642583\n",
            "[40]\tvalid_0's rmse: 0.0394792\n",
            "[50]\tvalid_0's rmse: 0.0279328\n",
            "[60]\tvalid_0's rmse: 0.018574\n",
            "[70]\tvalid_0's rmse: 0.0103822\n",
            "[80]\tvalid_0's rmse: 0.00430672\n",
            "[90]\tvalid_0's rmse: 0.000847503\n",
            "[100]\tvalid_0's rmse: 0.000847503\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.000847503\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.217988\n",
            "[20]\tvalid_0's rmse: 0.15474\n",
            "[30]\tvalid_0's rmse: 0.116199\n",
            "[40]\tvalid_0's rmse: 0.0889203\n",
            "[50]\tvalid_0's rmse: 0.0694039\n",
            "[60]\tvalid_0's rmse: 0.0617086\n",
            "[70]\tvalid_0's rmse: 0.0558565\n",
            "[80]\tvalid_0's rmse: 0.0508024\n",
            "[90]\tvalid_0's rmse: 0.0491868\n",
            "[100]\tvalid_0's rmse: 0.0491868\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.0491868\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.22877\n",
            "[20]\tvalid_0's rmse: 0.166523\n",
            "[30]\tvalid_0's rmse: 0.127507\n",
            "[40]\tvalid_0's rmse: 0.100724\n",
            "[50]\tvalid_0's rmse: 0.0806273\n",
            "[60]\tvalid_0's rmse: 0.0721972\n",
            "[70]\tvalid_0's rmse: 0.0656316\n",
            "[80]\tvalid_0's rmse: 0.0603035\n",
            "[90]\tvalid_0's rmse: 0.0580393\n",
            "[100]\tvalid_0's rmse: 0.0580393\n",
            "Early stopping, best iteration is:\n",
            "[85]\tvalid_0's rmse: 0.0580393\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.158702\n",
            "[20]\tvalid_0's rmse: 0.119476\n",
            "[30]\tvalid_0's rmse: 0.0827809\n",
            "[40]\tvalid_0's rmse: 0.0565413\n",
            "[50]\tvalid_0's rmse: 0.0395784\n",
            "[60]\tvalid_0's rmse: 0.0278772\n",
            "[70]\tvalid_0's rmse: 0.019817\n",
            "[80]\tvalid_0's rmse: 0.0133574\n",
            "[90]\tvalid_0's rmse: 0.00693346\n",
            "[100]\tvalid_0's rmse: 0.00693346\n",
            "Early stopping, best iteration is:\n",
            "[89]\tvalid_0's rmse: 0.00693346\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.147758\n",
            "[20]\tvalid_0's rmse: 0.108988\n",
            "[30]\tvalid_0's rmse: 0.0740998\n",
            "[40]\tvalid_0's rmse: 0.05004\n",
            "[50]\tvalid_0's rmse: 0.0342207\n",
            "[60]\tvalid_0's rmse: 0.0257547\n",
            "[70]\tvalid_0's rmse: 0.020277\n",
            "[80]\tvalid_0's rmse: 0.0159565\n",
            "[90]\tvalid_0's rmse: 0.0133611\n",
            "[100]\tvalid_0's rmse: 0.0133611\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.0133611\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.175563\n",
            "[20]\tvalid_0's rmse: 0.120065\n",
            "[30]\tvalid_0's rmse: 0.0863295\n",
            "[40]\tvalid_0's rmse: 0.0620427\n",
            "[50]\tvalid_0's rmse: 0.045555\n",
            "[60]\tvalid_0's rmse: 0.0380698\n",
            "[70]\tvalid_0's rmse: 0.0318922\n",
            "[80]\tvalid_0's rmse: 0.0266938\n",
            "[90]\tvalid_0's rmse: 0.023046\n",
            "[100]\tvalid_0's rmse: 0.023046\n",
            "Early stopping, best iteration is:\n",
            "[89]\tvalid_0's rmse: 0.023046\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.304452\n",
            "[20]\tvalid_0's rmse: 0.252913\n",
            "[30]\tvalid_0's rmse: 0.220157\n",
            "[40]\tvalid_0's rmse: 0.197715\n",
            "[50]\tvalid_0's rmse: 0.179601\n",
            "[60]\tvalid_0's rmse: 0.171752\n",
            "[70]\tvalid_0's rmse: 0.165482\n",
            "[80]\tvalid_0's rmse: 0.160539\n",
            "[90]\tvalid_0's rmse: 0.15602\n",
            "[100]\tvalid_0's rmse: 0.15602\n",
            "Early stopping, best iteration is:\n",
            "[88]\tvalid_0's rmse: 0.15602\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.304585\n",
            "[20]\tvalid_0's rmse: 0.244525\n",
            "[30]\tvalid_0's rmse: 0.196781\n",
            "[40]\tvalid_0's rmse: 0.166198\n",
            "[50]\tvalid_0's rmse: 0.142139\n",
            "[60]\tvalid_0's rmse: 0.128569\n",
            "[70]\tvalid_0's rmse: 0.123387\n",
            "[80]\tvalid_0's rmse: 0.117634\n",
            "[90]\tvalid_0's rmse: 0.112816\n",
            "[100]\tvalid_0's rmse: 0.112816\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's rmse: 0.112816\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.183747\n",
            "[20]\tvalid_0's rmse: 0.124095\n",
            "[30]\tvalid_0's rmse: 0.0760617\n",
            "[40]\tvalid_0's rmse: 0.049855\n",
            "[50]\tvalid_0's rmse: 0.0318839\n",
            "[60]\tvalid_0's rmse: 0.0235331\n",
            "[70]\tvalid_0's rmse: 0.0186556\n",
            "[80]\tvalid_0's rmse: 0.0149381\n",
            "[90]\tvalid_0's rmse: 0.0146153\n",
            "[100]\tvalid_0's rmse: 0.0146153\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0146153\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.183189\n",
            "[20]\tvalid_0's rmse: 0.119281\n",
            "[30]\tvalid_0's rmse: 0.073541\n",
            "[40]\tvalid_0's rmse: 0.0508806\n",
            "[50]\tvalid_0's rmse: 0.0375924\n",
            "[60]\tvalid_0's rmse: 0.0308666\n",
            "[70]\tvalid_0's rmse: 0.0251644\n",
            "[80]\tvalid_0's rmse: 0.0211572\n",
            "[90]\tvalid_0's rmse: 0.0173391\n",
            "[100]\tvalid_0's rmse: 0.0165217\n",
            "[110]\tvalid_0's rmse: 0.0157724\n",
            "[120]\tvalid_0's rmse: 0.0149933\n",
            "[130]\tvalid_0's rmse: 0.0149933\n",
            "Early stopping, best iteration is:\n",
            "[112]\tvalid_0's rmse: 0.0149933\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.197033\n",
            "[20]\tvalid_0's rmse: 0.131499\n",
            "[30]\tvalid_0's rmse: 0.090076\n",
            "[40]\tvalid_0's rmse: 0.0679992\n",
            "[50]\tvalid_0's rmse: 0.0522393\n",
            "[60]\tvalid_0's rmse: 0.0425132\n",
            "[70]\tvalid_0's rmse: 0.0382369\n",
            "[80]\tvalid_0's rmse: 0.0350176\n",
            "[90]\tvalid_0's rmse: 0.0346513\n",
            "[100]\tvalid_0's rmse: 0.0346513\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's rmse: 0.0346513\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.22805\n",
            "[20]\tvalid_0's rmse: 0.162003\n",
            "[30]\tvalid_0's rmse: 0.117389\n",
            "[40]\tvalid_0's rmse: 0.0919494\n",
            "[50]\tvalid_0's rmse: 0.0756425\n",
            "[60]\tvalid_0's rmse: 0.0630265\n",
            "[70]\tvalid_0's rmse: 0.0585347\n",
            "[80]\tvalid_0's rmse: 0.0543166\n",
            "[90]\tvalid_0's rmse: 0.0543166\n",
            "[100]\tvalid_0's rmse: 0.0543166\n",
            "Early stopping, best iteration is:\n",
            "[80]\tvalid_0's rmse: 0.0543166\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0847116\n",
            "[20]\tvalid_0's rmse: 0.038958\n",
            "[30]\tvalid_0's rmse: 0.0120293\n",
            "[40]\tvalid_0's rmse: 0.00154773\n",
            "[50]\tvalid_0's rmse: 0.0117003\n",
            "Early stopping, best iteration is:\n",
            "[38]\tvalid_0's rmse: 0.00141778\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0897922\n",
            "[20]\tvalid_0's rmse: 0.0393742\n",
            "[30]\tvalid_0's rmse: 0.010884\n",
            "[40]\tvalid_0's rmse: 0.00601989\n",
            "[50]\tvalid_0's rmse: 0.0166312\n",
            "Early stopping, best iteration is:\n",
            "[35]\tvalid_0's rmse: 0.000149327\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.103862\n",
            "[20]\tvalid_0's rmse: 0.0524566\n",
            "[30]\tvalid_0's rmse: 0.0150517\n",
            "[40]\tvalid_0's rmse: 0.00529341\n",
            "[50]\tvalid_0's rmse: 0.0142807\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's rmse: 0.000804899\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.128925\n",
            "[20]\tvalid_0's rmse: 0.0859285\n",
            "[30]\tvalid_0's rmse: 0.0596401\n",
            "[40]\tvalid_0's rmse: 0.0330821\n",
            "[50]\tvalid_0's rmse: 0.0184922\n",
            "[60]\tvalid_0's rmse: 0.011506\n",
            "[70]\tvalid_0's rmse: 0.00672172\n",
            "[80]\tvalid_0's rmse: 0.00713134\n",
            "[90]\tvalid_0's rmse: 0.00920787\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.00422931\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.133181\n",
            "[20]\tvalid_0's rmse: 0.0945101\n",
            "[30]\tvalid_0's rmse: 0.0637138\n",
            "[40]\tvalid_0's rmse: 0.0439889\n",
            "[50]\tvalid_0's rmse: 0.0292016\n",
            "[60]\tvalid_0's rmse: 0.0208115\n",
            "[70]\tvalid_0's rmse: 0.016917\n",
            "[80]\tvalid_0's rmse: 0.0123336\n",
            "[90]\tvalid_0's rmse: 0.0107964\n",
            "[100]\tvalid_0's rmse: 0.0107964\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 0.0107964\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0560303\n",
            "[20]\tvalid_0's rmse: 0.0172677\n",
            "[30]\tvalid_0's rmse: 0.00207217\n",
            "[40]\tvalid_0's rmse: 0.0126469\n",
            "[50]\tvalid_0's rmse: 0.0182583\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's rmse: 0.000746943\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0771966\n",
            "[20]\tvalid_0's rmse: 0.0451606\n",
            "[30]\tvalid_0's rmse: 0.0258388\n",
            "[40]\tvalid_0's rmse: 0.0119466\n",
            "[50]\tvalid_0's rmse: 0.00578394\n",
            "[60]\tvalid_0's rmse: 0.000128063\n",
            "[70]\tvalid_0's rmse: 0.00468404\n",
            "[80]\tvalid_0's rmse: 0.00735701\n",
            "Early stopping, best iteration is:\n",
            "[60]\tvalid_0's rmse: 0.000128063\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0247958\n",
            "[20]\tvalid_0's rmse: 0.0216876\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.0100258\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00235811\n",
            "[20]\tvalid_0's rmse: 0.00325694\n",
            "[30]\tvalid_0's rmse: 0.00135248\n",
            "[40]\tvalid_0's rmse: 0.000645163\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's rmse: 0.000168926\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.502485\n",
            "[20]\tvalid_0's rmse: 0.502485\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.502485\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.511336\n",
            "[20]\tvalid_0's rmse: 0.511336\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.511336\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.444358\n",
            "[20]\tvalid_0's rmse: 0.444358\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.444358\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.398313\n",
            "[20]\tvalid_0's rmse: 0.398313\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.398313\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.416187\n",
            "[20]\tvalid_0's rmse: 0.416187\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.416187\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.42288\n",
            "[20]\tvalid_0's rmse: 0.42288\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.42288\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.496021\n",
            "[20]\tvalid_0's rmse: 0.496021\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.496021\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.374953\n",
            "[20]\tvalid_0's rmse: 0.374953\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.374953\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.490488\n",
            "[20]\tvalid_0's rmse: 0.490488\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.490488\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.505679\n",
            "[20]\tvalid_0's rmse: 0.505679\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.505679\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.367176\n",
            "[20]\tvalid_0's rmse: 0.367176\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.367176\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.464969\n",
            "[20]\tvalid_0's rmse: 0.464969\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.464969\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.461365\n",
            "[20]\tvalid_0's rmse: 0.461365\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.461365\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.441812\n",
            "[20]\tvalid_0's rmse: 0.441812\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.441812\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.365738\n",
            "[20]\tvalid_0's rmse: 0.365738\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.365738\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.298912\n",
            "[20]\tvalid_0's rmse: 0.233926\n",
            "[30]\tvalid_0's rmse: 0.184904\n",
            "[40]\tvalid_0's rmse: 0.147211\n",
            "[50]\tvalid_0's rmse: 0.118008\n",
            "[60]\tvalid_0's rmse: 0.0951235\n",
            "[70]\tvalid_0's rmse: 0.0768057\n",
            "[80]\tvalid_0's rmse: 0.0619827\n",
            "[90]\tvalid_0's rmse: 0.0593864\n",
            "[100]\tvalid_0's rmse: 0.0593765\n",
            "[110]\tvalid_0's rmse: 0.0571611\n",
            "[120]\tvalid_0's rmse: 0.0571716\n",
            "[130]\tvalid_0's rmse: 0.0553117\n",
            "[140]\tvalid_0's rmse: 0.0553042\n",
            "Early stopping, best iteration is:\n",
            "[126]\tvalid_0's rmse: 0.0552891\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.3163\n",
            "[20]\tvalid_0's rmse: 0.251126\n",
            "[30]\tvalid_0's rmse: 0.204918\n",
            "[40]\tvalid_0's rmse: 0.16826\n",
            "[50]\tvalid_0's rmse: 0.139836\n",
            "[60]\tvalid_0's rmse: 0.117427\n",
            "[70]\tvalid_0's rmse: 0.109926\n",
            "[80]\tvalid_0's rmse: 0.10983\n",
            "[90]\tvalid_0's rmse: 0.112753\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.108257\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.155927\n",
            "[20]\tvalid_0's rmse: 0.0932797\n",
            "[30]\tvalid_0's rmse: 0.0759457\n",
            "[40]\tvalid_0's rmse: 0.0556052\n",
            "[50]\tvalid_0's rmse: 0.0395384\n",
            "[60]\tvalid_0's rmse: 0.0303964\n",
            "[70]\tvalid_0's rmse: 0.0310867\n",
            "[80]\tvalid_0's rmse: 0.031568\n",
            "[90]\tvalid_0's rmse: 0.0294395\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.0280966\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.144714\n",
            "[20]\tvalid_0's rmse: 0.0872457\n",
            "[30]\tvalid_0's rmse: 0.0459301\n",
            "[40]\tvalid_0's rmse: 0.0153384\n",
            "[50]\tvalid_0's rmse: 0.00815246\n",
            "[60]\tvalid_0's rmse: 0.0109944\n",
            "[70]\tvalid_0's rmse: 0.0133807\n",
            "Early stopping, best iteration is:\n",
            "[53]\tvalid_0's rmse: 0.00684057\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.230911\n",
            "[20]\tvalid_0's rmse: 0.174067\n",
            "[30]\tvalid_0's rmse: 0.132177\n",
            "[40]\tvalid_0's rmse: 0.101325\n",
            "[50]\tvalid_0's rmse: 0.0938121\n",
            "[60]\tvalid_0's rmse: 0.0907259\n",
            "[70]\tvalid_0's rmse: 0.0889866\n",
            "[80]\tvalid_0's rmse: 0.084304\n",
            "[90]\tvalid_0's rmse: 0.0832136\n",
            "[100]\tvalid_0's rmse: 0.0785215\n",
            "[110]\tvalid_0's rmse: 0.0744168\n",
            "[120]\tvalid_0's rmse: 0.0705862\n",
            "[130]\tvalid_0's rmse: 0.067023\n",
            "[140]\tvalid_0's rmse: 0.0635952\n",
            "[150]\tvalid_0's rmse: 0.0623308\n",
            "[160]\tvalid_0's rmse: 0.0623308\n",
            "Early stopping, best iteration is:\n",
            "[147]\tvalid_0's rmse: 0.0613632\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.221589\n",
            "[20]\tvalid_0's rmse: 0.165035\n",
            "[30]\tvalid_0's rmse: 0.12504\n",
            "[40]\tvalid_0's rmse: 0.101198\n",
            "[50]\tvalid_0's rmse: 0.091722\n",
            "[60]\tvalid_0's rmse: 0.088556\n",
            "[70]\tvalid_0's rmse: 0.0862956\n",
            "[80]\tvalid_0's rmse: 0.0805082\n",
            "[90]\tvalid_0's rmse: 0.0760369\n",
            "[100]\tvalid_0's rmse: 0.071631\n",
            "[110]\tvalid_0's rmse: 0.0675666\n",
            "[120]\tvalid_0's rmse: 0.0640692\n",
            "[130]\tvalid_0's rmse: 0.0604649\n",
            "[140]\tvalid_0's rmse: 0.0593096\n",
            "[150]\tvalid_0's rmse: 0.0564123\n",
            "[160]\tvalid_0's rmse: 0.0555773\n",
            "[170]\tvalid_0's rmse: 0.0555773\n",
            "Early stopping, best iteration is:\n",
            "[151]\tvalid_0's rmse: 0.0555595\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.173677\n",
            "[20]\tvalid_0's rmse: 0.121707\n",
            "[30]\tvalid_0's rmse: 0.0853568\n",
            "[40]\tvalid_0's rmse: 0.0586367\n",
            "[50]\tvalid_0's rmse: 0.0384177\n",
            "[60]\tvalid_0's rmse: 0.0220987\n",
            "[70]\tvalid_0's rmse: 0.0106644\n",
            "[80]\tvalid_0's rmse: 0.00564803\n",
            "[90]\tvalid_0's rmse: 0.00389642\n",
            "[100]\tvalid_0's rmse: 0.00210198\n",
            "[110]\tvalid_0's rmse: 0.000590022\n",
            "[120]\tvalid_0's rmse: 0.00144126\n",
            "[130]\tvalid_0's rmse: 0.000181122\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 9.87594e-06\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.237277\n",
            "[20]\tvalid_0's rmse: 0.185397\n",
            "[30]\tvalid_0's rmse: 0.149432\n",
            "[40]\tvalid_0's rmse: 0.12226\n",
            "[50]\tvalid_0's rmse: 0.100347\n",
            "[60]\tvalid_0's rmse: 0.0833068\n",
            "[70]\tvalid_0's rmse: 0.0680902\n",
            "[80]\tvalid_0's rmse: 0.0630169\n",
            "[90]\tvalid_0's rmse: 0.0578866\n",
            "[100]\tvalid_0's rmse: 0.0584969\n",
            "[110]\tvalid_0's rmse: 0.0538222\n",
            "[120]\tvalid_0's rmse: 0.0517233\n",
            "[130]\tvalid_0's rmse: 0.0498259\n",
            "[140]\tvalid_0's rmse: 0.0485031\n",
            "[150]\tvalid_0's rmse: 0.0463862\n",
            "[160]\tvalid_0's rmse: 0.044604\n",
            "[170]\tvalid_0's rmse: 0.044604\n",
            "Early stopping, best iteration is:\n",
            "[156]\tvalid_0's rmse: 0.044604\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.250573\n",
            "[20]\tvalid_0's rmse: 0.200153\n",
            "[30]\tvalid_0's rmse: 0.164226\n",
            "[40]\tvalid_0's rmse: 0.136559\n",
            "[50]\tvalid_0's rmse: 0.115048\n",
            "[60]\tvalid_0's rmse: 0.0968634\n",
            "[70]\tvalid_0's rmse: 0.0811266\n",
            "[80]\tvalid_0's rmse: 0.0783882\n",
            "[90]\tvalid_0's rmse: 0.0761288\n",
            "[100]\tvalid_0's rmse: 0.0739688\n",
            "[110]\tvalid_0's rmse: 0.0698286\n",
            "[120]\tvalid_0's rmse: 0.0681241\n",
            "[130]\tvalid_0's rmse: 0.0666465\n",
            "[140]\tvalid_0's rmse: 0.0651018\n",
            "[150]\tvalid_0's rmse: 0.063649\n",
            "[160]\tvalid_0's rmse: 0.060884\n",
            "[170]\tvalid_0's rmse: 0.060884\n",
            "Early stopping, best iteration is:\n",
            "[158]\tvalid_0's rmse: 0.0607843\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.167387\n",
            "[20]\tvalid_0's rmse: 0.115916\n",
            "[30]\tvalid_0's rmse: 0.0787724\n",
            "[40]\tvalid_0's rmse: 0.0522936\n",
            "[50]\tvalid_0's rmse: 0.0327756\n",
            "[60]\tvalid_0's rmse: 0.0166532\n",
            "[70]\tvalid_0's rmse: 0.00331178\n",
            "[80]\tvalid_0's rmse: 0.00389812\n",
            "[90]\tvalid_0's rmse: 0.00641556\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 5.22579e-06\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.159558\n",
            "[20]\tvalid_0's rmse: 0.111137\n",
            "[30]\tvalid_0's rmse: 0.0760936\n",
            "[40]\tvalid_0's rmse: 0.0577439\n",
            "[50]\tvalid_0's rmse: 0.0397556\n",
            "[60]\tvalid_0's rmse: 0.0289907\n",
            "[70]\tvalid_0's rmse: 0.0265207\n",
            "[80]\tvalid_0's rmse: 0.0261741\n",
            "[90]\tvalid_0's rmse: 0.0222401\n",
            "[100]\tvalid_0's rmse: 0.0219897\n",
            "[110]\tvalid_0's rmse: 0.0217033\n",
            "[120]\tvalid_0's rmse: 0.018687\n",
            "[130]\tvalid_0's rmse: 0.0179411\n",
            "[140]\tvalid_0's rmse: 0.0149105\n",
            "[150]\tvalid_0's rmse: 0.0138273\n",
            "[160]\tvalid_0's rmse: 0.0127469\n",
            "[170]\tvalid_0's rmse: 0.012786\n",
            "Early stopping, best iteration is:\n",
            "[159]\tvalid_0's rmse: 0.0120505\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.189117\n",
            "[20]\tvalid_0's rmse: 0.141174\n",
            "[30]\tvalid_0's rmse: 0.103655\n",
            "[40]\tvalid_0's rmse: 0.0786472\n",
            "[50]\tvalid_0's rmse: 0.0610525\n",
            "[60]\tvalid_0's rmse: 0.054365\n",
            "[70]\tvalid_0's rmse: 0.0544678\n",
            "Early stopping, best iteration is:\n",
            "[57]\tvalid_0's rmse: 0.0512047\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.320819\n",
            "[20]\tvalid_0's rmse: 0.275613\n",
            "[30]\tvalid_0's rmse: 0.236451\n",
            "[40]\tvalid_0's rmse: 0.212808\n",
            "[50]\tvalid_0's rmse: 0.198106\n",
            "[60]\tvalid_0's rmse: 0.185857\n",
            "[70]\tvalid_0's rmse: 0.176289\n",
            "[80]\tvalid_0's rmse: 0.168274\n",
            "[90]\tvalid_0's rmse: 0.167822\n",
            "[100]\tvalid_0's rmse: 0.166771\n",
            "[110]\tvalid_0's rmse: 0.163885\n",
            "[120]\tvalid_0's rmse: 0.161127\n",
            "[130]\tvalid_0's rmse: 0.160384\n",
            "[140]\tvalid_0's rmse: 0.160384\n",
            "Early stopping, best iteration is:\n",
            "[125]\tvalid_0's rmse: 0.160384\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.327019\n",
            "[20]\tvalid_0's rmse: 0.279401\n",
            "[30]\tvalid_0's rmse: 0.245087\n",
            "[40]\tvalid_0's rmse: 0.214387\n",
            "[50]\tvalid_0's rmse: 0.198418\n",
            "[60]\tvalid_0's rmse: 0.186615\n",
            "[70]\tvalid_0's rmse: 0.172561\n",
            "[80]\tvalid_0's rmse: 0.170391\n",
            "[90]\tvalid_0's rmse: 0.164468\n",
            "[100]\tvalid_0's rmse: 0.165283\n",
            "[110]\tvalid_0's rmse: 0.162752\n",
            "[120]\tvalid_0's rmse: 0.159998\n",
            "[130]\tvalid_0's rmse: 0.158804\n",
            "[140]\tvalid_0's rmse: 0.158804\n",
            "Early stopping, best iteration is:\n",
            "[122]\tvalid_0's rmse: 0.15792\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.204725\n",
            "[20]\tvalid_0's rmse: 0.157246\n",
            "[30]\tvalid_0's rmse: 0.118611\n",
            "[40]\tvalid_0's rmse: 0.0962302\n",
            "[50]\tvalid_0's rmse: 0.0817928\n",
            "[60]\tvalid_0's rmse: 0.0695041\n",
            "[70]\tvalid_0's rmse: 0.0571575\n",
            "[80]\tvalid_0's rmse: 0.0521422\n",
            "[90]\tvalid_0's rmse: 0.0494718\n",
            "[100]\tvalid_0's rmse: 0.0438496\n",
            "[110]\tvalid_0's rmse: 0.0424271\n",
            "[120]\tvalid_0's rmse: 0.0408257\n",
            "[130]\tvalid_0's rmse: 0.0408257\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 0.0408257\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.20134\n",
            "[20]\tvalid_0's rmse: 0.149907\n",
            "[30]\tvalid_0's rmse: 0.111978\n",
            "[40]\tvalid_0's rmse: 0.0885535\n",
            "[50]\tvalid_0's rmse: 0.0747123\n",
            "[60]\tvalid_0's rmse: 0.0661546\n",
            "[70]\tvalid_0's rmse: 0.0586574\n",
            "[80]\tvalid_0's rmse: 0.0539878\n",
            "[90]\tvalid_0's rmse: 0.0469835\n",
            "[100]\tvalid_0's rmse: 0.0424331\n",
            "[110]\tvalid_0's rmse: 0.0406234\n",
            "[120]\tvalid_0's rmse: 0.0370102\n",
            "[130]\tvalid_0's rmse: 0.0340512\n",
            "[140]\tvalid_0's rmse: 0.0348775\n",
            "Early stopping, best iteration is:\n",
            "[124]\tvalid_0's rmse: 0.0339681\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.217466\n",
            "[20]\tvalid_0's rmse: 0.163206\n",
            "[30]\tvalid_0's rmse: 0.126001\n",
            "[40]\tvalid_0's rmse: 0.0966167\n",
            "[50]\tvalid_0's rmse: 0.0784932\n",
            "[60]\tvalid_0's rmse: 0.0632589\n",
            "[70]\tvalid_0's rmse: 0.051956\n",
            "[80]\tvalid_0's rmse: 0.043746\n",
            "[90]\tvalid_0's rmse: 0.0358069\n",
            "[100]\tvalid_0's rmse: 0.0280263\n",
            "[110]\tvalid_0's rmse: 0.0230433\n",
            "[120]\tvalid_0's rmse: 0.0216886\n",
            "[130]\tvalid_0's rmse: 0.0216886\n",
            "Early stopping, best iteration is:\n",
            "[113]\tvalid_0's rmse: 0.0212598\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.241571\n",
            "[20]\tvalid_0's rmse: 0.18351\n",
            "[30]\tvalid_0's rmse: 0.141596\n",
            "[40]\tvalid_0's rmse: 0.112082\n",
            "[50]\tvalid_0's rmse: 0.0935532\n",
            "[60]\tvalid_0's rmse: 0.0831732\n",
            "[70]\tvalid_0's rmse: 0.0751287\n",
            "[80]\tvalid_0's rmse: 0.0661235\n",
            "[90]\tvalid_0's rmse: 0.0579927\n",
            "[100]\tvalid_0's rmse: 0.049862\n",
            "[110]\tvalid_0's rmse: 0.0426503\n",
            "[120]\tvalid_0's rmse: 0.0404205\n",
            "[130]\tvalid_0's rmse: 0.0404205\n",
            "Early stopping, best iteration is:\n",
            "[117]\tvalid_0's rmse: 0.0397071\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0840021\n",
            "[20]\tvalid_0's rmse: 0.0287084\n",
            "[30]\tvalid_0's rmse: 0.00607254\n",
            "[40]\tvalid_0's rmse: 0.0271818\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's rmse: 0.00137674\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0776961\n",
            "[20]\tvalid_0's rmse: 0.0312698\n",
            "[30]\tvalid_0's rmse: 0.0023098\n",
            "[40]\tvalid_0's rmse: 0.0165998\n",
            "[50]\tvalid_0's rmse: 0.0296481\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's rmse: 0.00119377\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.115732\n",
            "[20]\tvalid_0's rmse: 0.0686139\n",
            "[30]\tvalid_0's rmse: 0.0431673\n",
            "[40]\tvalid_0's rmse: 0.0244\n",
            "[50]\tvalid_0's rmse: 0.0109452\n",
            "[60]\tvalid_0's rmse: 0.0142294\n",
            "[70]\tvalid_0's rmse: 0.015932\n",
            "Early stopping, best iteration is:\n",
            "[50]\tvalid_0's rmse: 0.0109452\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.123289\n",
            "[20]\tvalid_0's rmse: 0.0756483\n",
            "[30]\tvalid_0's rmse: 0.0444531\n",
            "[40]\tvalid_0's rmse: 0.0201891\n",
            "[50]\tvalid_0's rmse: 0.0234235\n",
            "[60]\tvalid_0's rmse: 0.0207767\n",
            "Early stopping, best iteration is:\n",
            "[44]\tvalid_0's rmse: 0.0159989\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.129717\n",
            "[20]\tvalid_0's rmse: 0.075771\n",
            "[30]\tvalid_0's rmse: 0.0447611\n",
            "[40]\tvalid_0's rmse: 0.0262172\n",
            "[50]\tvalid_0's rmse: 0.0170229\n",
            "[60]\tvalid_0's rmse: 0.0152819\n",
            "[70]\tvalid_0's rmse: 0.0153291\n",
            "[80]\tvalid_0's rmse: 0.0140136\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's rmse: 0.0129414\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0283145\n",
            "[20]\tvalid_0's rmse: 0.0123337\n",
            "[30]\tvalid_0's rmse: 0.0314426\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's rmse: 0.00217251\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0558058\n",
            "[20]\tvalid_0's rmse: 0.015392\n",
            "[30]\tvalid_0's rmse: 0.00454896\n",
            "[40]\tvalid_0's rmse: 0.00806167\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's rmse: 8.35839e-05\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.0303313\n",
            "[20]\tvalid_0's rmse: 0.0424748\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's rmse: 0.0133414\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tvalid_0's rmse: 0.00910219\n",
            "[20]\tvalid_0's rmse: 0.0184014\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's rmse: 0.000582763\n",
            "Best params found: {'objective': 'regression', 'num_leaves': 15, 'learning_rate': 0.04, 'max_depth': -1, 'min_data_in_leaf': 1, 'feature_fraction': 0.9, 'min_gain_to_split': 0.01, 'bagging_fraction': 0.9, 'bagging_freq': 1, 'metric': 'rmse', 'verbosity': -1} with score 1.461022329665359\n",
            "Predicting with last input shape: (1, 75)\n",
            "avg. MAE: 1.481800998129844\n",
            "avg. MAPE: 3.2652058672715\n",
            "avg. Log Loss: 0.002044798327042896\n",
            "avg. RMSE: 1.481800998129844\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_metrics['LightGBM'] = rolling_test(LGBM, X, y, chunk=(window + predictions), predictions=predictions, sequence_length=sequence_length, model_name='LightGBM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QRTkWS7h5fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9dbddfc9-2e2f-40d9-e19e-94a8df708fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting prediction for 2023-02-10\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.05, 'gamma': 0.05} with score 2373.9275557596084\n",
            "starting prediction for 2023-02-13\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.01, 'gamma': 0.05} with score 2344.982137778206\n",
            "starting prediction for 2023-02-14\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.01, 'gamma': 0.05} with score 2335.2568957742733\n",
            "starting prediction for 2023-02-15\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.05, 'gamma': 0.1} with score 2313.9772277064276\n",
            "starting prediction for 2023-02-16\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.01, 'gamma': 0.1} with score 2291.0437889117106\n",
            "starting prediction for 2023-02-17\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.05, 'gamma': 0.1} with score 2258.226005992376\n",
            "starting prediction for 2023-02-20\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.01, 'gamma': 0.1} with score 2198.2443381563216\n",
            "starting prediction for 2023-02-21\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.01, 'gamma': 0.1} with score 2140.9035576658534\n",
            "starting prediction for 2023-02-22\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.01, 'gamma': 0.1} with score 2027.3072649137375\n",
            "starting prediction for 2023-02-23\n",
            "training window shape: X_fit=(50, 15), y_fit=(50, 1)\n",
            "test window shape: X_test=(1, 15), y_test=(1, 1)\n",
            "X dims pre-sequencing: (50, 15)\n",
            "y dims pre-sequencing: (50, 1)\n",
            "stacked shape: (50, 16)\n",
            "Best params found: {'kernel': 'sigmoid', 'C': 15, 'epsilon': 0.05, 'gamma': 0.1} with score 1913.2933762336756\n",
            "avg. MAE: 3.68465300284634\n",
            "avg. MAPE: 8.316079559758656\n",
            "avg. Log Loss: 0.007921321534843756\n",
            "avg. RMSE: 3.68465300284634\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_metrics['SVM'] = rolling_test(SVM, X, y, chunk=(window + predictions), predictions=predictions, sequence_length=sequence_length, model_name='SVM')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(model_metrics)\n",
        "\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "gpBX_6qsHeC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "8855a737-2c4e-4ae9-c9a4-65bb9dee5cc9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Nave Forecast   TFT-GRU  LightGBM       SVM\n",
              "MAE             1.611000  3.377261  1.481801  3.684653\n",
              "MAPE            3.564271  7.389591  3.265206  8.316080\n",
              "Log Loss        0.002742  0.009346  0.002045  0.007921\n",
              "RMSE            1.611000  3.377261  1.481801  3.684653"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b9a3fdf-591e-4ab2-99ed-560533ecc521\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nave Forecast</th>\n",
              "      <th>TFT-GRU</th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>SVM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>1.611000</td>\n",
              "      <td>3.377261</td>\n",
              "      <td>1.481801</td>\n",
              "      <td>3.684653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>3.564271</td>\n",
              "      <td>7.389591</td>\n",
              "      <td>3.265206</td>\n",
              "      <td>8.316080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Log Loss</th>\n",
              "      <td>0.002742</td>\n",
              "      <td>0.009346</td>\n",
              "      <td>0.002045</td>\n",
              "      <td>0.007921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>1.611000</td>\n",
              "      <td>3.377261</td>\n",
              "      <td>1.481801</td>\n",
              "      <td>3.684653</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b9a3fdf-591e-4ab2-99ed-560533ecc521')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4b9a3fdf-591e-4ab2-99ed-560533ecc521 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4b9a3fdf-591e-4ab2-99ed-560533ecc521');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c7d97078-3a19-46e8-a14a-46b88b832893\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7d97078-3a19-46e8-a14a-46b88b832893')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c7d97078-3a19-46e8-a14a-46b88b832893 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_cf3ca814-8321-4804-8503-2882a7f792ef\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cf3ca814-8321-4804-8503-2882a7f792ef button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Na\\u00efve Forecast\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4573952860045258,\n        \"min\": 0.002741774827865124,\n        \"max\": 3.564270837823829,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.611,\n          3.564270837823829,\n          0.002741774827865124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TFT-GRU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.018709766100674,\n        \"min\": 0.009345886400813682,\n        \"max\": 7.389590955448448,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.3772614746093743,\n          7.389590955448448,\n          0.009345886400813682\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LightGBM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.3350606365174555,\n        \"min\": 0.002044798327042896,\n        \"max\": 3.2652058672715,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.481800998129844,\n          3.2652058672715,\n          0.002044798327042896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SVM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4029696458660785,\n        \"min\": 0.007921321534843756,\n        \"max\": 8.316079559758656,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.68465300284634,\n          8.316079559758656,\n          0.007921321534843756\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latex_code = metrics_df.to_latex(index=False)\n",
        "with open('/content/drive/My Drive/Risk Forecasting/model_metrics_table.tex', 'w') as f:\n",
        "    f.write(latex_code)"
      ],
      "metadata": {
        "id": "_20ul76_Lk4O"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}